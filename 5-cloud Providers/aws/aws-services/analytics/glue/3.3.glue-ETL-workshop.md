# ğŸš€ **AWS Glue ETL Workshop: Building a Data Pipeline with AWS Glue**

> "Data is the new oil, but you need a **refinery** to process itâ€”**AWS Glue is that refinery!**"

This workshop **guides you step-by-step** in setting up an **end-to-end data pipeline** using **AWS Glue** to extract, transform, and load (ETL) data from a MySQL database into Amazon S3.

By the end of this, youâ€™ll have **mastered AWS Glue** and will be **ready to build real-world ETL pipelines like a pro!** ğŸ’ª

---

## ğŸ— **[Workshop Architecture](https://catalog.us-east-1.prod.workshops.aws/workshops/0cba1e21-10d6-4e8e-b35f-a09338ee68d9/en-US/introduction)**

Company ABC is an online pet product retailer. They have recently launched a new product line for pets. They were expecting it to be a great success but sales from some regions were below their expectation. Marketing research suggests that limited awareness about the product in those regions is the main reason for inadequate sales. To address this problem, they want to start a targeted mail campaign in those regions that shows potential for increased sales. The team needs a fast turnaround, they donâ€™t have much time to write code and manage infrastructure. At the same time, to avoid using any Personal Identifiable Information (PII) in their mail campaign, any customer PII data need to be redacted from their mailing list.

### **Actions**

As a member of the ABC data engineering team, you are tasked to extract customer, product, and sales data. Following the extract you transform that data and create a dataset that has sales performance data for pet product categories by zip codes where specific product types need to be marketed. Sensitive data also needs to be redacted before allowing downstream analytics.

### **Data Model**

The data model provided from the sales MySQL database is provided here:

<div style="text-align: center;">
  <img src="images/glue-workshop-1-data-model.png" alt="Data Model for Glue Workshop 1" />
</div>

### **Workshop Architecture**

<div style="text-align: center;">
  <img src="images/glue-workshop-1-architecture.png" alt="Glue Workshop 1 Architecture" />
</div>

### **Workshop Solution with Visual ETL**

![glue-workshop-1-solution-with-visual-etl](images/glue-workshop-1-solution-with-visual-etl.png)

---

## ğŸ¯ **Workshop Goals**

In this hands-on workshop, we will:  
âœ… **Extract Data from an Amazon RDS MySQL Database**  
âœ… **Use AWS Glue Crawler to Discover the Schema**  
âœ… **Store Metadata in the AWS Glue Data Catalog**  
âœ… **Transform and Aggregate Data using AWS Glue ETL**  
âœ… **Store the Processed Data in Amazon S3**  
âœ… **Query the Data Using Amazon Athena**

---

## ğŸ›  **Step 1: Setting Up the MySQL Database**

### ğŸ”¹ **1ï¸âƒ£ Deploy Amazon RDS (MySQL)**

- We use **Amazon RDS Aurora MySQL**, which contains:  
  âœ… `products` â†’ Stores product details.  
  âœ… `sales` â†’ Records sales transactions.  
  âœ… `customers` â†’ Contains customer info.

ğŸ’¡ **Best Practice**: Use **AWS CloudFormation** to deploy the RDS database **automatically**.

---

## ğŸ“‚ **Step 2: Setting Up the AWS Glue Data Catalog**

The **AWS Glue Data Catalog** is a **metadata repository** that organizes databases and tables.

### ğŸ”¹ **2ï¸âƒ£ Create the Glue Database**

- Navigate to **AWS Glue Console â†’ Databases â†’ Create New Database**
- Database Name: **`glue_workshop`**
- Click **Create** âœ…

### ğŸ”¹ **3ï¸âƒ£ Create the AWS Glue Crawler**

- Navigate to **AWS Glue Console â†’ Crawlers â†’ Create Crawler**
- **Name**: `workshop_crawler`
- **Data Source**: **JDBC Connection to MySQL**
- **Target Database**: `glue_workshop`
- **Tables to Crawl**: `customers`, `sales`, `products`
- **Schedule**: **On-Demand**
- **IAM Role**: Select an IAM Role with **Glue & S3 permissions**
- Click **Create & Run** ğŸš€

ğŸ’¡ **Best Practice**: Use **AWS Secrets Manager** to store **database credentials securely**.

---

## ğŸ— **Step 3: Creating the AWS Glue ETL Job**

### ğŸ”¹ **4ï¸âƒ£ Add Source Tables**

- Navigate to **AWS Glue Studio â†’ Create New ETL Job**
- Click **Visual ETL**
- Add Source Tables: **`customers`**, **`sales`**, **`products`**
- Select **Glue Catalog** as the source

---

### ğŸ”„ **5ï¸âƒ£ Join Tables**

We will **join the three tables** using:

- `customer_id` â†’ Join **Sales & Customers**
- `product_id` â†’ Join **Sales & Products**

#### **How to Perform the Join in AWS Glue Studio**

1ï¸âƒ£ **Click â€œAdd Joinâ€ â†’ Select Sales Table**  
2ï¸âƒ£ **Join with Customers Table on `customer_id`**  
3ï¸âƒ£ **Join with Products Table on `product_id`**  
4ï¸âƒ£ **Set Join Type**: **Inner Join**

ğŸ’¡ **Best Practice**: Use **Inner Joins** to remove NULL values.

---

### ğŸ¯ **6ï¸âƒ£ Filtering Data**

We only need **valid sales records** where **quantity > 0**.

1ï¸âƒ£ **Click â€œAdd Transformationâ€ â†’ Choose â€œFilterâ€**  
2ï¸âƒ£ **Set Condition**: `quantity > 0`

ğŸ’¡ **Best Practice**: Apply **filters early** to reduce **data size and processing time**.

---

### ğŸ“Š **7ï¸âƒ£ Aggregate Data**

We will **group data by zip code** and **sum total sales**.

1ï¸âƒ£ **Click â€œAdd Transformationâ€ â†’ Choose â€œAggregateâ€**  
2ï¸âƒ£ **Group By** â†’ `zip_code`  
3ï¸âƒ£ **Aggregate Column** â†’ `SUM(total_sales_price)`

ğŸ’¡ **Best Practice**: Store aggregated data in **columnar formats like Parquet**.

---

## ğŸ“‚ **Step 4: Loading Transformed Data into Amazon S3**

### **ğŸ”¹ Store Data in S3**

We now **store the processed data** in **Amazon S3**.

1ï¸âƒ£ **Click â€œAdd Targetâ€ â†’ Choose â€œAmazon S3â€**  
2ï¸âƒ£ **Set Output Format** â†’ **Parquet**  
3ï¸âƒ£ **Specify S3 Target** â†’ `s3://glue-workshop/aggregated_sales/`

ğŸ’¡ **Best Practice**: Partition data by `year/month/day` to optimize queries in **Athena**.

---

## âš™ï¸ **Step 5: AWS Glue ETL Job Configuration**

| **Setting**           | **Value**            |
| --------------------- | -------------------- |
| **Glue Version**      | 4.0 (Latest)         |
| **Worker Type**       | `G.2X`               |
| **Number of Workers** | 10                   |
| **Timeout**           | 60 min               |
| **Logging**           | Enabled (CloudWatch) |
| **Job Bookmarking**   | Enabled              |

ğŸ’¡ **Best Practice**: Use **job bookmarking** to **avoid duplicate processing**.

---

## ğŸš€ **Step 6: Running & Monitoring the Job**

After saving the job, click **"Run"**.

- Monitor progress in **AWS Glue Console â†’ Jobs â†’ Run Details**.
- View **Spark logs** in **AWS CloudWatch**.
- Check output files in **Amazon S3**.

---

## ğŸ¯ **Final Step: Querying Data in Amazon Athena**

Once the **aggregated sales data** is stored in S3, we can use **Amazon Athena** to query it.

```sql
SELECT zip_code, SUM(total_sales_price)
FROM aggregated_sales
GROUP BY zip_code
ORDER BY SUM(total_sales_price) DESC;
```

ğŸ’¡ **Best Practice**: Use **Athena + Glue Data Catalog** for **serverless SQL querying**.

---

## ğŸ† **Final Thoughts: Why Use AWS Glue ETL?**

ğŸ“Œ **Serverless** â†’ No infrastructure management.  
ğŸ“Œ **Visual ETL** â†’ Drag-and-drop interface.  
ğŸ“Œ **Supports Streaming ETL** â†’ Integrates with **Kinesis & Kafka**.  
ğŸ“Œ **Optimized for Big Data** â†’ Uses **Apache Spark** for distributed computing.
