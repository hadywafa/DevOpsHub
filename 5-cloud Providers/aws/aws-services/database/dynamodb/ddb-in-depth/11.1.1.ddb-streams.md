# ğŸ”„ **DynamoDB Streams: Capturing Table Changes in Real-Time**

## ğŸ“Œ **1. What is DynamoDB Streams?**

**DynamoDB Streams** is a feature that **captures real-time changes** (inserts, updates, deletes) on a table and **stores them in an ordered log for 24 hours**. This allows external applications to **react to data changes** asynchronously.

ğŸ”¹ **Key Features:**  
âœ” **Tracks item-level changes** (INSERT, UPDATE, DELETE).  
âœ” **Stores changes for up to 24 hours.**  
âœ” **Processes events in order per partition.**  
âœ” **Can trigger AWS Lambda for real-time event processing.**  
âœ” **Ensures exactly-once delivery per event in a shard.**

ğŸ’¡ **Think of DynamoDB Streams like a change-data-capture (CDC) log in SQL databases!**

---

## ğŸ” **2. How DynamoDB Streams Work Internally**

1ï¸âƒ£ **A table change occurs (INSERT, UPDATE, DELETE).**  
2ï¸âƒ£ **The change is recorded in a stream log (stored for 24 hours).**  
3ï¸âƒ£ **An external consumer (e.g., AWS Lambda, Kinesis, or EC2) reads the stream.**  
4ï¸âƒ£ **The consumer processes the event (e.g., sync data to another system).**

ğŸ“Œ **Example: If a new order is added, a stream event can trigger Lambda to notify the user.**

---

## ğŸ”¥ **3. Stream View Types (What Data is Captured?)**

When enabling streams, you must choose **what kind of changes** should be recorded:

| **Stream View Type**    | **What It Captures?**                      | **Use Case Example**                                   |
| ----------------------- | ------------------------------------------ | ------------------------------------------------------ |
| `NEW_IMAGE` ğŸ”¹          | Stores the **new version** of an item.     | Log real-time inventory changes.                       |
| `OLD_IMAGE` ğŸ”¹          | Stores the **old version** before changes. | Keep a history of deleted data.                        |
| `NEW_AND_OLD_IMAGES` ğŸ”¹ | Stores **both old and new versions**.      | Track data changes for auditing.                       |
| `KEYS_ONLY` ğŸ”¹          | Stores **only primary key attributes**.    | Detect item deletions without consuming extra storage. |

ğŸ’¡ **Choosing the right stream type depends on your business logic!**

---

## ğŸ”„ **DynamoDB Streams & Shards**

<div style="text-align: center;">
  <img src="images/ddb-streams-shards.png" alt="DynamoDB Streams Shards Diagram" />
</div>

---

DynamoDB Streams capture and log changes to items in a DynamoDB table, enabling **real-time event processing** and **change tracking**. The stream is made up of **shards**, which are logical groupings of records that help distribute and parallelize data processing.

### ğŸ“Œ **How Shards Work in DynamoDB Streams**

- **Each shard corresponds to a table partition** â€“ there is a **1:1 relationship** between DynamoDB table partitions and stream shards.
- **Each record in a shard represents a modification** (insert, update, delete) on an item in the corresponding partition.
- **Shards are hierarchical** â€“ When new shards are created (due to resharding), they are linked to parent shards.
- **Applications must process parent shards first** before reading from child shards to maintain order.

### ğŸ“Œ **Key Characteristics of Shards**

| **Feature**                   | **Description**                                                                          |
| ----------------------------- | ---------------------------------------------------------------------------------------- |
| **1:1 Relationship**          | Each partition has one corresponding shard.                                              |
| **Max 2 Consumers per Shard** | Only **two processes** can read from a single shard simultaneously.                      |
| **Hierarchical Structure**    | Parent shards must be read before child shards.                                          |
| **Shards Evolve Over Time**   | As table partitions split due to growth, new shards are created while older ones expire. |

### âœ… **Example: Reading from Shards**

1ï¸âƒ£ **A table has 4 partitions**, so 4 shards are created initially.  
2ï¸âƒ£ **When an update happens in Partition #2**, a stream record is added to **Shard #2**.  
3ï¸âƒ£ **A consumer application reads from all active shards** to process data changes.

#### ğŸ”¹ **Resharding Process**

<div style="text-align: center;">
  <img src="images/ddb-streams-resharding.png" alt="DynamoDB Stream Resharding" />
</div>

- When the table scales and new partitions are created, **new shards are added while old shards expire**.
- Applications must **read from parent shards first** before reading from child shards.

### ğŸ“Œ **Processing DynamoDB Stream Shards**

- **AWS Lambda or Kinesis Adapter** can be used to **consume and process stream data efficiently**.
- Applications using DynamoDB Streams should **track and manage shard evolution** to maintain **correct event processing order**.

--

## âš™ï¸ **4. Enabling DynamoDB Streams**

### âœ… **Enable Streams on an Existing Table (AWS CLI)**

```sh
aws dynamodb update-table \
    --table-name Orders \
    --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES
```

ğŸ“Œ **Now, every time an order is updated, both the old and new version will be stored in the stream.**

---

## ğŸ“Š **5. Stream Event Structure (Whatâ€™s Inside a Record?)**

Each stream record contains:

```json
{
  "eventID": "12345",
  "eventName": "MODIFY",
  "eventSource": "aws:dynamodb",
  "dynamodb": {
    "Keys": { "OrderID": { "S": "ORD-001" } },
    "OldImage": { "Status": { "S": "Pending" } },
    "NewImage": { "Status": { "S": "Shipped" } },
    "StreamViewType": "NEW_AND_OLD_IMAGES",
    "SequenceNumber": "987654321"
  }
}
```

ğŸ“Œ **Breakdown of Fields:**

- **`eventName`** â†’ `"INSERT"`, `"MODIFY"`, `"REMOVE"`
- **`Keys`** â†’ The **primary key** of the affected item.
- **`OldImage`** â†’ The **old version** (before update).
- **`NewImage`** â†’ The **new version** (after update).
- **`SequenceNumber`** â†’ Ensures events are processed in order.

---

## ğŸ”„ **6. Reading & Processing Streams**

### âœ… **Read Stream Data Using AWS CLI**

```sh
aws dynamodb describe-table --table-name Orders \
    --query "Table.LatestStreamArn"
```

ğŸ“Œ **This fetches the latest stream ARN, which is needed to process events.**

### âœ… **Fetch Stream Records**

```sh
aws dynamodb get-shard-iterator \
    --stream-arn arn:aws:dynamodb:us-east-1:123456789012:table/Orders/stream/2025-03-09T12:00:00.000 \
    --shard-id shardId-00000001642421212134-12345678 \
    --shard-iterator-type TRIM_HORIZON
```

ğŸ“Œ **Retrieves the first batch of records in the stream.**

---

## âš¡ **7. Automating Processing with AWS Lambda**

One of the most **common use cases** for DynamoDB Streams is triggering **AWS Lambda functions** on changes.

ğŸ“Œ **Example Use Cases:**  
âœ” **Notify users when an order status changes.**  
âœ” **Sync DynamoDB with Elasticsearch for search indexing.**  
âœ” **Update a caching layer when data changes.**

### âœ… **Step 1: Create a Lambda Function to Process Streams**

```python
import json

def lambda_handler(event, context):
    for record in event['Records']:
        event_name = record['eventName']
        new_image = record['dynamodb'].get('NewImage', {})
        old_image = record['dynamodb'].get('OldImage', {})

        print(f"Event Type: {event_name}")
        print(f"Old Item: {old_image}")
        print(f"New Item: {new_image}")

    return {"statusCode": 200, "body": json.dumps("Stream Processed!")}
```

ğŸ“Œ **This function logs every change made to the DynamoDB table.**

### âœ… **Step 2: Attach Lambda to DynamoDB Streams**

```sh
aws lambda create-event-source-mapping \
    --function-name ProcessOrderStream \
    --event-source arn:aws:dynamodb:us-east-1:123456789012:table/Orders/stream/2025-03-09T12:00:00.000 \
    --starting-position TRIM_HORIZON
```

ğŸ“Œ **Now, every time an order is inserted/updated, the Lambda function runs automatically!** ğŸš€

---

## ğŸ† **8. When Should You Use DynamoDB Streams?**

âœ… **Best Use Cases for Streams:**  
âœ” **Triggering real-time notifications** (e.g., send email on order updates).  
âœ” **Syncing data to external systems** (e.g., Elasticsearch, Redshift, another DB).  
âœ” **Building event-driven architectures** (e.g., microservices reacting to data changes).  
âœ” **Auditing data changes** (e.g., logging updates to an audit table).

âŒ **Avoid Using Streams When:**

- You need **long-term storage of change history** (streams expire after 24 hours).
- You want **exact real-time processing** (streams have slight latency).
- You have **high-throughput workloads** (streams add some read overhead).

---

## ğŸ¯ **9. Key Takeaways**

âœ” **DynamoDB Streams capture real-time changes (INSERT, UPDATE, DELETE).**  
âœ” **Stores change records for 24 hours.**  
âœ” **Supports four view types (`NEW_IMAGE`, `OLD_IMAGE`, etc.).**  
âœ” **Can trigger AWS Lambda for automatic processing.**  
âœ” **Commonly used for real-time notifications, data sync, and event-driven apps.**  
âœ” **Stream processing is ordered per partition.**

---

### ğŸ¯ **Final Thought**

ğŸ’¡ **If SQL databases use Change Data Capture (CDC), DynamoDB Streams provide the same capability in a serverless, event-driven way.**

ğŸ”¥ **Now, does DynamoDB Streams make sense from your SQL background?** ğŸ˜ƒ
