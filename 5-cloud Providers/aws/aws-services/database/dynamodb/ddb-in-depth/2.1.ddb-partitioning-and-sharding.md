# 🔥 **Partitioning vs. Sharding in DynamoDB: How Data is Stored & Scaled**

DynamoDB is a **serverless NoSQL database** that automatically distributes data across **multiple partitions and storage nodes** for **high availability and scalability**. However, its architecture differs from **traditional SQL partitioning and sharding**.

---

## 🏗 **Partitioning vs. Sharding: SQL vs. DynamoDB**

In SQL databases like PostgreSQL, **partitioning and sharding** are two distinct concepts:

| **Feature**      | **PostgreSQL (SQL Partitioning & Sharding)**                                    | **DynamoDB (NoSQL Auto-Scaling)**                                                              |
| ---------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| **Partitioning** | **Divides a single table** into logical segments within the **same database**   | **DynamoDB automatically distributes data** across multiple **partitions** based on a hash key |
| **Sharding**     | **Splits the entire database** into independent databases across multiple nodes | **DynamoDB partitions are stored across multiple physical storage nodes** (shards)             |
| **Management**   | **Manual setup** (requires defining partitioning/sharding rules)                | **Fully automated**, AWS manages partitioning, scaling, and replication                        |
| **Scalability**  | **Limited by database server resources**                                        | **Infinite scaling** by adding more partitions across multiple nodes                           |

💡 **DynamoDB blurs the line between partitioning and sharding** because it **automatically manages both**, ensuring performance and availability.

---

## 🔄 **How DynamoDB Uses Partitioning & Sharding Together**

DynamoDB stores data **horizontally distributed across multiple partitions**, with each partition **replicated for durability**.

### 📌 **Step 1: Data is Partitioned (Logical Segments)**

- When data is written to DynamoDB, **it hashes the partition key** to determine the partition where the data will be stored.
- **Each partition holds a subset of table data** and has **a fixed max storage of 10GB**.

### 📌 **Step 2: Partitions Are Sharded Across Multiple Servers**

- Each **partition is replicated across 3 Availability Zones (AZs)**.
- AWS **automatically splits partitions across multiple nodes** when needed.

### 📌 **Step 3: Read/Write Requests Are Routed to Partitions**

- When you **query DynamoDB**, AWS routes requests to the **correct partition** using the hash key.
- **If traffic increases**, AWS **automatically scales partitions**, ensuring smooth operation.

---

## 📊 **Visualizing DynamoDB's Partitioning & Sharding Architecture**

```mermaid
graph TD
    A[DynamoDB Table] -->|Hashes Partition Key| B[Partition 1]
    A -->|Hashes Partition Key| C[Partition 2]
    A -->|Hashes Partition Key| D[Partition 3]

    B -->|Sharded & Replicated| E[Storage Node A (AZ1)]
    B -->|Sharded & Replicated| F[Storage Node B (AZ2)]
    B -->|Sharded & Replicated| G[Storage Node C (AZ3)]

    C -->|Sharded & Replicated| H[Storage Node D (AZ1)]
    C -->|Sharded & Replicated| I[Storage Node E (AZ2)]
    C -->|Sharded & Replicated| J[Storage Node F (AZ3)]
```

✅ **Each partition is stored across multiple availability zones for high durability**.  
✅ **AWS handles the replication and rebalancing automatically**.

---

## 📌 **How DynamoDB Handles Scaling & Data Distribution**

### ✅ **Partitions: The Core Logical Unit**

- A **partition is a unit of storage** that holds a portion of the table’s data.
- **A single partition has a max limit of 10GB** and a read/write capacity limit.
- If a partition **reaches its limit**, AWS **automatically splits it into two partitions**.

### ✅ **DynamoDB Nodes (Physical Servers)**

- Each DynamoDB **node hosts multiple partitions**.
- AWS **ensures that partitions are evenly distributed** across nodes for load balancing.

### ✅ **Replication Across Availability Zones**

- Each partition **is automatically replicated across 3 AZs** to ensure high availability.
- If one node fails, **data is still available from another AZ**.

---

## 🔥 **How This Works in Practice (Example)**

### **Scenario: Storing User Profiles in DynamoDB**

Imagine you're building a **social media app** that stores **user profiles**.

| **UserID (Partition Key)** | **Username** | **Email**           | **Region** |
| -------------------------- | ------------ | ------------------- | ---------- |
| `USR1001`                  | Alice        | <alice@email.com>   | US         |
| `USR2002`                  | Bob          | <bob@email.com>     | EU         |
| `USR3003`                  | Charlie      | <charlie@email.com> | Asia       |

---

### **How DynamoDB Handles It Internally**

> 📌 Each of the partition may be taught of as Hashable like data structure that hosts multiple key-value pairs. These key-value pairs belong to a single table shard. The Key is referred to as the **Partition key**. Value may be a single item or an **Item collection**. In this illustration value is an item collection. Each item collection may be thought of as a Hashtable like data structure. Items in the collection are identified by an inner key that is referred to as the **Sort key** or **Range key**.

![dynamodb-partition-components](images/dynamodb-partition-components.png)

> 💡 Note: A table MUST be defined with a Partition key and Sort key for a table is optional. More on this topic later.

#### 1️⃣ **Partitioning Happens First**

- DynamoDB **hashes UserID** (`USR1001`) to **determine a partition**.
- Example:

  ```ini
  hash('USR1001') → Partition #1
  hash('USR2002') → Partition #2
  hash('USR3003') → Partition #3
  ```

#### 2️⃣ **Sharding Distributes Partitions Across Servers**

- **Partition #1 might be stored on Server A**
- **Partition #2 might be stored on Server B**
- **Partition #3 might be stored on Server C**

This **automatic distribution** ensures that no single server is overloaded.

---

## 📈 **DynamoDB Auto-Scaling & Performance**

### ✅ **Zero-Impact Auto-Scaling**

- If a partition **exceeds 10GB or traffic increases**, AWS **automatically creates new partitions**.
- Example:

  ```ini
  Partition #1 → (Splits into) → Partition #4 & Partition #5
  ```

- This process **happens in the background** with **zero downtime**.

### ✅ **Partition Key Selection Matters**

- **A poorly chosen partition key can cause hot partitions** (one node overloaded while others remain idle).
- **A good partition key ensures even data distribution**.

---

## 🏁 **Key Takeaways**

1️⃣ **Partitioning in DynamoDB** → Uses **hash-based partitioning** to distribute data.  
2️⃣ **Sharding in DynamoDB** → **Partitions are spread across multiple servers** for scalability.  
3️⃣ **Replication Across AZs** → Each partition is **automatically replicated across 3 AZs**.  
4️⃣ **Fully Automated Scaling** → AWS **handles partitioning, splitting, and balancing transparently**.  
5️⃣ **Partition Key Matters** → Choosing a **high-cardinality partition key** prevents **hot partitions**.

---

## 🚀 **Final Thought**

While DynamoDB **uses partitioning techniques**, it is **essentially a sharded database** because **each partition can be stored across multiple servers**.  
💡 **Would you like an example of querying this data efficiently? Let me know!** 🚀
