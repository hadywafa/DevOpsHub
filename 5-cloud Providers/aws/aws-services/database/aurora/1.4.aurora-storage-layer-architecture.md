# ğŸ— **Amazon Aurora Storage Nodes â€“ Behind the Scenes**

Amazon Auroraâ€™s storage architecture is **unique compared to traditional RDBMS systems** because it **decouples compute from storage** and uses a **distributed, fault-tolerant storage layer**. But how exactly does this work behind the scenes?

---

![alt text](image.png)

## ğŸ¢ **What are Aurora Storage Nodes?**

### ğŸ” **Aurora Storage Layer: High-Level Overview**

âœ” Auroraâ€™s **storage layer is separate from the database instances**.  
âœ” Data is stored in **10GB â€œprotection groupsâ€ (shards)**.  
âœ” Each 10GB segment is **replicated 6 times across 3 AWS Availability Zones (AZs)**.  
âœ” Aurora **does not use traditional block storage (EBS)**â€”it is a **distributed storage engine** built specifically for the cloud.

### âš™ï¸ **How Aurora Storage Nodes Are Structured**

Auroraâ€™s storage nodes consist of:

- **âœ”ï¸ Protection Groups** â€“ Smallest unit of storage (10GB), replicated **6 times across 3 AZs**.
- **âœ”ï¸ Page Cache** â€“ Manages in-memory copies of data pages.
- **âœ”ï¸ Redo Log Storage** â€“ Aurora does **physical storage-based replication** instead of WAL (Write-Ahead Log) shipping.
- **âœ”ï¸ Storage Volume** â€“ A collection of **storage segments that auto-scale up to 128TiB**.
- **âœ”ï¸ Consistency Model** â€“ Uses a **quorum-based write system** to ensure durability.

ğŸ“Œ **Key Takeaways:**

- **âœ” Aurora does not store the full database separately in each AZ**â€”it **distributes chunks** of data across multiple AZs.
- **âœ” Each storage node keeps track of only a part of the dataset**.
- **âœ” This architecture reduces replication lag and increases fault tolerance**.

---

## ğŸ”„ **2ï¸âƒ£ How Do Aurora Instances Stay in Sync Across AZs?**

### ğŸ— **How Writes Are Handled in Aurora**

âœ” **A database write happens only in the writer node (primary instance)**.  
âœ” **Storage nodes apply changes to all replicas (reader nodes) at the storage layer, not at the database layer**.  
âœ” **Writes are committed only when at least 4 out of 6 storage nodes confirm the change (Quorum-Based Writes)**.

ğŸ“Œ **Step-by-step write process:**

- **1ï¸âƒ£** A write transaction is sent to the **primary instance**.
- **2ï¸âƒ£** Aurora **creates a redo log record** (instead of modifying physical data immediately).
- **3ï¸âƒ£** The **redo log is sent to 6 storage nodes** in **3 different AZs**.
- **4ï¸âƒ£** **At least 4 out of 6 storage nodes must acknowledge the write** for it to be considered durable.
- **5ï¸âƒ£** Once **confirmed**, the write is **committed** and available for read replicas.

âœ… **This means:**

- Aurora **does not use traditional WAL (Write-Ahead Logging)** like PostgreSQL.
- **Storage nodes apply redo logs directly to their segments**, reducing network overhead.
- **Commit latency is much lower than RDS** because **Auroraâ€™s shared storage layer ensures all nodes have the same data at the same time**.

---

### ğŸ” **How Reads Are Handled in Aurora**

âœ” **Aurora read replicas do not need to receive WAL logs** or **copy full datasets**.  
âœ” Since **all instances share the same distributed storage**, read replicas just **query the existing storage nodes**.  
âœ” This **eliminates replication lag**, which is common in traditional RDS read replicas.

ğŸ“Œ **Step-by-step read process:**

- **1ï¸âƒ£** A read request is sent to a **read replica**.
- **2ï¸âƒ£** The read replica **fetches data directly from the shared storage layer**.
- **3ï¸âƒ£** The read request **returns immediately without waiting for WAL log replication**.

âœ… **This means:**

- Aurora replicas **don't need to sync a full dataset**â€”they just query the same storage nodes.
- **Failover is much faster (~30 seconds)** because the **standby replica already has all the data loaded**.
- **No replication lag** since **all replicas share the same storage volume**.

---

## ğŸ”€ **3ï¸âƒ£ How Does Aurora Sync Writes & Reads Between Instances?**

### ğŸ”¹ **Quorum-Based Write System**

âœ” **Aurora storage nodes operate in a distributed quorum model**.  
âœ” **Writes are committed when at least 4 out of 6 storage nodes acknowledge them**.  
âœ” If a node fails, Aurora **reconstructs the missing data from other nodes** without downtime.

### ğŸ”¹ **Low-Latency Read Scaling**

âœ” Since **read replicas directly access shared storage**, there is **no need for physical data replication**.  
âœ” Replicas **query the same distributed volume as the writer instance**, ensuring near-instant updates.  
âœ” **Aurora supports up to 15 read replicas** with **<100ms replication lag**.

---

## ğŸ¯ **Final Summary: How Aurora Storage Works Behind the Scenes**

| **Feature**           | **Aurora**                                                |
| --------------------- | --------------------------------------------------------- |
| **Storage Model**     | **Distributed, shared storage (not EBS)**                 |
| **Storage Unit**      | **10GB protection groups (shards)**                       |
| **Data Replication**  | **6 copies across 3 AZs**                                 |
| **Write Consistency** | **Quorum-based (4/6 storage nodes must confirm a write)** |
| **Read Scaling**      | **Up to 15 replicas, no replication lag**                 |
| **Failover Speed**    | **~30 seconds (instant access to shared storage)**        |

ğŸ’¡ **Final Thought**:  
âœ” **Auroraâ€™s shared storage removes replication lag**.  
âœ” **Writes are applied at the storage layer instead of using WAL logs**.  
âœ” **Failover is faster because all nodes share the same underlying storage**.

ğŸ“Œ **Would you like a deep dive into how Aurora handles crash recovery and self-healing? Let me know! ğŸš€ğŸ”¥**
