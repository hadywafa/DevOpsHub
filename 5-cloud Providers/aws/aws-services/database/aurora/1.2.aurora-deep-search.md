# Amazon Aurora’s Distributed Storage Architecture

Amazon Aurora uses a **purpose-built distributed storage layer** rather than traditional RAID or user-managed sharding. Aurora’s architecture decouples the database compute (SQL processing nodes) from a multi-tenant, fault-tolerant storage service. This storage layer is shared across all nodes of an Aurora cluster and spans multiple disks and nodes across Availability Zones (AZs) ([PowerPoint Presentation](https://www.percona.com/sites/default/files/ple19-slides/ple19-deep-dive-amazon-aurora.pdf#:~:text=Aurora%20scale,written%20in%2010GB%20%E2%80%9Cprotection%20groups%E2%80%9D)) ([Amazon Aurora storage - Amazon Aurora](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#:~:text=Aurora%20data%20is%20stored%20in,The%20amount%20of%20replication%20is)). Below is a high-level illustration of Aurora’s storage architecture, which provides a single **cluster volume** accessed by the primary and any replicas:

([ Why Choose Amazon Aurora Over Regular RDS? | Shikisoft Blog](https://blog.shikisoft.com/why-choose-aurora-over-regular-rds/)) _Aurora’s architecture separates the database instances (compute) from a distributed storage cluster. The **cluster volume** is striped across storage nodes in three AZs, with six copies of each data segment (two per AZ). This shared storage is automatically attached to all Aurora DB instances in the cluster._ ([ Why Choose Amazon Aurora Over Regular RDS? | Shikisoft Blog](https://blog.shikisoft.com/why-choose-aurora-over-regular-rds/#:~:text=storage%20is%20separate%20from%20the,will%20still%20have%206%20copies)) ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=The%20Aurora%20quorum%20In%20Aurora%2C,hasn%E2%80%99t%20responded%20for%20a%20while))

## Storage Layer Design and Data Distribution

Aurora’s storage engine is a **shared distributed volume** that all instances in the cluster connect to. Unlike a traditional RDS setup (where each database server has its own EBS volume or local RAID array), Aurora’s data is kept in a **single virtual volume** independent of the DB instances ([Amazon Aurora storage - Amazon Aurora](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#:~:text=The%20Aurora%20shared%20storage%20architecture,does%20Aurora%20remove%20the%20data)). This means you can add or remove Aurora instances without copying data – new instances simply attach to the existing storage, and all see the same data.

**Data distribution** is handled by splitting the volume into fixed-size segments and spreading them across many storage nodes. Aurora **partitions its storage into 10 GB chunks** (often called _segments_ or protection groups) which are the unit of distribution and replication ([PowerPoint Presentation](https://d1.awsstatic.com/events/reinvent/2020/Amazon_Aurora_storage_demystified_DAT401.pdf#:~:text=Aurora%20uses%20segmented%20storage%20Partition,in%20less%20than%20a%20minute)) ([Amazon Aurora Storage - Quick Notes](http://blog.asquareb.com/blog/2021/01/10/amazon-aurora-storage/#:~:text=to%20a%20%E2%80%9Chot%20log%E2%80%9D%20and,storage%20nodes%20which%20are%20EC2)). Each 10GB segment is stored with **six copies** across multiple nodes, forming a “protection group” for that chunk ([PowerPoint Presentation](https://d1.awsstatic.com/events/reinvent/2020/Amazon_Aurora_storage_demystified_DAT401.pdf#:~:text=Aurora%20uses%20segmented%20storage%20Partition,in%20less%20than%20a%20minute)) ([Amazon Aurora Storage - Quick Notes](http://blog.asquareb.com/blog/2021/01/10/amazon-aurora-storage/#:~:text=number%20of%20IOs%20by%20a,of%2C%20the%20storage%20nodes%20which)). These segments are striped across **hundreds or even thousands of storage nodes** in the Aurora storage fleet ([PowerPoint Presentation](https://www.percona.com/sites/default/files/ple19-slides/ple19-deep-dive-amazon-aurora.pdf#:~:text=system%20designed%20for%20databases%20Storage,to%2064TB%20Shared%20storage%20volume)) ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=In%20Aurora%2C%20we%20do%20this,to%20the%20storage%20tier%20are)). In effect, a large database is automatically **spread across many disks and nodes** behind the scenes. As your data grows, Aurora will add new 10GB segments (up to the maximum cluster size) without manual provisioning ([PowerPoint Presentation](https://d1.awsstatic.com/events/reinvent/2020/Amazon_Aurora_storage_demystified_DAT401.pdf#:~:text=Database%20resizing%20Volume%20size%20increases,storage%20usage%3A%20Volume%20Bytes%20Used)) ([Amazon Aurora storage - Amazon Aurora](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#:~:text=Aurora%20cluster%20volumes%20automatically%20grow,are%20reliability%20and%20high%20availability)). Aurora can currently scale up to 64 TiB or even 128 TiB of data per cluster (depending on engine version) with this seamless segment-by-segment growth ([Amazon Aurora storage - Amazon Aurora](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#:~:text=Aurora%20cluster%20volumes%20automatically%20grow,are%20reliability%20and%20high%20availability)).

Notably, Aurora’s storage layer is a **database-aware, distributed system** – it’s not using a traditional RAID controller or simple block replication. Instead of striping and mirroring at the hardware level, Aurora’s storage service intelligently manages data across nodes in multiple AZs. For example, it keeps **two copies of each segment in each of three AZs (6 copies total)** ([PowerPoint Presentation](https://www.percona.com/sites/default/files/ple19-slides/ple19-deep-dive-amazon-aurora.pdf#:~:text=of%20storage%20nodes%20distributed%20over,to%2064TB%20Shared%20storage%20volume)). This design achieves a similar goal to RAID 10 (striping + mirroring) but on a network/distributed scale and with built-in awareness of database logs and pages. The result is a storage system that can **tolerate node and AZ failures** and scale out throughput, which would be hard to achieve with a single-server RAID setup.

## Replication and Fault Tolerance Mechanisms

Aurora’s storage is **highly replicated and fault-tolerant by design**. Every piece of data (each 10GB segment) is stored six ways across three AZs, as mentioned. Aurora uses a _quorum model_ for replication: it writes to all 6 copies in parallel, but only needs a **quorum of 4 acknowledgments (4 of 6)** to consider a write committed ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=The%20Aurora%20quorum%20In%20Aurora%2C,hasn%E2%80%99t%20responded%20for%20a%20while)). This means the database node doesn’t have to wait for the slowest disk or network path – as soon as four storage nodes confirm, the transaction is durable. If one node is slow or temporarily offline, the others in the quorum still ensure the write succeeds without pause ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=In%20Aurora%2C%20we%20use%20a,set%20using%20a%20membership%20change)). Reads are typically served from a smaller quorum (any 3 of 6 copies can form a read quorum) or often from a single up-to-date copy for efficiency, since the system ensures at least one copy has the latest data ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=The%20Aurora%20quorum%20In%20Aurora%2C,hasn%E2%80%99t%20responded%20for%20a%20while)).

**Fault tolerance:** Aurora’s design can handle the loss of an entire AZ plus additional failures without losing data. Because data is duplicated in each AZ, losing one AZ (which takes out 2 copies of every segment) still leaves 4 copies available – enough to both read and **continue writing (since 4/6 quorum remains) ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=A%20six,independent%20AZs%20in%20the%20region))**. In the extreme case of an AZ failure _plus_ one more node failure elsewhere (“AZ+1”), Aurora would still have 3 out of 6 copies, which guarantees no data loss; the system would pause writes if needed and immediately begin repairing to restore full quorum ([PowerPoint Presentation](https://d1.awsstatic.com/events/reinvent/2020/Amazon_Aurora_storage_demystified_DAT401.pdf#:~:text=What%20if%20an%20AZ%20fails%3F,copies%20%E2%80%A2%20Recover%20write%20availability)) ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=A%20six,independent%20AZs%20in%20the%20region)). Aurora continuously monitors the health of storage nodes and segments. If a node fails or a segment copy becomes unavailable, the storage service automatically **replicates the segment to a new node** to rebuild six good copies ([PowerPoint Presentation](https://d1.awsstatic.com/events/reinvent/2020/Amazon_Aurora_storage_demystified_DAT401.pdf#:~:text=What%20if%20an%20AZ%20fails%3F,copies%20%E2%80%A2%20Recover%20write%20availability)) ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=the%20six%20copies,set%20using%20a%20membership%20change)). Thanks to the 10GB segment size and a fast network, replacing a lost segment copy is very quick – on the order of under a minute on a 10 Gbit link ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=In%20Aurora%2C%20we%20do%20this,to%20the%20storage%20tier%20are)). This rapid self-healing means the window of reduced redundancy is very small, greatly reducing the chance of a second failure causing harm. Aurora even scrubs data for bit rot and uses **background repair**: it regularly scans copies and if any bit errors or inconsistencies are found in one copy, it corrects it using the other copies ([ Why Choose Amazon Aurora Over Regular RDS? | Shikisoft Blog](https://blog.shikisoft.com/why-choose-aurora-over-regular-rds/#:~:text=Image%3A%20Amazon%20Aurora%20)). All of this happens behind the scenes without admin intervention.

In practice, Aurora’s storage replication provides **higher durability and availability** than a typical single-AZ database with RAID. A normal RAID array might protect against one disk failing, but Aurora’s approach ensures that **even if an entire data center (AZ) goes down, your database keeps running** with no data loss ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=A%20six,independent%20AZs%20in%20the%20region)). Failover is faster too – since all instances share storage, promoting a replica to primary doesn’t require data copy or sync. Any Aurora Replica can take over in seconds because the data is already current on the shared volume (at most, it applies the last few log records in memory) ([Aurora Read replicas for failover instead of multi - AZ](https://repost.aws/questions/QU55de583lR_6INgzuawwHRw/aurora-read-replicas-for-failover-instead-of-multi-az#:~:text=Aurora%20Read%20replicas%20for%20failover,AZ%20is%20very%20much%20applicable)) ([Amazon Aurora Under the Hood: Quorum Reads and Mutating State | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-reads-and-mutating-state/#:~:text=read%20replicas,without%20loss%20of%20data%20or)).

## Performance Optimizations and Scalability Features

Aurora’s storage architecture isn’t just about reliability – it’s also optimized for performance and scale. A key difference is that Aurora’s database engine uses a **log-based storage approach**. Instead of writing data pages to disk on every change (as InnoDB or PostgreSQL normally would), the Aurora engine **only writes redo log records to the storage layer** for persistence ([Amazon Aurora Under the Hood: Quorum Reads and Mutating State | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-reads-and-mutating-state/#:~:text=However%2C%20Aurora%20avoids%20quorum%20amplification,Neither%20are%20possible%20for%20reads)). The storage nodes understand database redo logs and are responsible for applying those logs to the actual data pages in the background. In other words, when a transaction commits, the Aurora instance sends the log records (representing the changes) to the storage cluster, which durably stores them and later **assembles pages from these logs** ([Amazon Aurora Under the Hood: Quorum Reads and Mutating State | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-reads-and-mutating-state/#:~:text=However%2C%20Aurora%20avoids%20quorum%20amplification,Neither%20are%20possible%20for%20reads)). This design has several performance benefits:

- **Minimal I/O per commit:** The database node only performs **a single write operation per transaction** to the storage system (the redo log write) which is acknowledged by the distributed storage ([Amazon Aurora Storage - Quick Notes](http://blog.asquareb.com/blog/2021/01/10/amazon-aurora-storage/#:~:text=to%20traditional%20database%20where%20log,storage%20nodes%20which%20are%20EC2)). It doesn’t need to flush data pages or maintain a separate binary log for replicas – the shared storage takes care of durability and replication. This dramatically reduces the I/O overhead. (By some estimates, Aurora cuts the number of disk I/Os per transaction by a factor of ~7 compared to a traditional setup that writes logs, double-writes pages for crash safety, etc. ([Amazon Aurora Storage - Quick Notes](http://blog.asquareb.com/blog/2021/01/10/amazon-aurora-storage/#:~:text=to%20traditional%20database%20where%20log,storage%20nodes%20which%20are%20EC2)).)

- **No double-writes or expensive fsyncs:** Because the storage layer guarantees atomicity of each log record across replicas, Aurora avoids the InnoDB “doublewrite” mechanism and heavy fsync calls. The log records are small and sequential, which is very efficient for the SSD-based storage nodes. The storage nodes then coalesce log records and create/update the actual data pages asynchronously in the background ([Amazon Aurora Storage - Quick Notes](http://blog.asquareb.com/blog/2021/01/10/amazon-aurora-storage/#:~:text=In%20the%20background%20the%20storage,When%20there%20are%20multiple)). This offloading of work means the database instance can acknowledge commits quickly and let the storage tier handle the harder part of persisting and merging data.

- **High throughput via parallelism:** The Aurora volume being striped across many nodes means **I/O operations can be parallelized**. Different segments (10GB chunks) can be read or written independently on different storage nodes simultaneously. This gives Aurora the ability to handle very high throughput and IOPS, beyond the limits of a single disk. In benchmarks, Aurora has demonstrated up to _5× the throughput of stock MySQL_ on similar hardware, largely thanks to this distributed design ([ Why Choose Amazon Aurora Over Regular RDS? | Shikisoft Blog](https://blog.shikisoft.com/why-choose-aurora-over-regular-rds/#:~:text=Aurora%E2%80%99s%20performance%20is%20better%20and,more%20consistent%20than%20Amazon%20RDS)).

- **Reduced replication lag:** In Aurora, read replicas do not need to maintain a separate copy of the data. All replicas share the same storage volume, so the primary instance’s writes are immediately visible to replicas at the storage level. The only delay is that a replica instance will apply the incoming redo log stream to its local buffer cache to stay in sync ([Amazon Aurora Under the Hood: Quorum Reads and Mutating State | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-reads-and-mutating-state/#:~:text=read%20replicas,without%20loss%20of%20data%20or)). This **async log shipping to the replicas’ caches** has negligible impact on the primary’s performance (it doesn’t wait for replicas to apply changes) and typically results in replicas lagging only tens of milliseconds behind. There’s no lengthy I/O or network process to copy data pages between primary and replica. This design also makes **failover faster and more predictable**, since any replica can be promoted without data loss ([Aurora Read replicas for failover instead of multi - AZ](https://repost.aws/questions/QU55de583lR_6INgzuawwHRw/aurora-read-replicas-for-failover-instead-of-multi-az#:~:text=Aurora%20Read%20replicas%20for%20failover,AZ%20is%20very%20much%20applicable)).

- **Automatic storage scaling:** Aurora’s storage automatically grows (and recently, can shrink) on demand. You **don’t need to pre-provision** a huge disk or manually add volumes; the cluster volume will extend itself in 10GB increments as your database size increases ([PowerPoint Presentation](https://d1.awsstatic.com/events/reinvent/2020/Amazon_Aurora_storage_demystified_DAT401.pdf#:~:text=Database%20resizing%20Volume%20size%20increases,storage%20usage%3A%20Volume%20Bytes%20Used)) ([Amazon Aurora storage - Amazon Aurora](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#:~:text=Aurora%20cluster%20volumes%20automatically%20grow,are%20reliability%20and%20high%20availability)). It can scale up to 128 TiB without manual sharding. This elasticity means you can start small and let the storage layer handle growth, which simplifies management and avoids performance issues from running out of space. Similarly, if you delete a lot of data (drop tables, etc.), Aurora will reclaim and **contract the storage usage** to save cost ([PowerPoint Presentation](https://d1.awsstatic.com/events/reinvent/2020/Amazon_Aurora_storage_demystified_DAT401.pdf#:~:text=Example%3A%20Database%20resizing)) ([Amazon Aurora and Local Storage - DEV Community](https://dev.to/aws-builders/amazon-aurora-and-local-storage-50of#:~:text=architecture,Please%20refer%20to)).

- **Compute/storage separation for read scaling:** Because the storage is shared, you can **add multiple Aurora Replicas (read-only instances)** to scale out read workload without copying data. All replicas access the same storage concurrently. This not only improves read throughput but also isolates read-heavy traffic from the writer instance’s CPU/memory, since each read replica has its own compute resources. Yet, they all leverage the single storage copy of the data, which is more efficient than each replica having its own full copy (as in conventional replication) ([Multi-az deployment in AWS Aurora and read replicas](https://stackoverflow.com/questions/34016947/multi-az-deployment-in-aws-aurora-and-read-replicas#:~:text=Unlike%20other%20RDS%20offerings%2C%20read,each%20of%20three%20availability%20zones)) ([Amazon Aurora Under the Hood: Quorum Reads and Mutating State | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-reads-and-mutating-state/#:~:text=read%20replicas,without%20loss%20of%20data%20or)).

Additionally, Aurora’s architecture has enabled features like **Parallel Query**, which uses the distributed storage nodes’ computing power for certain heavy read queries. In Aurora Parallel Query, the database can push down scan and filter operations to the storage layer, which then **executes those operations across all the storage nodes in parallel** and returns a reduced data set to the database node ([Amazon Aurora Parallel Query](https://aws.amazon.com/rds/aurora/parallel-query/#:~:text=Queries%20are%20also%20slowed%20down,less%20data%20over%20the%20network)) ([New – Parallel Query for Amazon Aurora | AWS News Blog](https://aws.amazon.com/blogs/aws/new-parallel-query-for-amazon-aurora/#:~:text=New%20%E2%80%93%20Parallel%20Query%20for,storage%20nodes%2C%20with%20speed)). This leverages the fact that each storage node has CPU and can access its segment of data quickly, thereby accelerating analytical queries that would otherwise scan large tables. It’s another example of performance optimization made possible by Aurora’s storage design (although it targets specific use cases).

## Comparison with RAID and Sharding Approaches

**Compared to traditional RAID:** Aurora does _not_ use RAID in the typical sense of a local disk array. In a regular database server, you might configure multiple drives in RAID 1 or 5/6 to protect against disk failure, but those drives are all in one location (and an outage can take them all out). Aurora instead relies on its networked storage layer for redundancy. It keeps multiple copies on independent nodes and AZs, effectively achieving a level of reliability beyond what RAID on a single server could do. For example, a RAID 1 or RAID 5 might tolerate one disk failure; Aurora’s six-way replication can tolerate an entire data center failure plus additional disk failures ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=A%20six,independent%20AZs%20in%20the%20region)). Another difference is performance: hardware RAID can stripe for throughput, but still you’re limited by the bandwidth of one machine or a single storage controller. Aurora’s “striping” is over many servers and network paths, allowing much higher aggregate I/O. Also, Aurora’s storage intelligence (understanding redo logs and pages) goes beyond RAID’s block-level mirroring – it performs **database-specific optimizations** (like not writing the same page twice for crash safety, etc.) that RAID can’t do. In short, Aurora replaces the need for RAID with a more powerful **distributed replication mechanism** built into the database storage service. This is essentially a **proprietary distributed storage system** engineered for Aurora’s needs, rather than any off-the-shelf RAID technology ([PowerPoint Presentation](https://www.percona.com/sites/default/files/ple19-slides/ple19-deep-dive-amazon-aurora.pdf#:~:text=Aurora%20scale,written%20in%2010GB%20%E2%80%9Cprotection%20groups%E2%80%9D)).

**Compared to database sharding:** Sharding typically means splitting a database’s data across multiple independent database instances (each with its own storage) based on some key or table, in order to scale out when one instance can’t handle the load or size. Aurora’s approach is different – it gives you the ability to scale _up to very large sizes and high throughput on a single database cluster_, so that many use cases won’t require explicit sharding. The storage layer’s auto-distribution of data across nodes acts kind of like “under the hood” sharding, but it’s **completely transparent to the user**. You still interact with one logical database and one endpoint. You don’t have to manage partitioning logic or handle cross-shard queries – the Aurora storage hides that complexity by presenting a unified volume. In practice, this means you can grow an Aurora database to dozens of terabytes and high read/write rates without redesigning your app for sharding ([Amazon Aurora and Local Storage - DEV Community](https://dev.to/aws-builders/amazon-aurora-and-local-storage-50of#:~:text=Sounds%20familiar%3F%20%E2%80%9CNo%20space%20left,push%20the%20local%20storage%20limits)).

That said, Aurora’s architecture focuses on **scaling storage and read capacity** rather than scaling out a single query across multiple compute nodes (aside from the special Parallel Query feature). All writes go through the single primary node, and a given query is handled by one instance. If your workload grows beyond what even a very large Aurora instance can handle (CPU or memory-wise), you might still need to introduce sharding at the application level or use multiple Aurora clusters. But compared to a traditional setup, Aurora pushes that sharding boundary much farther out by eliminating the usual limits on storage size, IOPS, and replica lag. Essentially, Aurora gives some of the benefits of sharding (like distributing storage and read load) **without the application complexity**, up to a very high scale.

In summary, Amazon Aurora’s storage engine is a **cloud-native, distributed storage system** built specifically for databases. It doesn’t rely on standard RAID arrays, and it doesn’t require the user to shard data across multiple databases. Instead, Aurora automatically spreads data across many storage nodes with **six-way replication, quorum commits, and transparent fault tolerance** ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=The%20Aurora%20quorum%20In%20Aurora%2C,hasn%E2%80%99t%20responded%20for%20a%20while)) ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=A%20six,independent%20AZs%20in%20the%20region)). This design handles large volumes and high throughput by scaling storage out, while presenting a single consistent database to the application. The result is a highly durable, available, and performant storage foundation for MySQL/PostgreSQL-compatible Aurora databases – one that **combines the best of high-end commercial databases with the scale-out resiliency of cloud storage solutions**. Through techniques like log-based writes, distributed segment storage, and multi-AZ replication, Aurora’s storage layer achieves performance and reliability characteristics beyond what traditional monolithic databases (with local RAID) or simple sharded setups can offer ([ Why Choose Amazon Aurora Over Regular RDS? | Shikisoft Blog](https://blog.shikisoft.com/why-choose-aurora-over-regular-rds/#:~:text=Aurora%E2%80%99s%20performance%20is%20better%20and,more%20consistent%20than%20Amazon%20RDS)) ([PowerPoint Presentation](https://www.percona.com/sites/default/files/ple19-slides/ple19-deep-dive-amazon-aurora.pdf#:~:text=Aurora%20scale,written%20in%2010GB%20%E2%80%9Cprotection%20groups%E2%80%9D)).

**Sources:**

1. Amazon Web Services – _Amazon Aurora Storage Architecture_ (AWS Aurora User Guide) ([Amazon Aurora storage - Amazon Aurora](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#:~:text=Aurora%20data%20is%20stored%20in,The%20amount%20of%20replication%20is)) ([Amazon Aurora storage - Amazon Aurora](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#:~:text=The%20Aurora%20shared%20storage%20architecture,does%20Aurora%20remove%20the%20data))
2. AWS Database Blog – _Amazon Aurora under the hood: quorum model and fault tolerance_ ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=The%20Aurora%20quorum%20In%20Aurora%2C,hasn%E2%80%99t%20responded%20for%20a%20while)) ([Amazon Aurora under the hood: quorums and correlated failure | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/#:~:text=A%20six,independent%20AZs%20in%20the%20region))
3. AWS Database Blog – _Amazon Aurora under the hood: quorum reads & writes_ ([Amazon Aurora Under the Hood: Quorum Reads and Mutating State | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-reads-and-mutating-state/#:~:text=However%2C%20Aurora%20avoids%20quorum%20amplification,Neither%20are%20possible%20for%20reads)) ([Amazon Aurora Under the Hood: Quorum Reads and Mutating State | AWS Database Blog](https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-reads-and-mutating-state/#:~:text=read%20replicas,without%20loss%20of%20data%20or))
4. AsquareB Blog – _Amazon Aurora Storage – Quick Notes_ (implementation details on log writes and segments) ([Amazon Aurora Storage - Quick Notes](http://blog.asquareb.com/blog/2021/01/10/amazon-aurora-storage/#:~:text=to%20traditional%20database%20where%20log,storage%20nodes%20which%20are%20EC2)) ([Amazon Aurora Storage - Quick Notes](http://blog.asquareb.com/blog/2021/01/10/amazon-aurora-storage/#:~:text=In%20the%20background%20the%20storage,When%20there%20are%20multiple))
5. Percona Live Slides – _Deep Dive on Amazon Aurora_ (Aurora architecture overview) ([PowerPoint Presentation](https://www.percona.com/sites/default/files/ple19-slides/ple19-deep-dive-amazon-aurora.pdf#:~:text=system%20designed%20for%20databases%20Storage,to%2064TB%20Shared%20storage%20volume))
6. Shikisoft Tech Blog – _Why Choose Amazon Aurora over RDS?_ (Aurora vs RDS differences) ([ Why Choose Amazon Aurora Over Regular RDS? | Shikisoft Blog](https://blog.shikisoft.com/why-choose-aurora-over-regular-rds/#:~:text=storage%20is%20separate%20from%20the,will%20still%20have%206%20copies))
