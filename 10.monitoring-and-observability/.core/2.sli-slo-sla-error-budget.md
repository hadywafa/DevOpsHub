# 🌡️ **SLI, SLO, SLA & Error Budgets**

## 🔴 **Why These Concepts Exist**

Let’s start with the problem 👇  
In traditional ops, teams monitor tons of metrics: CPU, latency, disk space, etc.  
But **none of these tell you if your users are actually happy.**

Example:

> “My CPU is 80%, but users are fine.”  
> “My CPU is 20%, but users can’t log in.” 😬

So, Google’s SRE philosophy said:

> “We need measurable goals that represent _user happiness and reliability._”

That’s where **SLIs**, **SLOs**, **SLAs**, and **Error Budgets** come in.

---

## 🏛️ **The Three Reliability Terms (SLI → SLO → SLA)**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Term       | Full Form               | Focus       | Description                                                                     |
| ---------- | ----------------------- | ----------- | ------------------------------------------------------------------------------- |
| 📊 **SLI** | Service Level Indicator | Measurement | A metric that tells you how your service performs (e.g., success rate, latency) |
| 🎯 **SLO** | Service Level Objective | Target      | The desired performance target (e.g., 99.9% of requests succeed)                |
| 🤝 **SLA** | Service Level Agreement | Contract    | A **business agreement** with customers, often with penalties if not met        |

</div>

---

Let’s visualize this relationship 👇

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

```mermaid
flowchart LR
A[SLI: Measured Data] --> B[SLO: Target Goal]
B --> C[SLA: Formal Agreement]
```

</div>

---

## 📊 **1. SLI (Service Level Indicator)**

### 🧠 Definition:

> A **quantitative measurement** of a service’s reliability or performance, from the _user’s perspective._

SLI = Actual measured value.

---

<div align="center" style="background-color:#111721; border-radius: 10px; border: 2px solid">
    <img src="image/2.sli-slo-sla-error-budget/1761847548110.png" alt="SLI, SLO, SLA & Error Budgets" style="width: 60%">
</div>

---

### 🧮 Examples:

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Category        | Example SLI                          | Description                     |
| --------------- | ------------------------------------ | ------------------------------- |
| 🕒 Latency      | % of requests                        | Measures how fast responses are |
| 💥 Errors       | % of failed requests                 | Measures reliability            |
| 📶 Availability | Successful requests ÷ total requests | Measures uptime                 |
| 🧱 Saturation   | CPU utilization, memory              | Measures load on system         |

</div>

---

🧩 **SLI = good_events / total_events**

Example:  
If 9,950 of 10,000 requests succeeded:  
→ SLI = 9,950 / 10,000 = **99.5 % success rate**

---

## 🎯 **2. SLO (Service Level Objective)**

### 🧠 Definition:

> An **SLO is the target value or range** of your SLI that defines acceptable reliability.

It’s what you _aim for_, not what you always achieve.

Example:

> “Our service should have 99.9 % availability over 30 days.”

If your measured SLI = 99.92 % → ✅ Within SLO.  
If it drops to 99.6 % → ❌ SLO violated.

---

### 💡 Choosing the Right SLO

You pick an SLO based on:

1. **User tolerance** (How much failure users can accept)
2. **Business impact** (What downtime costs money)
3. **Engineering cost** (Higher reliability = more \$\$\$)

Example trade-off:

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Availability | Monthly Downtime | Cost & Complexity |
| ------------ | ---------------- | ----------------- |
| 99 %         | 7.3 hours        | Low               |
| 99.9 %       | 43 minutes       | Medium            |
| 99.99 %      | 4 minutes        | High              |
| 99.999 %     | 26 seconds       | Very High 🚀      |

</div>

---

> Each extra “9” costs a fortune.

---

## 🤝 **3. SLA (Service Level Agreement)**

### 🧠 Definition:

> An **SLA** is a **formal agreement between the provider and the customer** about the expected service level, often tied to **penalties**.

It’s like the _legal cousin_ of SLO.

- SLAs are for **external communication** (customers, contracts).
- SLOs are for **internal goals** (engineering teams).

### Example:

> SLA: “If uptime drops below 99.9 % this month, we refund 10 % of the bill.”

So:

- **SLI** = “We achieved 99.7 % uptime.”
- **SLO** = “We target 99.9 % uptime.”
- **SLA** = “We refund users if uptime < 99.9 %.”

---

## 🌋 **The Error Budget — The Secret Weapon of SREs**

Now comes the magic 💥

### 🧠 Definition:

> An **Error Budget** is the amount of unreliability your system is allowed before violating the SLO.

It’s literally the **difference between perfection and your SLO**.

### 📐 Formula:

```ini
Error Budget = 100 % – SLO %
```

Example:

- SLO = 99.9 %
- Error Budget = 0.1 %

This means:

> You can afford 0.1 % of failures before you must stop risky deployments.

### 🎯 Purpose

- Balances **stability vs. innovation**
- When reliability is good → engineers can release fast
- When SLO is violated → freeze releases, fix stability first

This is the **core feedback loop** of SRE.

---

### 📈 Example in Action

Let’s say your API has SLO = 99.9 % (monthly window).

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Metric                          | Value     |
| ------------------------------- | --------- |
| Total requests                  | 1,000,000 |
| Allowed failed requests (0.1 %) | 1,000     |

</div>

If you exceed 1,000 failed requests → error budget exhausted → pause new releases until reliability improves.

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

```mermaid
graph TD
A[User Traffic] --> B[Monitoring]
B --> C[SLI Calculation]
C --> D[SLO Comparison]
D --> E{Error Budget OK?}
E -->|Yes| F[Deploy new features 🚀]
E -->|No| G[Focus on reliability 🔧]
```

</div>

---

## ⁉️ **Why Error Budgets Are Game-Changers**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Benefit                  | Description                                     |
| ------------------------ | ----------------------------------------------- |
| ⚖️ Balances priorities   | Aligns Dev & Ops — “move fast but not too fast” |
| 🚨 Reduces alert fatigue | Alert only when you’re burning the budget       |
| 📅 Improves planning     | Reliability becomes measurable & trackable      |
| 🤝 Builds trust          | Business + engineering speak same language      |

</div>

---

This creates a **data-driven reliability culture** — not emotional debates like:

> “We can’t release, it’s risky!”
> Instead → “We’ve used 80 % of our error budget — let’s stabilize.”

---

## 📝 **Real-World Example — Google Search Service**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Concept      | Example                                         |
| ------------ | ----------------------------------------------- |
| SLI          | % of search queries under 400 ms                |
| SLO          | 99.99 % of queries under 400 ms                 |
| SLA          | 99.9 % uptime guaranteed to enterprise partners |
| Error Budget | 0.01 % (≈ 4.3 minutes downtime per month)       |

</div>

---

If latency spikes cause 0.02 % of slow queries → they’ve burned double the budget and must pause feature rollouts.

---

## 🔔 **Alerting with SLOs and Budgets**

Traditional alerting:

> “CPU > 80 %” → noisy and irrelevant.

SLO-based alerting:

> “We’ve used 70 % of our error budget in 2 days.”

That’s what _modern SRE monitoring systems_ like **Prometheus**, **Datadog**, or **CloudWatch SLOs** now support — alerting on **user-impacting reliability degradation**, not low-level metrics.

---

## 📝 **Example: SLI, SLO, SLA Combined**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Concept | Example                                  | Analogy                           |
| ------- | ---------------------------------------- | --------------------------------- |
| **SLI** | 99.7 % uptime measured                   | Thermometer — what you observe    |
| **SLO** | Target 99.9 % uptime                     | Goal — what you want              |
| **SLA** | 99.9 % uptime guaranteed, or refund 10 % | Legal contract — what you promise |

</div>

---

## 🖼️ **Visualization — The SRE Reliability Loop**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

```mermaid
flowchart LR
A[Monitor SLIs] --> B[Compare to SLOs]
B --> C[Calculate Error Budget]
C --> D{Budget Remaining?}
D -->|Yes| E[Deploy new features]
D -->|No| F[Stabilize system, fix reliability]
E --> G[Measure again]
F --> G
G --> A
```

</div>

---

This loop ensures you:

- Continuously measure user experience
- Quantify acceptable failure
- Balance innovation with reliability

---

## 🏁 **Summary Table — The Complete Picture**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Concept          | Purpose                           | Who Uses It      | Typical Value | Example                    |
| ---------------- | --------------------------------- | ---------------- | ------------- | -------------------------- |
| **SLI**          | Measure actual performance        | Monitoring tools | Real-time     | 99.7 % successful requests |
| **SLO**          | Set target for reliability        | Engineers, SREs  | Target        | 99.9 % uptime per month    |
| **SLA**          | Define contractual guarantees     | Business/Legal   | Minimum       | 99.9 % uptime or refund    |
| **Error Budget** | Quantify acceptable unreliability | SRE/DevOps       | Difference    | 0.1 % failures allowed     |

</div>

---

## 🧠 **Final Takeaway**

- **Monitoring** tells you _what happened_
- **Observability** helps you _understand why_
- **SLIs/SLOs/SLAs** make it _measurable and meaningful_
- **Error Budgets** make it _actionable_

In short:

> **Observability shows the map.  
> SLOs define the destination.  
> Error budgets tell you when to stop for repairs.**
