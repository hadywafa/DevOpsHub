# 🎙️ **Transcript: Explaining My Azure DevOps + AKS CI/CD Pipeline**

> **Sure.**
> In my current role, I’ve been working on setting up and managing a complete CI/CD pipeline on **Azure DevOps** integrated with **Azure Kubernetes Service (AKS)**.
>
> We follow a **Helm-based deployment model** with **two environments — Dev and Prod**, and our branches follow the same naming convention:
> the `dev` branch automatically deploys to the **Dev AKS environment**,
> and the `main` branch is used for **production** deployment after review and approval.

---

## **1️⃣ Build and Image Pipeline (Azure DevOps CI)**

> Whenever developers push code or open a pull request on `dev` or `main`, our Azure DevOps pipeline triggers.
>
> The pipeline performs a full CI cycle:
>
> 1. **Checkout the code**
> 2. **Run SonarQube** to analyze code quality and enforce quality gates.
> 3. **Run Trivy** to scan both the source dependencies and later the built image for vulnerabilities.
>
> If both checks pass, we move to the **Docker build stage**.

---

### 🧩 **Image Building and Tagging Strategy**

> We build a Docker image for each microservice and tag it using a **semantic version combined with the short Git SHA**, for example:
> `myapp:1.3.0-4f8d2c1`
>
> This gives us an immutable reference that’s easy to trace back to a specific commit.
>
> In addition, we maintain a **moving tag** like `myapp:dev-latest` or `myapp:prod-latest` for the respective environments.
>
> The immutable tag (`1.3.0-4f8d2c1`) is used in **prod** deployments to ensure full traceability,
> while the **latest tag** helps Dev quickly test iterative builds without updating Helm values every commit.

---

### 🏷️ **Push to Azure Container Registry (ACR)**

> After building the image, the pipeline pushes it to **Azure Container Registry (ACR)**.
>
> Azure DevOps has a built-in ACR task that authenticates using a service connection with a managed identity — so no credentials are exposed in the pipeline.
>
> The registry acts as the single source of truth for all container images.

---

## **2️⃣ Deployment Flow (Push-Based Helm Deployment)**

> Once the image is pushed to ACR, the same pipeline (or a subsequent release pipeline) takes care of deployment using **Helm charts**.
>
> We follow a **push-based approach**, meaning the pipeline connects to the AKS cluster using a service connection and runs Helm commands directly to apply the deployment.

---

### 🧩 **Pipeline Deployment Steps**

1. **Pulls the Helm chart** (either from our Helm repo or a shared charts directory).
2. **Updates values.yaml** dynamically with:

   - The **new image tag**
   - Environment-specific configurations (replicas, secrets, ingress hostname, etc.)

3. **Packages the chart** and **deploys** it using:

   ```bash
   helm upgrade --install myapp ./charts/myapp -f ./environments/dev/values.yaml --namespace dev
   ```

4. For production (`main` branch), the pipeline requires **manual approval** before deploying to AKS:

   ```bash
   helm upgrade --install myapp ./charts/myapp -f ./environments/prod/values.yaml --namespace prod
   ```

> This gives us automated Dev deployments and controlled Production rollouts.

---

### 🧱 **Why Helm Fits Perfectly**

> Helm gives us consistent templating and versioning.
> Each environment has its own `values.yaml`, so we can adjust replica counts, secrets, and ingress domains easily without changing core manifests.
>
> It also integrates nicely with Azure DevOps variable groups — we can inject secrets, URLs, or connection strings at runtime.

---

## **3️⃣ Database — Migration to Azure SQL Server**

> Initially, our database was hosted on an **Azure VM running SQL Server**,
> but we migrated to **Azure SQL Database (PaaS)** for multiple reasons:
>
> - **Scalability:** no need to manage VM sizing or OS updates.
> - **Backup & HA:** built-in high availability and automated backups.
> - **Security:** managed identity integration and transparent data encryption.
> - **Cost efficiency:** we only pay for DTUs/vCores, not full VM uptime.
>
> We used the **Azure Database Migration Service (DMS)** to migrate the schema and data online with minimal downtime.
>
> After migration, our AKS workloads connect to Azure SQL via a private endpoint — meaning the database is not publicly accessible, and all traffic stays within the Azure network.

---

## **4️⃣ Persistent Volume — Azure Files for Shared Storage**

> **Sure.**
> In our AKS setup, we don’t host any databases inside the cluster — our main database runs on **Azure SQL Server**, fully managed by Azure.
>
> But some of our applications still need **shared file storage** for things like **user uploads**, **generated reports**, or **exported CSV files** that multiple pods need to read and write.
>
> For that, we use **Azure Files** through the **CSI driver**, integrated natively with AKS.

---

### 🧩 Why Azure Files?

> We picked **Azure Files** because it provides:
>
> - **SMB or NFS share access**, so multiple pods can mount the same volume (`ReadWriteMany`).
> - Fully managed storage with **encryption**, **backups**, and **auto-scaling** handled by Azure.
> - Seamless integration with **Kubernetes** — no manual provisioning, it’s dynamic.

---

### 🏗️ How it works in our pipeline

> In our **Helm chart**, we define a **PersistentVolumeClaim (PVC)** that uses the **Azure Files StorageClass**.
> When we deploy through our **Azure DevOps pipeline**, Helm automatically requests the PVC, and the **CSI driver** provisions an Azure File Share behind the scenes.
>
> Every pod mounts that PVC under `/app/media` or `/app/uploads`, depending on the service.

---

### 🧾 Example (YAML)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-media
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: azurefile-csi-premium
  resources:
    requests:
      storage: 100Gi
```

Then, inside the **deployment**:

```yaml
volumeMounts:
  - name: shared-media
    mountPath: /app/uploads
volumes:
  - name: shared-media
    persistentVolumeClaim:
      claimName: shared-media
```

---

### 🌍 Real example

> For example, our reporting service generates PDFs and stores them on this shared volume.
> The frontend pods can serve those files directly, and a cleanup job runs nightly to archive older files to **Azure Blob Storage**.
>
> In **dev**, we use the standard tier (lower cost).
> In **prod**, we use **Premium_LRS** for higher IOPS and throughput.

---

### 🧠 Common Interview Questions & Answers

**Q1: Why use Azure Files instead of Azure Disks?**

> Because we need multiple pods to read/write at the same time. Azure Disks only support `ReadWriteOnce`, but Azure Files supports `ReadWriteMany`.

**Q2: How is it secured?**

> The Azure File Share is mounted using **Kubernetes Secrets** created by the **CSI driver**.
> Everything is **encrypted at rest and in transit** automatically.

**Q3: How do you back it up or retain files?**

> We use **Azure File Share snapshots**, which are managed by Azure itself.
> For old files, we move them to **Azure Blob Storage** using a cron job.

**Q4: What happens if you scale pods?**

> All replicas mount the same Azure File Share, so new pods automatically see the same files — no extra config needed.

---

## **5️⃣ Networking Flow (from DNS to Application)**

> Let me explain the full flow when a user accesses our application.

### 🌍 **Step-by-Step Traffic Flow**

1. **DNS Resolution:**
   The client enters `app.mycompany.com`.
   This DNS record is managed in **Azure DNS Zone** and points to our **Azure Application Gateway Public IP** or **Azure Public Load Balancer**, depending on the setup.

2. **Ingress Controller:**
   In AKS, we use the **NGINX Ingress Controller** deployed via Helm.
   The load balancer forwards the request to NGINX based on the hostname.

3. **Ingress + TLS:**
   The ingress definition routes traffic to the correct service (e.g., `myapp-service`) inside the cluster.
   For HTTPS, we use **cert-manager with Let’s Encrypt**.
   Cert-manager automatically provisions and renews TLS certificates.
   We configured it once in our Helm chart:

   ```yaml
   ingress:
     enabled: true
     hosts:
       - host: app.mycompany.com
         paths: ["/"]
     tls:
       - secretName: myapp-tls
         hosts:
           - app.mycompany.com
   ```

   cert-manager requests a Let’s Encrypt certificate for `app.mycompany.com` and stores it as a Kubernetes secret.

4. **Service and Pod Resolution:**
   The Ingress sends traffic to the **ClusterIP service**,
   which then routes requests to the pods running the app in that namespace.

5. **Response Flow:**
   The response travels back through the service → ingress → load balancer → DNS → client.
   Everything is encrypted end-to-end using HTTPS.

---

## **6️⃣ Monitoring, Security, and Scaling**

> For monitoring, we use **Azure Monitor** and **Container Insights** to collect AKS metrics,
> and **Prometheus/Grafana** (deployed via Helm) for detailed dashboards inside the cluster.
>
> Scaling is handled using **Horizontal Pod Autoscaler (HPA)**,
> which adjusts replicas based on CPU and memory utilization.
>
> We also have **Pod Disruption Budgets** and **Liveness/Readiness probes** to ensure stability during updates.

---

## **7️⃣ Security and Access Control**

> - **Managed Identities** are used for connecting AKS pods to Azure SQL securely — no credentials in configs.
> - **Role-Based Access Control (RBAC)** is applied in both Azure DevOps and AKS namespaces.
> - **Trivy** ensures no vulnerable dependencies or base images reach the cluster.
> - **Network Policies** isolate traffic between namespaces, especially between dev and prod.

---

## 🧩 **Final Summary**

> So to summarize our Azure pipeline flow:
>
> - Code pushed to Azure DevOps triggers CI →
>   runs **SonarQube** and **Trivy**, builds and tags a Docker image →
>   pushes it to **ACR** →
>   updates **Helm values** with the new image tag →
>   deploys to AKS via Helm for `dev`, or waits for approval for `prod`.
>
> The application sits behind an **NGINX ingress controller**,
> TLS is handled automatically with **cert-manager + Let’s Encrypt**,
> and all services connect securely to **Azure SQL Database** via private endpoints.
>
> The result is a secure, automated, and scalable CI/CD workflow with full observability and compliance.

---

## 🎯 **Common Interview Questions + Perfect Answers**

### ❓**1. Why did you choose Helm for deployments instead of plain YAML or Kustomize?**

> Helm provides versioned releases, parameterized values, and rollback capability.
> We can reuse the same chart for dev and prod just by switching values.yaml files, which makes it cleaner and more maintainable.

---

### ❓**2. Why a push-based deployment and not Argo CD (pull-based)?**

> Since our team already uses Azure DevOps end-to-end, a push-based Helm pipeline was faster to integrate.
> It gives us centralized visibility and control for approvals inside Azure DevOps.
>
> That said, for more advanced GitOps or multi-cluster scenarios, a pull-based approach like ArgoCD would be a great next step.

---

### ❓**3. Why did you migrate SQL Server from VM to Azure SQL Database?**

> Mainly to offload infrastructure management.
> Azure SQL gives built-in HA, automated backups, patching, and better integration with managed identities.
> It reduces operational overhead and improves security posture since the database runs on a private endpoint.

---

### ❓**4. How do you manage image versioning between environments?**

> We use a two-tier tagging strategy:
>
> - Immutable tags (`1.3.0-4f8d2c1`) for prod — to guarantee reproducibility.
> - Moving tags (`dev-latest`) for fast iteration in dev.
>
> The Helm chart always pulls the appropriate tag based on environment.

---

### ❓**5. How is TLS managed and renewed?**

> We use **cert-manager** with the **Let’s Encrypt cluster issuer** configured via Helm.
> It automatically issues and renews certificates, so we don’t handle them manually.
> The certs are stored as Kubernetes secrets and mounted automatically by the ingress.

---

### ❓**6. How do you ensure security in your pipeline and AKS cluster?**

> - Use **Managed Identity** for all connections (no hardcoded secrets).
> - **SonarQube and Trivy** scans in CI.
> - **Network Policies** between namespaces.
> - **RBAC** roles for least privilege access.
> - **TLS everywhere** — ingress and SQL connections.

---

### ❓**7. What’s your rollback strategy?**

> We use Helm’s built-in rollback capability.
> Each release is versioned, so we can do `helm rollback myapp <previous-release>` instantly if something fails.
> Database migrations are version-controlled too, and we take snapshots before major releases.

---

### ❓**8. How do you handle scaling?**

> We use **HPA** for pods and can scale manually for node pools through AKS.
> In the future, we might integrate **KEDA** for event-driven scaling if workloads become queue-based.

---

### ❓**9. What happens when a user accesses your domain?**

> The DNS resolves to the public IP of the load balancer →
> traffic reaches the NGINX ingress →
> routed to the correct Kubernetes service →
> the pod handles it and returns the response →
> all encrypted with TLS via Let’s Encrypt.

---

### ❓**10. How do you monitor and observe the system?**

> We use **Azure Monitor + Container Insights** for cluster-level telemetry,
> and **Prometheus + Grafana** inside AKS for detailed dashboards.
> Alerts are integrated into Microsoft Teams for real-time notifications.

---

## 💬 **Final Wrap-up Sentence**

> So overall, this Azure DevOps + AKS setup gives us an automated, secure, and observable CI/CD workflow —
> from code commit to container deployment, backed by quality and security gates, Helm-based consistency, and a reliable PaaS database.
>
> It’s fully traceable, easy to roll back, and scalable as we grow.
