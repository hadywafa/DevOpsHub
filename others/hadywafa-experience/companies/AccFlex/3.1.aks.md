# ðŸŽ™ï¸ **Transcript: Explaining My Azure DevOps + AKS CI/CD Pipeline**

> **Sure.**
> In my current role, Iâ€™ve been working on setting up and managing a complete CI/CD pipeline on **Azure DevOps** integrated with **Azure Kubernetes Service (AKS)**.
>
> We follow a **Helm-based deployment model** with **two environments â€” Dev and Prod**, and our branches follow the same naming convention:
> the `dev` branch automatically deploys to the **Dev AKS environment**,
> and the `main` branch is used for **production** deployment after review and approval.

---

## **1ï¸âƒ£ Build and Image Pipeline (Azure DevOps CI)**

> Whenever developers push code or open a pull request on `dev` or `main`, our Azure DevOps pipeline triggers.
>
> The pipeline performs a full CI cycle:
>
> 1. **Checkout the code**
> 2. **Run SonarQube** to analyze code quality and enforce quality gates.
> 3. **Run Trivy** to scan both the source dependencies and later the built image for vulnerabilities.
>
> If both checks pass, we move to the **Docker build stage**.

---

### ðŸ§© **Image Building and Tagging Strategy**

> We build a Docker image for each microservice and tag it using a **semantic version combined with the short Git SHA**, for example:
> `myapp:1.3.0-4f8d2c1`
>
> This gives us an immutable reference thatâ€™s easy to trace back to a specific commit.
>
> In addition, we maintain a **moving tag** like `myapp:dev-latest` or `myapp:prod-latest` for the respective environments.
>
> The immutable tag (`1.3.0-4f8d2c1`) is used in **prod** deployments to ensure full traceability,
> while the **latest tag** helps Dev quickly test iterative builds without updating Helm values every commit.

---

### ðŸ·ï¸ **Push to Azure Container Registry (ACR)**

> After building the image, the pipeline pushes it to **Azure Container Registry (ACR)**.
>
> Azure DevOps has a built-in ACR task that authenticates using a service connection with a managed identity â€” so no credentials are exposed in the pipeline.
>
> The registry acts as the single source of truth for all container images.

---

## **2ï¸âƒ£ Deployment Flow (Push-Based Helm Deployment)**

> Once the image is pushed to ACR, the same pipeline (or a subsequent release pipeline) takes care of deployment using **Helm charts**.
>
> We follow a **push-based approach**, meaning the pipeline connects to the AKS cluster using a service connection and runs Helm commands directly to apply the deployment.

---

### ðŸ§© **Pipeline Deployment Steps**

1. **Pulls the Helm chart** (either from our Helm repo or a shared charts directory).
2. **Updates values.yaml** dynamically with:

   - The **new image tag**
   - Environment-specific configurations (replicas, secrets, ingress hostname, etc.)

3. **Packages the chart** and **deploys** it using:

   ```bash
   helm upgrade --install myapp ./charts/myapp -f ./environments/dev/values.yaml --namespace dev
   ```

4. For production (`main` branch), the pipeline requires **manual approval** before deploying to AKS:

   ```bash
   helm upgrade --install myapp ./charts/myapp -f ./environments/prod/values.yaml --namespace prod
   ```

> This gives us automated Dev deployments and controlled Production rollouts.

---

### ðŸ§± **Why Helm Fits Perfectly**

> Helm gives us consistent templating and versioning.
> Each environment has its own `values.yaml`, so we can adjust replica counts, secrets, and ingress domains easily without changing core manifests.
>
> It also integrates nicely with Azure DevOps variable groups â€” we can inject secrets, URLs, or connection strings at runtime.

---

## **3ï¸âƒ£ Database â€” Migration to Azure SQL Server**

> Initially, our database was hosted on an **Azure VM running SQL Server**,
> but we migrated to **Azure SQL Database (PaaS)** for multiple reasons:
>
> - **Scalability:** no need to manage VM sizing or OS updates.
> - **Backup & HA:** built-in high availability and automated backups.
> - **Security:** managed identity integration and transparent data encryption.
> - **Cost efficiency:** we only pay for DTUs/vCores, not full VM uptime.
>
> We used the **Azure Database Migration Service (DMS)** to migrate the schema and data online with minimal downtime.
>
> After migration, our AKS workloads connect to Azure SQL via a private endpoint â€” meaning the database is not publicly accessible, and all traffic stays within the Azure network.

---

## **4ï¸âƒ£ Persistent Volume â€” Azure Files for Shared Storage**

> **Sure.**
> In our AKS setup, we donâ€™t host any databases inside the cluster â€” our main database runs on **Azure SQL Server**, fully managed by Azure.
>
> But some of our applications still need **shared file storage** for things like **user uploads**, **generated reports**, or **exported CSV files** that multiple pods need to read and write.
>
> For that, we use **Azure Files** through the **CSI driver**, integrated natively with AKS.

---

### ðŸ§© Why Azure Files?

> We picked **Azure Files** because it provides:
>
> - **SMB or NFS share access**, so multiple pods can mount the same volume (`ReadWriteMany`).
> - Fully managed storage with **encryption**, **backups**, and **auto-scaling** handled by Azure.
> - Seamless integration with **Kubernetes** â€” no manual provisioning, itâ€™s dynamic.

---

### ðŸ—ï¸ How it works in our pipeline

> In our **Helm chart**, we define a **PersistentVolumeClaim (PVC)** that uses the **Azure Files StorageClass**.
> When we deploy through our **Azure DevOps pipeline**, Helm automatically requests the PVC, and the **CSI driver** provisions an Azure File Share behind the scenes.
>
> Every pod mounts that PVC under `/app/media` or `/app/uploads`, depending on the service.

---

### ðŸ§¾ Example (YAML)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-media
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: azurefile-csi-premium
  resources:
    requests:
      storage: 100Gi
```

Then, inside the **deployment**:

```yaml
volumeMounts:
  - name: shared-media
    mountPath: /app/uploads
volumes:
  - name: shared-media
    persistentVolumeClaim:
      claimName: shared-media
```

---

### ðŸŒ Real example

> For example, our reporting service generates PDFs and stores them on this shared volume.
> The frontend pods can serve those files directly, and a cleanup job runs nightly to archive older files to **Azure Blob Storage**.
>
> In **dev**, we use the standard tier (lower cost).
> In **prod**, we use **Premium_LRS** for higher IOPS and throughput.

---

### ðŸ§  Common Interview Questions & Answers

**Q1: Why use Azure Files instead of Azure Disks?**

> Because we need multiple pods to read/write at the same time. Azure Disks only support `ReadWriteOnce`, but Azure Files supports `ReadWriteMany`.

**Q2: How is it secured?**

> The Azure File Share is mounted using **Kubernetes Secrets** created by the **CSI driver**.
> Everything is **encrypted at rest and in transit** automatically.

**Q3: How do you back it up or retain files?**

> We use **Azure File Share snapshots**, which are managed by Azure itself.
> For old files, we move them to **Azure Blob Storage** using a cron job.

**Q4: What happens if you scale pods?**

> All replicas mount the same Azure File Share, so new pods automatically see the same files â€” no extra config needed.

---

## **5ï¸âƒ£ Networking Flow (from DNS to Application)**

> Let me explain the full flow when a user accesses our application.

### ðŸŒ **Step-by-Step Traffic Flow**

1. **DNS Resolution:**
   The client enters `app.mycompany.com`.
   This DNS record is managed in **Azure DNS Zone** and points to our **Azure Application Gateway Public IP** or **Azure Public Load Balancer**, depending on the setup.

2. **Ingress Controller:**
   In AKS, we use the **NGINX Ingress Controller** deployed via Helm.
   The load balancer forwards the request to NGINX based on the hostname.

3. **Ingress + TLS:**
   The ingress definition routes traffic to the correct service (e.g., `myapp-service`) inside the cluster.
   For HTTPS, we use **cert-manager with Letâ€™s Encrypt**.
   Cert-manager automatically provisions and renews TLS certificates.
   We configured it once in our Helm chart:

   ```yaml
   ingress:
     enabled: true
     hosts:
       - host: app.mycompany.com
         paths: ["/"]
     tls:
       - secretName: myapp-tls
         hosts:
           - app.mycompany.com
   ```

   cert-manager requests a Letâ€™s Encrypt certificate for `app.mycompany.com` and stores it as a Kubernetes secret.

4. **Service and Pod Resolution:**
   The Ingress sends traffic to the **ClusterIP service**,
   which then routes requests to the pods running the app in that namespace.

5. **Response Flow:**
   The response travels back through the service â†’ ingress â†’ load balancer â†’ DNS â†’ client.
   Everything is encrypted end-to-end using HTTPS.

---

## **6ï¸âƒ£ Monitoring, Security, and Scaling**

> For monitoring, we use **Azure Monitor** and **Container Insights** to collect AKS metrics,
> and **Prometheus/Grafana** (deployed via Helm) for detailed dashboards inside the cluster.
>
> Scaling is handled using **Horizontal Pod Autoscaler (HPA)**,
> which adjusts replicas based on CPU and memory utilization.
>
> We also have **Pod Disruption Budgets** and **Liveness/Readiness probes** to ensure stability during updates.

---

## **7ï¸âƒ£ Security and Access Control**

> - **Managed Identities** are used for connecting AKS pods to Azure SQL securely â€” no credentials in configs.
> - **Role-Based Access Control (RBAC)** is applied in both Azure DevOps and AKS namespaces.
> - **Trivy** ensures no vulnerable dependencies or base images reach the cluster.
> - **Network Policies** isolate traffic between namespaces, especially between dev and prod.

---

## ðŸ§© **Final Summary**

> So to summarize our Azure pipeline flow:
>
> - Code pushed to Azure DevOps triggers CI â†’
>   runs **SonarQube** and **Trivy**, builds and tags a Docker image â†’
>   pushes it to **ACR** â†’
>   updates **Helm values** with the new image tag â†’
>   deploys to AKS via Helm for `dev`, or waits for approval for `prod`.
>
> The application sits behind an **NGINX ingress controller**,
> TLS is handled automatically with **cert-manager + Letâ€™s Encrypt**,
> and all services connect securely to **Azure SQL Database** via private endpoints.
>
> The result is a secure, automated, and scalable CI/CD workflow with full observability and compliance.

---

## ðŸŽ¯ **Common Interview Questions + Perfect Answers**

### â“**1. Why did you choose Helm for deployments instead of plain YAML or Kustomize?**

> Helm provides versioned releases, parameterized values, and rollback capability.
> We can reuse the same chart for dev and prod just by switching values.yaml files, which makes it cleaner and more maintainable.

---

### â“**2. Why a push-based deployment and not Argo CD (pull-based)?**

> Since our team already uses Azure DevOps end-to-end, a push-based Helm pipeline was faster to integrate.
> It gives us centralized visibility and control for approvals inside Azure DevOps.
>
> That said, for more advanced GitOps or multi-cluster scenarios, a pull-based approach like ArgoCD would be a great next step.

---

### â“**3. Why did you migrate SQL Server from VM to Azure SQL Database?**

> Mainly to offload infrastructure management.
> Azure SQL gives built-in HA, automated backups, patching, and better integration with managed identities.
> It reduces operational overhead and improves security posture since the database runs on a private endpoint.

---

### â“**4. How do you manage image versioning between environments?**

> We use a two-tier tagging strategy:
>
> - Immutable tags (`1.3.0-4f8d2c1`) for prod â€” to guarantee reproducibility.
> - Moving tags (`dev-latest`) for fast iteration in dev.
>
> The Helm chart always pulls the appropriate tag based on environment.

---

### â“**5. How is TLS managed and renewed?**

> We use **cert-manager** with the **Letâ€™s Encrypt cluster issuer** configured via Helm.
> It automatically issues and renews certificates, so we donâ€™t handle them manually.
> The certs are stored as Kubernetes secrets and mounted automatically by the ingress.

---

### â“**6. How do you ensure security in your pipeline and AKS cluster?**

> - Use **Managed Identity** for all connections (no hardcoded secrets).
> - **SonarQube and Trivy** scans in CI.
> - **Network Policies** between namespaces.
> - **RBAC** roles for least privilege access.
> - **TLS everywhere** â€” ingress and SQL connections.

---

### â“**7. Whatâ€™s your rollback strategy?**

> We use Helmâ€™s built-in rollback capability.
> Each release is versioned, so we can do `helm rollback myapp <previous-release>` instantly if something fails.
> Database migrations are version-controlled too, and we take snapshots before major releases.

---

### â“**8. How do you handle scaling?**

> We use **HPA** for pods and can scale manually for node pools through AKS.
> In the future, we might integrate **KEDA** for event-driven scaling if workloads become queue-based.

---

### â“**9. What happens when a user accesses your domain?**

> The DNS resolves to the public IP of the load balancer â†’
> traffic reaches the NGINX ingress â†’
> routed to the correct Kubernetes service â†’
> the pod handles it and returns the response â†’
> all encrypted with TLS via Letâ€™s Encrypt.

---

### â“**10. How do you monitor and observe the system?**

> We use **Azure Monitor + Container Insights** for cluster-level telemetry,
> and **Prometheus + Grafana** inside AKS for detailed dashboards.
> Alerts are integrated into Microsoft Teams for real-time notifications.

---

## ðŸ’¬ **Final Wrap-up Sentence**

> So overall, this Azure DevOps + AKS setup gives us an automated, secure, and observable CI/CD workflow â€”
> from code commit to container deployment, backed by quality and security gates, Helm-based consistency, and a reliable PaaS database.
>
> Itâ€™s fully traceable, easy to roll back, and scalable as we grow.
