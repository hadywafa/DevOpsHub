# 👺 **Evolution of Azure Services**

## 🎙️ Transcript — Hadaf Solutions (Detailed Cloud Journey)

> **Sure.**
> At Hadaf Solutions, my role was mainly **Full Stack Developer (around 60%)** with **DevOps and cloud responsibilities (around 40%)**.
> We were responsible for building and maintaining several internal and client-facing applications — mostly in **.NET Core**, **Angular**, and **SQL Server**, initially all hosted on **Azure Virtual Machines (VMs)** with IIS.

---

### 🏁 Phase 1 — Starting on Azure Virtual Machines

> We started in a very traditional setup — everything was hosted on a couple of **Azure VMs**.
>
> Each VM had **IIS** installed, hosting multiple APIs and frontends together.
> We’d RDP into the server, manually deploy builds, configure bindings, restart IIS — all the classic manual steps.
>
> This setup was okay for a small user base, but as we scaled, it became hard to maintain.
>
> One particular API — we called it **TeamHub** — started getting **very high traffic** from multiple internal systems. It handled real-time employee data and integrations.
>
> The problem was:
>
> - All APIs were hosted on the same VM.
> - When TeamHub experienced high load, it affected everything else (CPU, memory).
> - Scaling vertically (bigger VM) became costly and limited.
>
> That was when I proposed moving **TeamHub** to a **dedicated Azure App Service**.

---

### ☁️ Phase 2 — Transition to Azure App Service (First Modernization Step)

> We chose **Azure App Service** as the first modernization step because it was the **quickest and safest** way to separate the heavy-load API from the VM without rewriting the app.
>
> I containerized nothing yet — just deployed the existing .NET project directly into App Service.
>
> The reasons it made sense:
>
> 1. **Easy scaling:** we could scale horizontally with just a slider (instance count).
> 2. **No server maintenance:** Azure handled the OS, patches, IIS.
> 3. **Quick migration:** we could deploy through **Azure DevOps Pipelines** instead of RDP/manual copying.
> 4. **Zero downtime deployment:** using **staging slots** for swap-based releases.

---

### 🔒 Handling Dependencies (Private APIs & Networking Challenge)

> But TeamHub wasn’t standalone — it depended on several **private APIs** that were still hosted inside the original VM’s private network (for HR, payroll, and attendance).
>
> To handle that securely, I configured **VNet Integration** on the App Service so it could access those internal APIs privately, without going over the public internet.
>
> In addition, I set up **Private Endpoints** for those backend APIs to ensure **end-to-end private connectivity**.
> This way, the new App Service stayed isolated from the public network but still communicated internally through the **Azure VNet**.

**Diagram (conceptually):**

```ini
[Azure App Service: TeamHub]
       │
   (VNet Integration)
       │
[Private APIs on Azure VM via Private Endpoint]
```

> So, effectively, TeamHub became our first **semi-cloud-native component**, and it handled load beautifully — we could auto-scale based on CPU/memory or even request count.

---

### ⚙️ Phase 3 — Experimenting with Azure Container Instances (ACI)

> After that success, we started containerizing smaller background services and jobs using **Azure Container Instances (ACI)**.
>
> ACI was perfect for **short-lived** or **lightweight** workloads — for example:
>
> - Scheduled data synchronization jobs.
> - Report generation tasks.
> - One-off ETL-style workers.
>
> The benefit was that we didn’t have to manage infrastructure — just push a container image, and Azure would run it.

---

> I built Dockerfiles for these tasks, pushed images to **Azure Container Registry (ACR)**, and deployed ACI through **Terraform** or directly via the pipeline.

🧠 **Why ACI fit this use case:**

> We didn’t need a whole Kubernetes cluster for simple batch jobs — ACI is fast, cost-efficient, and serverless for containers.

---

### 🚀 Phase 4 — Scaling with Azure Container Apps (ACP)

> Then we introduced **Azure Container Apps (ACP)** for mid-size microservices that needed autoscaling and internal communication but didn’t justify a full AKS cluster.
>
> For example, our **notification service** and **reporting API** moved to **Container Apps**.
>
> The reasons:
>
> - Easier to deploy than AKS (no cluster management).
> - Supports **Dapr** for service-to-service communication.
> - Built-in **revision control** and **zero-downtime rollouts**.
> - Works great with **Azure DevOps CI/CD pipelines** and **ACR**.

---

> I created pipeline templates that:
>
> - Build the image
> - Tag it (using `dev-<shortSHA>` or `v1.2.3` for prod)
> - Push it to ACR
> - Deploy it directly to Container Apps using the **Azure CLI task** in DevOps.

---

### ☸️ Phase 5 — Full migration to Azure Kubernetes Service (AKS)

> Finally, as we added more services and wanted centralized networking, observability, and fine-grained scaling, we migrated to **Azure Kubernetes Service (AKS)**.
>
> I led this migration — setting up the **cluster with Terraform**, managing **Helm charts**, and defining **dev** and **prod** namespaces.
>
> We used **Azure DevOps** pipelines to deploy Helm releases automatically:
>
> - Commits to `dev` → auto-deploy to `dev` namespace.
> - Merges to `main` → approval gate → deploy to `prod`.
>
> We integrated:
>
> - **Prometheus & Grafana** for monitoring,
> - **Azure Monitor & Log Analytics** for diagnostics,
> - **Cert-Manager + Let’s Encrypt** for ingress TLS,
> - **Azure Key Vault** for secret management,
> - **Azure Files PVCs** for shared media.

---

> So by the end, we had a fully automated CI/CD pipeline, containerized microservices, and secure, observable infrastructure on AKS.

---

### 🧠 Example Interview Dialogue

**Q: Why did you start with App Service before going to AKS?**

> Because App Service gave us a **quick win**. We needed to **isolate the TeamHub API** quickly due to high load, and App Service allowed us to do that **without changing the app architecture**.
>
> It offered **scaling**, **staging slots**, and **easy deployment via Azure DevOps** — all without managing Kubernetes at that stage.
> Once we had more containerized workloads and microservices, it made sense to move to **AKS** for full orchestration.

---

**Q: How did you handle private API communication between App Service and the old VM?**

> I used **VNet Integration** on App Service and created **Private Endpoints** for the backend APIs.
> This ensured **private, secure communication** — no public exposure — while still allowing seamless API calls between App Service and the VM network.

---

**Q: Why did you later use ACI and ACP before AKS?**

> ACI was great for **simple container jobs** — fast, serverless, and low-cost for background tasks.
> ACP bridged the gap — it gave us **autoscaling and revision control** without managing a full Kubernetes cluster.
> Once we needed **multi-service orchestration, advanced ingress, and observability**, we standardized on **AKS**.

---

**Q: What was your role in building this migration path?**

> I was directly involved in each step:
>
> - I set up **pipelines** in Azure DevOps for CI/CD.
> - Wrote **Terraform** scripts for infrastructure.
> - Created **Helm charts** for our microservices.
> - Implemented **monitoring, TLS, and autoscaling** configurations in AKS.
>
> So, I didn’t just deploy code — I helped **design and automate the full infrastructure evolution** from VMs to AKS.

---

**Q: What did you learn from this migration?**

> The main lesson is to **evolve step-by-step**, not jump directly to Kubernetes.
> Each stage (VM → App Service → ACI → ACP → AKS) served a real purpose —
> it helped us improve **scalability**, **security**, and **automation** gradually, without disrupting production.

---

### 🎯 Short Summary to End With

> In short — at Hadaf, I started from legacy IIS apps on Azure VMs and led a full modernization journey:
>
> - **Separated high-load APIs** into App Service with private connectivity.
> - **Containerized smaller workloads** with ACI and ACP.
> - **Standardized microservices** on AKS with full CI/CD automation.
>
> By the end, we had a modern, scalable, DevOps-driven cloud environment that significantly reduced downtime, improved performance, and simplified deployments.
