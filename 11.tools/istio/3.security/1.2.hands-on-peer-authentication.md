# üß† Scenario Overview

We‚Äôll create **3 namespaces**:

1. `team-a` ‚Üí **STRICT mTLS** namespace (Zero Trust team üîê)
2. `team-b` ‚Üí **PERMISSIVE** namespace (in migration üòÖ)
3. `shared` ‚Üí **inherits mesh default** (PERMISSIVE)

And **4 workloads**:

- `team-a/backend` (app label: `app: a-backend`)
- `team-a/legacy` (app label: `app: legacy`) ‚Üí will have **workload-level DISABLE**
- `team-b/client` (app label: `app: b-client`)
- `shared/api` (app label: `app: shared-api`)

We‚Äôll apply:

- Mesh-wide **PERMISSIVE**
- Namespace-level:

  - `team-a` ‚Üí STRICT
  - `team-b` ‚Üí PERMISSIVE (explicit)

- Workload-level:

  - `team-a/legacy` ‚Üí DISABLE (override ns STRICT)

Then we‚Äôll TEST:

- ‚úÖ mTLS-only access where STRICT
- ‚úÖ Plaintext still allowed where PERMISSIVE
- ‚ùå Failures when trying plaintext into STRICT targets
- ‚ùå Failures when calling legacy with mTLS required, etc.

---

## üèóÔ∏è Step 0 ‚Äì Prereqs (Sidecar Mode Setup)

```bash
## Install Istio in default (sidecar) profile
istioctl install --set profile=default -y

## Create namespaces
kubectl create namespace team-a
kubectl create namespace team-b
kubectl create namespace shared

## Enable sidecar injection for all these namespaces
kubectl label namespace team-a istio-injection=enabled
kubectl label namespace team-b istio-injection=enabled
kubectl label namespace shared istio-injection=enabled
```

You can check:

```bash
kubectl get ns -L istio-injection
```

---

## üß± Step 1 ‚Äì Deploy Simple Test Workloads

We‚Äôll use `hashicorp/http-echo` for ultra-simple HTTP responses.

```yaml
## file: workloads.yaml
apiVersion: v1
kind: Service
metadata:
  name: a-backend
  namespace: team-a
spec:
  ports:
    - port: 8080
  selector:
    app: a-backend
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a-backend
  namespace: team-a
spec:
  replicas: 1
  selector:
    matchLabels:
      app: a-backend
  template:
    metadata:
      labels:
        app: a-backend
    spec:
      containers:
        - name: app
          image: hashicorp/http-echo
          args: ["-text=Hello from A-BACKEND"]
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: legacy
  namespace: team-a
spec:
  ports:
    - port: 8080
  selector:
    app: legacy
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: legacy
  namespace: team-a
spec:
  replicas: 1
  selector:
    matchLabels:
      app: legacy
  template:
    metadata:
      labels:
        app: legacy
    spec:
      containers:
        - name: app
          image: hashicorp/http-echo
          args: ["-text=Hello from LEGACY in team-a"]
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: b-client
  namespace: team-b
spec:
  ports:
    - port: 8080
  selector:
    app: b-client
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: b-client
  namespace: team-b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: b-client
  template:
    metadata:
      labels:
        app: b-client
    spec:
      containers:
        - name: app
          image: curlimages/curl
          command: ["sh", "-c"]
          args:
            - |
              while true; do
                echo "b-client ready";
                sleep 3600;
              done
---
apiVersion: v1
kind: Service
metadata:
  name: shared-api
  namespace: shared
spec:
  ports:
    - port: 8080
  selector:
    app: shared-api
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shared-api
  namespace: shared
spec:
  replicas: 1
  selector:
    matchLabels:
      app: shared-api
  template:
    metadata:
      labels:
        app: shared-api
    spec:
      containers:
        - name: app
          image: hashicorp/http-echo
          args: ["-text=Hello from SHARED-API"]
          ports:
            - containerPort: 8080
```

Apply:

```bash
kubectl apply -f workloads.yaml
```

Wait:

```bash
kubectl get pods -n team-a
kubectl get pods -n team-b
kubectl get pods -n shared
```

Make sure all pods are `Running` and have `istio-proxy` sidecars (2/2 containers).

---

## üß™ Step 2 ‚Äì Baseline: Test Everything BEFORE Any Policy

Let‚Äôs test from `b-client` to others.

```bash
POD_B=$(kubectl get pod -n team-b -l app=b-client -o jsonpath='{.items[0].metadata.name}')

## Call team-a backend (no policy yet)
kubectl exec -n team-b $POD_B -- curl -s a-backend.team-a.svc.cluster.local:8080

## Call team-a legacy
kubectl exec -n team-b $POD_B -- curl -s legacy.team-a.svc.cluster.local:8080

## Call shared-api
kubectl exec -n team-b $POD_B -- curl -s shared-api.shared.svc.cluster.local:8080
```

All three should respond:

- `Hello from A-BACKEND`
- `Hello from LEGACY in team-a`
- `Hello from SHARED-API`

Good. Now we make it secure üîê

---

## üåç Step 3 ‚Äì Mesh-Wide PeerAuthentication: PERMISSIVE

We set the **default for the mesh** (entire cluster) to **PERMISSIVE**:

```yaml
## file: peerauth-mesh-permissive.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: mesh-default
  namespace: istio-system
spec:
  mtls:
    mode: PERMISSIVE
```

Apply:

```bash
kubectl apply -f peerauth-mesh-permissive.yaml
```

Mesh behavior now:

- All workloads accept **both** plaintext and mTLS.

Retest from `b-client`:

```bash
kubectl exec -n team-b $POD_B -- curl -s a-backend.team-a.svc.cluster.local:8080
kubectl exec -n team-b $POD_B -- curl -s legacy.team-a.svc.cluster.local:8080
kubectl exec -n team-b $POD_B -- curl -s shared-api.shared.svc.cluster.local:8080
```

Still works ‚úÖ

---

## üß± Step 4 ‚Äì Namespace-Level Policies

Now we specialize per namespace.

## 4.1 `team-a` = STRICT mTLS

```yaml
## file: peerauth-team-a-strict.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: ns-policy
  namespace: team-a
spec:
  mtls:
    mode: STRICT
```

Apply:

```bash
kubectl apply -f peerauth-team-a-strict.yaml
```

Now:

- **Any inbound traffic to workloads in `team-a` must use mTLS.**

## 4.2 `team-b` = explicit PERMISSIVE (but from ns)

```yaml
## file: peerauth-team-b-permissive.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: ns-policy
  namespace: team-b
spec:
  mtls:
    mode: PERMISSIVE
```

Apply:

```bash
kubectl apply -f peerauth-team-b-permissive.yaml
```

`shared` has **no namespace policy**, so it inherits **mesh PERMISSIVE**.

---

## üß™ Step 5 ‚Äì Test with Namespace-Level STRICT

We didn‚Äôt configure mTLS **outbound** yet from `b-client`. So from Istio‚Äôs perspective, `b-client` ‚Üí `a-backend` is **plaintext**, but `a-backend` requires **STRICT**.

Test:

```bash
POD_B=$(kubectl get pod -n team-b -l app=b-client -o jsonpath='{.items[0].metadata.name}')

## Call strict namespace team-a
kubectl exec -n team-b $POD_B -- curl -s -v a-backend.team-a.svc.cluster.local:8080
```

Expected: ‚ùå Fail
You should see something like `Recv failure: Connection reset by peer` or HTTP error, because `a-backend` only accepts mTLS.

But calling shared (PERMISSIVE):

```bash
kubectl exec -n team-b $POD_B -- curl -s shared-api.shared.svc.cluster.local:8080
```

Still works ‚úÖ

---

## üîê Step 6 ‚Äì Enable mTLS OUTBOUND from `team-b` ‚Üí others

We tell Istio-sidecars to initiate mTLS when calling services in the mesh.

Create a **DestinationRule** in `team-b`:

```yaml
## file: dr-team-b-mtls.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: default
  namespace: team-b
spec:
  host: "*.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
```

Apply:

```bash
kubectl apply -f dr-team-b-mtls.yaml
```

Now test again from `b-client`:

```bash
kubectl exec -n team-b $POD_B -- curl -s a-backend.team-a.svc.cluster.local:8080
```

Expected: ‚úÖ `Hello from A-BACKEND`

Also:

```bash
kubectl exec -n team-b $POD_B -- curl -s shared-api.shared.svc.cluster.local:8080
```

Still works ‚úÖ (shared is PERMISSIVE anyway).

---

## üéØ Step 7 ‚Äì Workload-Level Override (Legacy App in STRICT Namespace)

Now we go deeper: within `team-a` (STRICT namespace) we override **one workload** to **DISABLE** mTLS (plaintext only) ‚Üí simulate a legacy app that can‚Äôt handle mTLS at all.

```yaml
## file: peerauth-legacy-disable.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: legacy-disable
  namespace: team-a
spec:
  selector:
    matchLabels:
      app: legacy
  mtls:
    mode: DISABLE
```

Apply:

```bash
kubectl apply -f peerauth-legacy-disable.yaml
```

Now behavior in `team-a`:

- `a-backend` ‚Üí STRICT (still)
- `legacy` ‚Üí **DISABLE** (plaintext ONLY)

---

## üß™ Step 8 ‚Äì Test Calls to `legacy` with Global mTLS Enabled

From `b-client` in `team-b`, we currently **force mTLS to all services** via our DestinationRule.

Try to call legacy now:

```bash
kubectl exec -n team-b $POD_B -- curl -s -v legacy.team-a.svc.cluster.local:8080
```

Expected: ‚ùå **Failure**

Reason:

- `b-client` ‚Üí sends **mTLS** (ISTIO_MUTUAL)
- `legacy` ‚Üí configured with `mtls: DISABLE` ‚Üí expects plaintext
- They can‚Äôt agree on a protocol ‚Üí traffic fails.

Now we prove plaintext would work.

---

## üß™ Step 9 ‚Äì Allow Plaintext to `legacy` (per-host override)

We refine the DestinationRule to:

- Keep mTLS for most services
- Use plaintext for `legacy.team-a.svc.cluster.local`

New DR in `team-b`:

```yaml
## file: dr-team-b-mix.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: default
  namespace: team-b
spec:
  host: "*.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
  subsets: []
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: legacy-plaintext
  namespace: team-b
spec:
  host: "legacy.team-a.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: DISABLE
```

Apply:

```bash
kubectl apply -f dr-team-b-mix.yaml
```

Now:

```bash
## to strict service (a-backend) ‚Üí still mTLS
kubectl exec -n team-b $POD_B -- curl -s a-backend.team-a.svc.cluster.local:8080

## to legacy (DISABLE) ‚Üí now plaintext allowed
kubectl exec -n team-b $POD_B -- curl -s legacy.team-a.svc.cluster.local:8080
```

Expected:

- `Hello from A-BACKEND` ‚úÖ (mTLS)
- `Hello from LEGACY in team-a` ‚úÖ (plaintext)

üéâ We now have **ALL levels working together**:

- Mesh-wide PERMISSIVE
- Namespace-level STRICT (team-a) & PERMISSIVE (team-b, shared)
- Workload-level DISABLE override (legacy)
- Per-host DestinationRule controlling when to use mTLS vs plaintext

---

## üïµÔ∏è Step 10 ‚Äì Verify mTLS status with `istioctl`

Check one specific path:

```bash
istioctl authn tls-check \
  $(kubectl get pod -n team-b -l app=b-client -o jsonpath='{.items[0].metadata.name}') \
  a-backend.team-a.svc.cluster.local
```

You should see:

- Client: mTLS (ISTIO_MUTUAL)
- Server: STRICT

For legacy:

```bash
istioctl authn tls-check \
  $(kubectl get pod -n team-b -l app=b-client -o jsonpath='{.items[0].metadata.name}') \
  legacy.team-a.svc.cluster.local
```

You should see:

- Client: DISABLE (plaintext)
- Server: DISABLE

---

## üü© What About Ambient Mode?

Good news:
**PeerAuthentication works the same (same CRDs) in Ambient mode.**

Differences:

- mTLS is handled by **ztunnel**, not Envoy sidecar.
- You normally **don‚Äôt need DestinationRule** to force mTLS ‚Äî ambient handles encryption automatically.
- STRICT on a namespace in Ambient means:

  - Any traffic between ambient-enrolled pods is mTLS
  - Traffic from non-ambient workloads fails

In other words, **the policies we wrote are reusable in ambient**, but some of the DR complexity disappears because ztunnel enforces mTLS centrally.

---

## üßÅ TL;DR

In this lab you saw:

- üåç **Mesh-wide** PeerAuthentication ‚Üí PERMISSIVE
- üß± **Namespace-level**:

  - `team-a` STRICT
  - `team-b` PERMISSIVE

- üéØ **Workload-level**:

  - `legacy` in `team-a` ‚Üí DISABLE (plaintext only)

- üéöÔ∏è DestinationRule controlling **who uses mTLS vs plaintext**
- ‚úÖ Real curl tests showing:

  - Failure into STRICT when using plaintext
  - Failure into DISABLE when using forced mTLS
  - Success when policies & DR align
