# â­ **Cilium Policy Fundamentals**

ğŸ”¥ _â€œUnderstand this chapter deeply, and every Cilium policy you write will make perfect sense.â€_

---

## ğŸ§  What Makes Cilium Different From Kubernetes Network Policies?

Cilium is not just â€œanother CNIâ€.
It changes **how network security is enforced** in Kubernetes.

### ğŸš€ **Kubernetes NetworkPolicy (standard behavior):**

- Works at **L3/L4** only (IP + port).
- Implemented through **iptables** or **IPVS** depending on CNI.
- Policies are limited and slow in large-scale clusters.

### âš™ï¸ **Cilium NetworkPolicy:**

- Enforced using **eBPF**, NOT iptables.
- Understands **L3 + L4 + L7 (HTTP, gRPC, Kafka, DNS)**.
- Faster, more scalable, more precise.
- Has additional advanced selectors and rule types.

---

## ğŸ¯ The Most Important Concept: **Pod Identity**

Cilium does **NOT** filter traffic using IPs.
IPs change. Endpoints restart. That would be chaos.

Cilium instead assigns each pod a **security identity** based on its labels.

### Example:

Pod:

```yaml
metadata:
  labels:
    app: db
    role: backend
    env: prod
```

Cilium identity = hash of these labels, e.g. `id=3841`.

Cilium stores all policy rules in terms of:

> ğŸ”’ _Source identity âœ Destination identity_

NOT source IP â†’ destination IP.

This is why labeling is **critical** in Cilium.

---

## ğŸ›ï¸ When Is a Pod Under â€œPolicy Enforcement Modeâ€?

THIS is the part most engineers donâ€™t understand at first.

### ğŸŸ¢ If **no** Cilium policy selects a pod's labels:

Cilium leaves the pod in **default-allow** mode:

```ini
Everyone can access the pod.
Pod can access everyone.
```

This is Kubernetes default.

### ğŸ” If **any** Cilium policy selects a pod â†’ that pod enters **policy-enforced mode**.

Policy-enforced mode means:

- For **ingress**:
  Only traffic explicitly allowed by the policy is allowed.
  Everything else is **DENIED**.

- For **egress**:
  Only the defined outbound rules are allowed.
  Everything else is **DENIED**.

### ğŸ“Œ Key rule:

> âœ”ï¸ Applying a **single** Cilium policy switches the pod from â€œdefault allowâ€ â†’ â€œdeny unless allowedâ€.

This is why your example with `db` pods worked.

---

## ğŸ” What Actually Happens When Traffic Arrives?

Letâ€™s say a packet reaches a pod.

Cilium checks:

### **STEP 1 â€” Does the destination pod have â€œpolicy enforcement: ingressâ€?**

- If **no** â†’ allow the packet (default).
- If **yes** â†’ move to step 2.

### **STEP 2 â€” Does any rule allow traffic from this source identity?**

- If **yes** â†’ allow
- If **no** â†’ drop

### Cilium enforces policies at:

- Pod ingress
- Pod egress
- L7 proxy (for HTTP/gRPC/Kafka rules)

All in eBPF path (ultra fast).

---

## ğŸ§ª How Cilium Decides Ingress/Egress Enforcement

This is EXTREMELY important, and the source of 90% confusion.

Cilium stores enforcement direction **per endpoint**:

Run:

```bash
cilium endpoint list
```

You will see columns:

```ini
Ingress Enforcement
Egress Enforcement
```

Values can be:

- `Disabled` â†’ no policies select this direction
- `Enabled` â†’ at least one policy enforces this direction

### Rules:

| What exists?                  | Ingress enforced? | Egress enforced? |
| ----------------------------- | ----------------- | ---------------- |
| A policy with `toEndpoints`   | âŒ                | âœ…               |
| A policy with `fromEndpoints` | âœ…                | âŒ               |
| Policies with both            | âœ…                | âœ…               |
| No policy                     | âŒ                | âŒ               |

---

## ğŸ“ Example â€” Why Your `db` Pod Was Blocked From Frontend

Your policy:

```yaml
spec:
  endpointSelector:
    matchLabels:
      role: db
  ingress:
    - fromEndpoints:
        - matchLabels:
            role: backend
```

Because this selects `role=db`, that pod becomes:

```ini
Ingress Enforcement = Enabled
Egress Enforcement = Disabled
```

Meaning:

- Only backend pods can access db pod.
- Frontend â†’ db is **denied**, even though you didnâ€™t specify a deny rule explicitly.

This is the â€œdefault deny once selectedâ€ rule.

---

## ğŸ”¥ How Cilium Applies Policy in eBPF

Cilium attaches eBPF programs at:

- **XDP** (very early in packet path)
- **TC ingress**
- **TC egress**
- **Socket-level hooks**
- **L7 proxy** when needed

When a packet arrives, Cilium:

1. Extracts source/destination security identities
2. Looks up policy maps:

   - `CILIUM_POLICY_MAP` (L3/L4)
   - `CILIUM_L7_POLICY_MAP`

3. Decides **allow** or **drop**
4. Updates Hubble for observability

This is why Cilium is extremely fast.

---

## ğŸ›°ï¸ The Three Types of Cilium Policies (Important Foundation)

Cilium gives 3 major policy types:

| Kind                             | Scope               | Use Case                      |
| -------------------------------- | ------------------- | ----------------------------- |
| `CiliumNetworkPolicy`            | Namespace-scoped    | microservice-level controls   |
| `CiliumClusterwideNetworkPolicy` | Cluster-wide        | platform teams / global rules |
| `CiliumLocalRedirectPolicy`      | Pod-local rerouting | service mesh patterns         |

In this course we focus on `CiliumNetworkPolicy` & `CiliumClusterwideNetworkPolicy`.

---

## ğŸ› ï¸ Debugging Policies â€” Your Essential Tools

### **1. Check endpoint enforcement**

```bash
cilium endpoint list
```

### **2. View applied policies**

```bash
cilium policy get
```

### **3. Watch traffic in real time**

```bash
cilium monitor -t drop,policy
```

### **4. Hubble flows**

```bash
hubble observe --verdict drop
```

This makes debugging **clear and visual**.

---

## ğŸ“ END OF TOPIC

If you understand:

- Security identity
- Endpoint selector
- Policy enforcement direction
- Default-allow vs policy-enforced
- Identity matching

Then writing policies becomes **logical**, not confusing.
