# S3

## Object Storage

- S3 is a simple Key-Value object store.
- An Object is a file and any metadata that describes it.
- An S3 Bucket is a container for objects.
- Each object has a key (name) and a value which is the data stored.
- Each object cannot exceed `5TB in size`.
- Buckets and Objects are `private by default`, you need to grant permissions, so they become accessible.
- We use `HTTP requests` when dealing with S3.
- S3 supports HTTP and HTTPS endpoints.

## Data Consistency Model

- GET, PUT, UPDATE, and DELETE are HTTP requests (or HTTP Verbs or Method options) used when dealing with S3 buckets.
- S3 offers Read after write (immediate or strong) consistency for all requests of S3 objects.
- Updates to an object in S3 are atomic (we never get partial or corrupt data).
- To access an object in a bucket, the URI format is

  - `https://[bucket-name].s3.[Region].amazonaws.com/[key-name]`

## Region Selection

- Choosing an AWS region for your S3 buckets can be based on:

  - Optimizing latency,
  - Minimizing costs,
  - Addressing regulatory requirements.

- Objects stored in an AWS Region will never leave the Region unless the object owner explicitly transfers or replicates them to another Region

---

# S3 Tiered Storage Classes

![s3-storage-classes](images/s3-storage-classes.png)

## Notes

---

### Infrequent Access

- Both IA classes are for real-time instant access.
- One-Zone IA (single AZ) has the same durability as S3-IA.
- One-Zone IA has less availability and is less resilient compared to S3-IA.
- Minimum storage duration of 30 days.
- If an object is deleted before 30 days, a 30 days charge will apply.
- AWS charges for requests and for data retrieval (per GB).

### Archiving Classes

- All archive classes have eleven 9’s durability.
- S3 Glacier Instant Retrieval: for rarely accessed data that requires msec retrieval time.
  - Higher data access charges compared to S3-IA class.
- S3 Glacier Flexible Retrieval: Use for archives where data may need to be retrieved in minutes.
  - 90 days minimum storage charge.
- S3 Glacier Deep Archive: Use for archives that rarely need to be accessed.
  - 180 days minimum storage charge.
- Both Glacier Flexible Retrieval and Glacier Deep Archive are for cold storage (data access is asynchronous)

#### Glacier Flexible Retrieval & Glacier Deep Archive retrieval options

- Are designed to sustain data loss in two facilities (They replicate data in 3+ AZs).
- Glacier archives are visible and available only through AWS S3 dashboard.
- To guarantee access for expedited retrieval, customers can purchase provisioned `retrieval capacity`.
- Deep_Archive & Bulk Retrieval is the cheapest of all S3 classes

| **Storage Class**                 | **Expedited**    | **Standard**    | **Bulk**        |
| --------------------------------- | ---------------- | --------------- | --------------- |
| **S3 Glacier Flexible Retrieval** | 1-5 minutes (\$) | 3-5 hours       | 5-12 hours      |
| **S3 Glacier Deep Archive**       | N/A              | Within 12 hours | Within 48 hours |

#### Glacier Deep Archive –Archives and Vaults

- An archive is the base storage unit in Glacier (40TB max size).
- An archive can be any data such as a file, a photo, or a document.

---

- A vault is a container to store archives.
- A vault is confined to an AWS region.
- A vault can store an unlimited number of archives.
- Maximum 1000 vaults per AWS region. We can get archives uploaded to vaults using:
- CLI, SDKs (not the AWS console).

### Intelligent Tiering

- S3 Intelligent-Tiering is a storage class intended to optimize storage costs.
- Use it when data access pattern is unknown, changing, or unpredictable (data lakes, data analytics, and new applications).
- It monitors data access patterns and automatically moves data (at the object level) to the most cost effective S3 access tier.
- There is no retrieval fees for Intelligent-Tiering.
- AWS charges a small monthly monitoring fee.
- If the data is accessed again, it gets moved back to the Frequent Access Tier.

---

- Activating the Archive Access Tier will bypass the Archive Instant Access Tier.
  - It is like S3 Glacier Flexible Retrieval storage class.
- Deep Archive Access Tier, when enabled, objects not accessed for 180 days (configurable to 730 days) will be moved in this class.
  - It like S3 Glacier Deep Archive tier.
- For both Archive and Deep Archive access tiers retrieval time is required.

![s3-intelligent-lifecycle-if-cold-storage-not-enabled](images/s3-intelligent-lifecycle-if-cold-storage-not-enabled.png)
![s3-intelligent-lifecycle-if-cold-storage-enabled](images/s3-intelligent-lifecycle-if-cold-storage-enabled.png)

---

# S3 Encryption

- Encryption in-transit means, encrypting data as it moves.
- Encryption at rest means, encrypting data while it is stored.
- Encryption at rest can be client-side or S3 (server-side) encryption

## **S3 Object Encryption At Transit**

S3 Provides HTTP and HTTPS endpoints.

<div align="center" style="padding: 0 20px;">
  <img src="images/encryption-at-transit.png" alt="encryption-at-transit">
</div>

- For encryption in-transit, we must use the HTTPS endpoint provided by S3. This can be enforced in the bucket policy
- Objects do not have to be encrypted at the client side if we choose not to.

## **S3 Object Encryption At Rest**

<div align="center" style="padding: 0 20px;">
  <img src="images/encryption-at-rest-types.png" alt="encryption-at-rest-types">
</div>

- Encryption at rest can be client-side or S3 (server-side) encryption.
- In client-side Encryption, the client is responsible for encrypting/decrypting the data. S3 has nothing to do with the encryption/decryption process or keys.
- In server-side Encryption, S3 manages the encryption/decryption and optionally the keys.
- We have three SSE encryption choices under S3, they are SSE-S3, SSE-KMS, and SSE-C

## Client-Side Encryption At Rest (CSE)

<div align="center" style="padding: 0 20px;">
  <img src="images/cse.gif" alt="Client-Side Encryption (CSE)">
</div>

- The client is responsible for the entire encryption/decryption and keys. AWS is not part of the encryption/decryption process.
- Each object is encrypted at the client side before it is moved to S3.
- Objects are encrypted in-transit automatically.

## Server-Side Encryption At Rest (SSE)

- Encryption/Decryption is done by S3 before the object is stored to disks
- Server-Side Encryption does not encrypt the object metadata.
- Depending on who provides the encryption keys, there are three SSE types:
  - **SSE-S3**
  - **SSE-KMS**
  - **SSE-C**

### 1. **SSE-S3**

<div align="center" style="padding: 0 20px;">
  <img src="images/sse-s3.gif" alt="sse-s3">
</div>

- Encryption Using S3-Managed Encryption Keys
- Each object will be encrypted with a separate data key created and managed by S3.
- **The Least customer overhead:** the customer has nothing to do with key creation or rotation

### 2. **SSE-KMS**

<div align="center" style="padding: 0 20px;">
<h3>
  It uses the AWS S3 default KMS key or a customer created/managed KMS key in AWS KMS.
</h3>
  <img src="images/sse-kms.gif" alt="sse-s3">
</div>

- The KMS key used must be in the same regions as the S3 bucket.
- The KMS key used can be created in KMS or imported to KMS by the customer
- Each object will be encrypted with a separate data key.
- Audit trail is provided for the KMS key usage.
- SSE-KMS with customer managed keys is chargeable (KMS key charge , plus data key request charges)

#### ⚠️ Potential Issues

Given that each object is encrypted with a separate data key. If we are using SSE-KMS extensively, this will add a lot of request charges and
may reach the KMS API operations (requests) per second limit in the account. This can lead to throttling of the additional requests.

#### ✅ Solution using bucket key

<div align="center" style="padding: 0 20px;">
<h3>
An S3 bucket key can be created by AWS KMS then used to generate the data
keys at the bucket level. It reduces the cost and the load on KMS.
</h3>
  <img src="images/sse-kms-bucket-key.gif" alt="sse-s3">
</div>

### 3. **SSE-C**

<div align="center" style="padding: 0 20px;">
<h3>
Encryption and decryption are done by S3 using a key provided by the customer.
</h3>
  <img src="images/sse-c.gif" alt="sse-s3">
</div>

- The customer must supply the data key with each encryption/decryption request.
- HTTPS must be used in-transit to protect the customer provided data keys
- SSE-C can only be done through SDKs.

## Bucket Default Encryption

This is a `bucket level` configuration.

<div align="center" style="padding: 0 20px;">
  <img src="images/default-encryption.png" alt="default-encryption">
</div>

- When enabled, all new object uploads without encryption information will be encrypted using the default encryption.
- If the request includes encryption information, the default encryption is not used.
- Pre-existing unencrypted data in the bucket before enabling this feature will not be encrypted automatically.
- We can choose between SSE-S3 or SSE-KMS

- S3 Glacier `Automatically` encrypts data at rest using AES-256-bit symmetric keys maintained by AWS.
- It supports secure data transfer in-transit over Secure Sockets Layer (SSL)

---

# S3 Access Policies

<div align="center" style="padding: 0 20px;">
<h3>
</h3>
  <img src="images/s3-access-policies.png" alt="S3 Access Policies">
</div>

- Access policies for bucket or its objects can be identity based policy or resource based policy
- Both S3 buckets and Glacier vaults support
  IAM resource-based policies. For Glacier vault, it is called Vault access policies

**Resource-Based Policies can be defined using:**

- Access Control Lists (ACLs)

  - XML format
  - up to 100 grants per policy
  - Policy can be applied on bucket or object level

- Bucket Policies
  - JSON format
  - Policy can be up to 20KB in size
  - Policy can be applied on bucket level only

## **Understanding Object Ownership in S3:**

**Object Ownership Option mean that:**
![s3-object-ownership](images/s3-object-ownership.png)

- Objects and buckets are private by default.
- Every resource in AWS is owned by an account.
- The resource owner of the bucket is the account of the IAM entity (IAM user, IAM Role or Root account) that created the bucket.

## Using ACLs (Not Recommended)

- Each bucket and object has an ACL attached to it as a sub-resource.
  - The ACL defines which AWS accounts or groups are granted access and the type of access.
- Using `Object Ownership`, we can disable ACLs;
  - The bucket owner automatically owns every object in the bucket.
  - Access control for the data in the bucket will be based on policies only, such as IAM policies, S3 bucket policies, virtual private cloud (VPC) endpoint policies, and AWS Organizations service control policies (SCPs).
- If you do not use Object Ownership, then the account that uploads the object is the object owner.
- AWS recommends disabling ACLs and using Object Ownership unless absolutely required otherwise.

## Using Resource Policy

its ....etc

### Examples

#### Granting Public Read Access

This example gives permissions to everyone (**public read access**) via the GetObject permission, on all objects in the S3 bucket `hw-bucket-01`.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicRead",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": ["arn:aws:s3:::hw-bucket-01/*"]
    }
  ]
}
```

#### Cross-Account Uploads

Using the `s3:x-amz-storage-class` condition key, cross account access can be provided to allow other accounts to upload objects in a bucket and **restrict them to upload only to a specific S3 storage class**.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "statement1",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::AccountB-ID:user/Ahmed"
      },
      "Action": "s3:PutObject",
      "Resource": ["arn:aws:s3:::hw-bucket-01/*"],
      "Condition": {
        "StringEquals": {
          "s3:x-amz-storage-class": ["STANDARD_IA"]
        }
      }
    }
  ]
}
```

#### Cross-Account Object Uploads - Public-Read Access

Using the `s3:x-amz-acl` Condition key, we can allow other accounts to upload objects conditionally upon granting public read access to their objects.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AddCannedAcl",
      "Effect": "Allow",
      "Principal": {
        "AWS": ["arn:aws:iam::accountA-ID:root", "arn:aws:iam::accountB-ID:root"]
      },
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::hw-bucket-01/*",
      "Condition": {
        "StringEquals": {
          "s3:x-amz-acl": "public-read"
        }
      }
    }
  ]
}
```

#### Object Uploads - Enforcing Encryption

Using the `s3:x-amz-server-side-encryption` condition key, we can enforce a specific server-side encryption for S3 object uploads.

- If the correct encryption is not specified in the upload request, the request will fail.
- Use AWS:KMS this means KMS encryption and use AES256 for SSE-S3

```json
{
  "Version": "2012-10-17",
  "Id": "PutObjectPolicy",
  "Statement": [
    {
      "Sid": "DenyUnEncryptedObjectUploads",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::hw-bucket-01/*",
      "Condition": {
        "StringNotEquals": {
          "s3:x-amz-server-side-encryption": "aws:kms"
        }
      }
    }
  ]
}
```

---

# S3 Bucket Replication

<div align="center" style="padding: 0 20px;">
  <img src="images/srr-crr.png" alt="Encryption In-Transit">
</div>

- Is an `asynchronous replication` at the bucket level from a source bucket to a destination bucket.
- TLS/SSL is used to encrypt replication data in-transit.
- Source and destination buckets can be in the same or different AWS accounts.
- Source and destination buckets can be in the same (SRR) or different (CRR) AWS Regions.
- `Both buckets must have versioning enabled.`
- We can replicate all objects, or a subset based on a prefix.

---

- We can configure the source bucket to replicate to more than one destination bucket (requires multiple replication rules).
- You can choose to copy the pre -existing objects in the bucket (one time batch operation).
- S3 requires an IAM role to replicate objects between buckets.
- We can choose to enable replication metrics and notifications (sent to CloudWatch).
- We can choose to replicate DELETE markers

**Replication Time Control (RTC):**

- We can enable S3 Replication Time Control (S3 RTC) in the replication configuration.
- S3 RTC replicates most objects in seconds and 99.99 percent of objects within 15 minutes (backed by a service-level agreement).

**Replication Encryption:**

- For encrypted objects in the source bucket, the following applies:
  - Objects encrypted with SSE-S3 are replicated.
  - You can optionally replicate objects encrypted with SSE-KMS.
  - Objects encrypted with SSE-C are not replicated.
- For un-encrypted objects in the source bucket, if the destination bucket has default encrypted enabled, the replicated objects will be encrypted following that.

**Use Cases:**

- **SRR:**
  - Use for aggregating log data into a single bucket (from the same or different account buckets).
  - Use to replicate between different environments that work on the same data (ex. production and testing).
  - Use to replicate critical data within the same region for data sovereignty laws.
- **CRR:**
  - Use to meet compliance requirements by storing a copy a distance away.
  - Use to minimize latency to users/customers by making the data available in other AWS Regions closer to them.
  - Use if availing the data for processing by clusters in two different regions.

---

# S3 Object Lock

S3's Object Lock feature provides a `Write Once Read Many (WORM)` data storage model and compliance in S3

- It protects objects from being deleted for a period of time or indefinitely.
- Can be configured at the `object or bucket level`.
  - `Bucket Versioning` must be enabled.
- It can be combined with CRR or SRR to replicate protected objects to a destination bucket

---

**We can use two different ways to enable Object Lock:**

- `Retention Period:` Can be used to specify a lock duration at the object and object version level.
- `Legal Hold`: Lock has no expiration until the hold is removed.
  - An object can have either, both, or none.
- 📒 It’s very important to note that protected object versions remain safe from being deleted or overwritten by a lifecycle configuration

## Retention modes

**There are two Retention modes in S3 Object Lock:**

- **Governance mode:** Protects against object or object version overwrites/deletes/altering-lock-settings by most users, except those with permissions to do so.
- **Compliance mode:** No one, not even the root user, can overwrite/delete a protected object for the duration of the retention period.

## Glacier Vault Lock Policies

By default, Glacier archives are immutable. However, they can be deleted.

S3 Glacier Vault Lock policies can be configured to enforce compliance control for
individual Glacier vaults.

- Write Once Read Many (WORM) compliance requirements can be met.
- Vault locks are used to prevent future changes.

**Example use cases:**

- Retain archives for one year before they can be deleted.
- Deny deleting archives based on a specific tag during an investigation

---

# S3 Select

## S3 Select (Query At The Source)

S3 Select provides the ability to run simple SQL statements to filter the contents of an Amazon S3 object to retrieve the required subset of data.

- It supports objects in CSV, JSON, and Apache Parquet formats and works on GZIP or Snappy compressed objects
- It supports queries on SSE encrypted objects.
- Query output can be in CSV or JSON.
- It reduces data transfer costs and enhances application performance by reducing latency and CPU cycles

### How it works

<div align="center" style="padding: 0 20px">
  <img src="images/s3-select.gif" alt="S3 Bucket Replication">
</div>

## S3 Glacier Select

S3 Glacier Select feature allows simple SQL statements to run directly (query in
place) on an archive without having to first restore the archive.

- Query results are saved to an S3 bucket.
- Glacier can now be considered part of your data lake

### How it works

<div align="center" style="padding: 0 20px">
  <img src="images/s3-select-glacier.gif" alt="S3 Bucket Replication">
</div>

---

# S3 Pre-Signed URLs

- All objects are private by default.
- The object owner can share the object by creating a pre-signed URL, using their own security credentials.
- `Signed URLs are object specific.`
- The entity that creates the signed URL can be an IAM user, root account, EC2 instance/APP, or someone with an STS token.
- This will work even on private buckets and objects

---

- Amazon S3 checks the expiration date and time of a signed URL at the time of the HTTP request.
- Depending on how you generate it (CLI, SDK, Console) the signed URL can be valid for up to 7 days.
- Pre-Signed URLS can also be used to upload an object to a private bucket.
  - This can be generated through SDKs only

---

# S3 Transfer Acceleration & Requester Pays

## S3 Transfer Accelerator

Transfer Acceleration is a chargeable service used to accelerate objects uploads into S3 buckets from users over long distances.

- It uses HTTPS for uploads.
- It utilizes CloudFront’s edge locations nearest to the source.
- Needs to be enabled on the bucket.
- Can be used with multipart upload.
- The farther away the upload source is from the Amazon S3 region, the higher the speed improvement expected

### How To Choice

This speed checker uses multipart uploads to transfer a file from your browser to various Amazon S3 regions with and without Amazon S3 Transfer Acceleration. It compares the speed results and shows the percentage difference for every region.

[https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html](https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html)

## S3 Requester Pays

if you are re-searchable institute and you provide a lot of data set for users for research, you can only pay for storage and the users (must be have iam account in aws) who needs the data will be for the data retrieval.

•Configure the bucket to be a "requester pays" bucket if the bucket owner would like to share large data sets but does not want to incur the read/download charges.
•The bucket owner continues to pay for storage, but the requester pays for data transfer.
•Anonymous access to requester pays buckets is not allowed.
ØThe requester must be authenticated (known to AWS) for billing purposes.
•The requests made to a requester pays bucket must include the x-amz-request-payerheader.
•Objects can be requested through the CLI and REST API

---

# S3 Event Notifications

<div align="center" style="padding: 0 0;">
  <img src="images/s3-event-notification.png" alt="S3 CORS">
</div>

S3 Event notifications is about sending notifications when certain events in the configured S3 bucket occur.

- S3 events like object create, delete, restore events as well as replications, lifecycle, or intelligent tiering events.
- S3 bucket-level event notifications can be configured to automatically send notifications to one of the following destinations:
  - SNS Topic
  - SQS Queue
  - Lambda Function

---

# S3 additional Features

## AWS Transfer Family

Transfer family provides customers access to a fully
managed FTP-enabled server in the cloud.

<div align="center" style="padding: 0 0;">
  <img src="images/aws-transfer-family.png" alt="AWS Transfer Family">
</div>

- It helps in migrating file transfer workflows into AWS.

**AWS Transfer Family includes:**

- SSH File Transfer Protocol (SFTP) – Relies on SSH.
- File Transfer Protocol Secure (FTPS) – Uses TLS/SSL.
- File Transfer Protocol (FTP).

## S3 Access Points

Amazon Access Points are named network endpoints that can be used to simplify access to S3 buckets and provide many applications access at scale for shared datasets in Amazon S3.

<div align="center" style="padding: 0 20px;">
  <img src="images/s3-access-point.png" alt="S3 Access Points">
</div>

- Access Points are attached to buckets.
- We can use S3 Access Points to perform S3 object operations, such as GetObject and PutObject.
- Each S3 Access Point has its own access point policy (resource-based policy) which defines how data can be accessed using that endpoint.
- We can configure block public access settings for each Access Point.

---

- We can configure an access point to accept requests only from a VPC (and not from the Internet).
- There are some limitations for S3 requests that cannot be done through access points, for example, S3 Website endpoints does not support access points.
- For a request to be allowed through the Access Point, it has to be allowed by both the Access Point policy and the Bucket Policy.
  - We can delegate all Bucket access to be through its Access Points.

## S3 Inventory

<div align="center" style="padding: 0 20px;">
  <img src="images/s3-inventory.png" alt="S3 Inventory">
</div>

Amazon S3 Inventory can be used to get a list of the objects (or a subset of the objects sharing a common prefix) in a source bucket and their corresponding metadata.

- S3 Inventory can provide the Inventory report (output) on a daily or weekly basis.
- Amazon S3 Inventory provides a CSV, Apache ORC, or Apache Parquet output file that gets stored in the destination bucket.
- We can use the Inventory data to report on the status of the objects for compliance or regulatory purposes.

## S3 Analytics and Insights

Analytics and insights in Amazon S3 can be used to understand, analyze, and
optimize S3 storage usage.

**S3 Analytics Storage Class Analysis:**

- It observes object access patterns and provides analysis results that can be used to determine when to transition the (using lifecycle policy) from S3 Standard to S3-IA.
- We can use it for all objects in a bucket, objects with a specific tag, or objects with a common prefix.
- Once configured, analysis data will appear in the console in 24-48 hours.

---

Amazon S3 Storage Lens provides a single view of S3 storage usage across all your S3 storage.

- It can generate insights at the organization, account, region, bucket, or prefix level.
- Using S3 Storage Lens, we can display usage and activity metrics in the console, interactive dashboard, or export/download the metrics data in CSV, or Parquet format.
- It has a free tier that can display the data up to 14 days, or a subscription to the advanced metrics and recommendations.
- All S3 Lens data is retained for 15 months.

## S3 Batch Operations

S3 Batch Operations can be used to perform large-scale batch operations on Amazon S3
objects.

Amazon S3 tracks progress, sends notifications and stores a detailed completion report.

**S3 Batch Operations can be used to:**

- Copy objects.
- Set object tags.
- Initiate object retrieval/restores from Glacier.
- Custom actions using the S3 objects (using Lambda functions).

These operations can be performed on a custom list of objects, or an Amazon S3 inventory report can be used to make generating larger lists of objects easy.
