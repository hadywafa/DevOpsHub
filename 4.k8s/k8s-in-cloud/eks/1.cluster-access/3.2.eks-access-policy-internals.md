# âš™ï¸ **How EKS Access Policies Work Internally**

> _â€œUnder the hood of AWSâ€™s new identity bridge between IAM and Kubernetes RBACâ€_

---

## ğŸ”´ **The Core Problem AWS Solved**

Before EKS Access Policies, this was the chain:

<div align="center" style="background-color: #2b3436ff; border-radius: 10px; border: 2px solid">

```mermaid
graph TD
A[AWS IAM Role/User] -->|AssumeRole / STS Token| B[EKS API Server]
B -->|Looks into aws-auth ConfigMap| C[Kubernetes RBAC]
C --> D[Kubernetes Resources]
```

</div>

The issue:

- `aws-auth` ConfigMap manually mapped IAM â†’ K8s usernames/groups.
- Updating it required `kubectl` admin access (chicken-egg problem).
- No visibility in IAM console or CloudTrail.
- No way to limit scope (namespace-level access).

AWS wanted to bring **IAM-native control** to EKS **without breaking Kubernetes authorization logic**.

---

## ğŸ–‡ï¸ **Modern Chain with Access Entries**

Hereâ€™s what happens **now** when you run `kubectl get pods` with Access Policy configured:

<div align="center" style="background-color: #2b3436ff; border-radius: 10px; border: 2px solid">

```mermaid
sequenceDiagram
participant IAM Role
participant STS
participant EKS Authenticator
participant EKS Access Entry
participant Access Policy
participant Kubernetes API Server
participant RBAC Engine

IAM Role->>STS: Get EKS token (aws eks get-token)
STS->>EKS Authenticator: Token validated (OIDC/JWT)
EKS Authenticator->>EKS Access Entry: Lookup principal (ARN)
EKS Access Entry->>Access Policy: Retrieve policy + namespace scope
Access Policy->>Kubernetes API Server: Translates into temporary ClusterRoleBindings
Kubernetes API Server->>RBAC Engine: Evaluate verbs (get/list/etc.)
RBAC Engine->>User: Allow or Deny
```

</div>

---

## ğŸªœ **Step-by-Step Internal Flow**

Letâ€™s go step by step.

### ğŸªª Step 1: Authentication (IAM â†’ STS â†’ Token)

When you run:

```bash
aws eks update-kubeconfig --role-arn arn:aws:iam::123:role/my-eks-role
```

Then `kubectl` uses:

```bash
aws eks get-token --cluster-name my-cluster
```

âœ… AWS CLI asks **STS** to issue a **temporary token** tied to your IAM role.
That token includes:

- The `assumed-role` ARN
- The `sessionName`
- A short-lived JWT signed by AWS

That token is sent as a Bearer token to the EKS API Server.

---

### ğŸ§° Step 2: EKS Authenticator Validates the Token

The **EKS API server** uses an **authentication webhook** called `aws-iam-authenticator` (now built-in to EKS).

It checks:

- Signature (via AWS public key)
- Expiry time (STS token validity)
- ARN of the caller (principal)

âœ… Once verified, EKS knows _who_ you are â€” e.g.
`arn:aws:sts::065656773845:assumed-role/orchida-tax-eks-admin/EKSGetTokenAuth`

---

### ğŸ§© Step 3: EKS Access Entry Lookup

EKS checks your clusterâ€™s **Access Entry Table** â€” this is stored in EKS control plane (not inside Kubernetes).

It finds an entry like this:

```json
{
  "principalArn": "arn:aws:iam::065656773845:role/orchida-tax-eks-admin",
  "accessPolicies": [
    {
      "policyName": "AmazonEKSAdminPolicy",
      "scope": { "type": "cluster" }
    }
  ],
  "groups": ["my-admin"]
}
```

This acts as the **IAM â†’ Kubernetes translation record**.

---

### ğŸ§  Step 4: Apply Access Policy Template

Each **Access Policy** (like `AmazonEKSAdminPolicy`) is a **predefined RBAC template** managed by AWS.
It defines what verbs/resources you can use, such as:

```yaml
rules:
  - apiGroups: [""]
    resources: ["pods", "services", "configmaps"]
    verbs: ["get", "list", "watch", "create", "update", "delete"]
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "create", "update"]
```

AWS applies these **dynamically** â€” it doesnâ€™t actually create YAML files in your cluster; instead, it **injects temporary RBAC bindings** at runtime.

Thatâ€™s why you donâ€™t see any new ClusterRoles when listing with `kubectl get clusterrole`.

---

### ğŸ§© Step 5: Scope Enforcement (Namespace or Cluster)

If your Access Policy is scoped like:

```bash
type=namespace, namespaces=["dev", "qa"]
```

Then AWS wraps that policy in a **RoleBinding** limited to those namespaces.

If itâ€™s `type=cluster`, it acts as a **ClusterRoleBinding**.

So internally:

- EKS checks which namespaces match your scope.
- Dynamically creates the equivalent bindings in memory (not as stored objects).
- Passes them to the Kubernetes API serverâ€™s **SubjectAccessReview** mechanism.

---

### ğŸ§® Step 6: RBAC Decision via SubjectAccessReview

Finally, the **Kubernetes RBAC Engine** does what it always does â€”  
checks whether your effective identity (group/role) can perform the requested action on the resource.

âœ… If allowed â†’ response returned  
âŒ If not â†’

```ini
Error from server (Forbidden): pods is forbidden: User "<arn>" cannot list resource "pods"
```

---

## ğŸ‘€ **What Actually Exists vs. Whatâ€™s Virtual**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Layer                                   | Physically Exists         | Description                                   |
| --------------------------------------- | ------------------------- | --------------------------------------------- |
| IAM Role                                | âœ…                        | Real AWS identity with permissions            |
| EKS Access Entry                        | âœ…                        | Stored in EKS control plane (AWS API managed) |
| Access Policy                           | âœ… (AWS-managed template) | JSON-defined template in AWS backend          |
| ClusterRoleBinding (from Access Policy) | âŒ (virtual)              | Generated at request time, not persisted      |
| Kubernetes RBAC                         | âœ…                        | Still used for all enforcement                |

</div>

AWS uses **ephemeral bindings** â€” thatâ€™s why even after removing an Access Policy, youâ€™re instantly locked out.

---

## ğŸ” **Key Internal Components**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Component                | Description                                               | Managed By |
| ------------------------ | --------------------------------------------------------- | ---------- |
| **AccessEntry API**      | Stores which IAM roles/users are linked to which clusters | AWS        |
| **AccessPolicy Engine**  | Maps high-level policies (Admin, View) to RBAC templates  | AWS        |
| **EKS Authenticator**    | Validates AWS tokens                                      | AWS        |
| **RBAC Engine (in K8s)** | Enforces allow/deny on requests                           | Kubernetes |

</div>

---

## ğŸ§­ **End-to-End Example**

Letâ€™s walk through `kubectl get nodes` for your case.

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Step | What Happens                                           | Where             |
| ---- | ------------------------------------------------------ | ----------------- |
| 1    | You run `kubectl get nodes`                            | Your terminal     |
| 2    | `aws eks get-token` generates STS JWT                  | AWS STS           |
| 3    | API server validates token via AWS OIDC                | EKS control plane |
| 4    | EKS looks up Access Entry                              | EKS control plane |
| 5    | Finds `AmazonEKSAdminPolicy`                           | AWS backend       |
| 6    | Translates to â€œnamespaced adminâ€ (no node permissions) | AWS backend       |
| 7    | Kubernetes RBAC denies node list request               | K8s API           |
| âŒ   | You get Forbidden                                      | Output            |

</div>

When you add `AmazonEKSClusterAdminPolicy`,  
Step 6 now grants `list/get` for `nodes`, so the API returns successfully.

---

## âš–ï¸ **Comparison: Old vs. New Model**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Aspect               | Old (aws-auth ConfigMap)       | New (Access Entry + Policy) |
| -------------------- | ------------------------------ | --------------------------- |
| Mapping location     | Inside cluster ConfigMap       | EKS control plane           |
| Management           | kubectl edit                   | AWS Console / CLI           |
| Namespace control    | No                             | Yes                         |
| Audit visibility     | None                           | CloudTrail integrated       |
| Misconfig risk       | High (lockout possible)        | Low                         |
| Real-time revocation | Slow (until ConfigMap reloads) | Immediate                   |

</div>

---

## ğŸ **Visual Summary**

<div align="center" style="background-color: #2b3436ff; border-radius: 10px; border: 2px solid">

```mermaid
graph TD
A[IAM Role/User] --> B[STS Token]
B --> C[EKS Authenticator]
C --> D[EKS Access Entry Lookup]
D --> E[Access Policy Engine]
E --> F[Kubernetes API Server]
F --> G[RBAC Enforcement]
G --> H["Resource Access (Pods, Nodes, PVC)"]
```

</div>

âœ… IAM authenticates  
ğŸŸ¢ EKS translates Access Policy â†’ virtual RBAC  
ğŸŸ£ Kubernetes enforces rules

---

## âš¡ **TL;DR**

> AWSâ€™s Access Policy system is an **IAM-aware RBAC translator**.
> It authenticates using STS â†’ maps via Access Entry â†’ translates policy to ephemeral RBAC bindings â†’ and lets Kubernetes still make the final authorization call.
