# üåê Deep Dive: Network-Level Flow of Watch Connections (mTLS + Ports + API Server Channels)

üëâ _‚ÄúNetwork-level flow of Watch connections (ports, certificates, and mTLS setup between kubelet/controller and API server)‚Äù_
so you understand exactly how the traffic moves securely between all these components?

---

## üß† 1Ô∏è‚É£ The Big Picture ‚Äî Who Talks to Whom

Every Kubernetes component that needs real-time cluster updates (kubelet, kube-proxy, controller-manager, scheduler, etc.) communicates **only with the API Server**.

They never talk directly to etcd or to each other.

Here‚Äôs the control plane topology üëá

```mermaid
---
config:
  look: handDrawn
---
graph TD
  subgraph Master["Control Plane Node"]
    A["API Server (TCP 6443)"]
    E["etcd (TCP 2379)"]
    CM["Controller Manager (localhost)"]
    S["Scheduler (localhost)"]
  end
  subgraph Worker["Worker Node"]
    K["kubelet"]
    KP["kube-proxy"]
  end

  K-->|HTTPS 6443|A
  KP-->|HTTPS 6443|A
  CM-->|localhost HTTPS|A
  S-->|localhost HTTPS|A
  A-->|gRPC 2379|E
```

‚úÖ Everything flows **through the API server**.
‚úÖ All communication is **TLS encrypted**.
‚úÖ All clients are **authenticated and authorized**.

---

## ‚öôÔ∏è 2Ô∏è‚É£ The Core Ports

| Port                   | Direction                                      | Purpose                                         |
| ---------------------- | ---------------------------------------------- | ----------------------------------------------- |
| **6443**               | Inbound to API Server                          | Main Kubernetes API (HTTPS + Watch connections) |
| **2379-2380**          | API Server ‚Üí etcd                              | etcd storage and watch                          |
| **10250**              | API Server ‚Üí kubelet                           | Exec, logs, metrics                             |
| **10255 (deprecated)** | kubelet (read-only, disabled by default)       | Old metrics endpoint                            |
| **10257 / 10259**      | Controller-manager / Scheduler HTTPS endpoints | Internal control plane                          |
| **10256**              | kube-proxy healthz                             | Node-local health probe                         |

So when kubelet watches Pods, the connection is:

```plaintext
kubelet (worker) ‚Üí API Server (6443, HTTPS)
```

When API server watches etcd, the connection is:

```plaintext
API Server ‚Üí etcd (2379, gRPC + TLS)
```

---

## üîê 3Ô∏è‚É£ TLS: Mutual Authentication (mTLS)

Every component talking to the API server must **prove its identity** via a certificate.

Let‚Äôs break this down by component üëá

| Component              | Authentication Method     | Cert / Token Source                                   | File Path                                 |
| ---------------------- | ------------------------- | ----------------------------------------------------- | ----------------------------------------- |
| **kubelet**            | x509 client cert          | Generated by kubeadm or issued via CSR                | `/var/lib/kubelet/pki/kubelet-client.crt` |
| **controller-manager** | x509 client cert          | kubeconfig file                                       | `/etc/kubernetes/controller-manager.conf` |
| **scheduler**          | x509 client cert          | kubeconfig file                                       | `/etc/kubernetes/scheduler.conf`          |
| **kubectl (user)**     | x509 cert or Bearer token | `$HOME/.kube/config`                                  | user-dependent                            |
| **kube-proxy**         | Service account token     | `/var/run/secrets/kubernetes.io/serviceaccount/token` |                                           |

‚úÖ API server also presents its own **server certificate**, signed by the cluster CA (`/etc/kubernetes/pki/apiserver.crt`).

---

## ü§ù 4Ô∏è‚É£ The TLS Handshake (Actual Flow)

Let‚Äôs visualize what happens when `kubelet` starts watching Pods.

```mermaid
sequenceDiagram
    participant Kubelet
    participant API as API Server
    participant CA as Cluster CA

    Kubelet->>API: TLS ClientHello (client cert: kubelet-client.crt)
    API-->>CA: Validate cert signature
    CA-->>API: Cert valid ‚úÖ
    API->>Kubelet: TLS ServerHello (apiserver.crt)
    Kubelet-->>CA: Validate server cert
    Kubelet->>API: HTTP GET /api/v1/pods?watch=true
    API-->>Kubelet: 200 OK (chunked JSON stream)
```

Once authenticated, the **HTTPS connection stays open indefinitely** for the Watch stream.

---

## üß© 5Ô∏è‚É£ Example Watch Request on the Wire

Let‚Äôs capture it using `tcpdump`:

```bash
sudo tcpdump -i eth0 port 6443 -A | grep "GET /api/v1/pods"
```

You‚Äôll see something like:

```
GET /api/v1/pods?fieldSelector=spec.nodeName=node1&watch=true HTTP/1.1
Host: kubernetes.default.svc
Authorization: Bearer eyJhbGciOi...
Accept: application/json, */*
```

The response is a **chunked transfer stream**, looking like this:

```
HTTP/1.1 200 OK
Transfer-Encoding: chunked
Content-Type: application/json
```

Then followed by multiple JSON fragments:

```
{"type":"ADDED","object":{...}}
{"type":"MODIFIED","object":{...}}
```

---

## üß† 6Ô∏è‚É£ How API Server Handles Thousands of Watch Connections

Each Watch connection is:

- A persistent **HTTP/1.1 keep-alive** or **HTTP/2** stream
- Multiplexed via **goroutines** inside the API server
- Associated with an **auth context** (user/role/namespace)

Internal data flow:

```mermaid
graph LR
  A[API Server Listener (6443)] -->|accept| G1[Goroutine 1: kubelet node1]
  A -->|accept| G2[Goroutine 2: kubelet node2]
  A -->|accept| G3[Goroutine 3: controller-manager]
  A -->|accept| G4[Goroutine 4: kube-proxy]
```

Each connection independently receives events via the **Watch Fan-Out** system.

---

## üß© 7Ô∏è‚É£ Inside the kubeconfig Files

Each control-plane component authenticates using a kubeconfig file.

Example: `/etc/kubernetes/kubelet.conf`

```yaml
apiVersion: v1
clusters:
  - cluster:
      certificate-authority: /etc/kubernetes/pki/ca.crt
      server: https://10.0.0.1:6443
    name: kubernetes
contexts:
  - context:
      cluster: kubernetes
      user: system:node:worker1
    name: system:node:worker1@kubernetes
users:
  - name: system:node:worker1
    user:
      client-certificate: /var/lib/kubelet/pki/kubelet-client.crt
      client-key: /var/lib/kubelet/pki/kubelet-client.key
current-context: system:node:worker1@kubernetes
```

When the kubelet runs, it loads this config ‚Üí initiates HTTPS ‚Üí verifies the API server‚Äôs CA ‚Üí starts the Watch stream securely.

---

## üß© 8Ô∏è‚É£ How the API Server Verifies Client Identity

Each client certificate‚Äôs **Common Name (CN)** or **Organization (O)** field defines its Kubernetes identity.

| Component          | CN                               | Organization     |
| ------------------ | -------------------------------- | ---------------- |
| kubelet            | `system:node:<node-name>`        | `system:nodes`   |
| controller-manager | `system:kube-controller-manager` | `system:masters` |
| scheduler          | `system:kube-scheduler`          | `system:masters` |
| kubectl user       | user-defined                     | varies           |

The API server uses this to map to **RBAC roles**.
Example RBAC rule for kubelets:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:node
roleRef:
  kind: ClusterRole
  name: system:node
subjects:
  - kind: Group
    name: system:nodes
```

‚úÖ That‚Äôs what allows the kubelet to **watch only its own Pods** securely.

---

## üß© 9Ô∏è‚É£ Watch Connection Lifecycle in Practice

| Phase                        | Description                                |
| ---------------------------- | ------------------------------------------ |
| **1. TLS Handshake**         | Client and server authenticate             |
| **2. HTTP GET /watch**       | Client initiates stream                    |
| **3. Persistent Connection** | API server streams JSON                    |
| **4. Network Idle Timeout**  | If idle too long, keepalive/heartbeat sent |
| **5. Connection Drop**       | Client retries with last `resourceVersion` |

---

## üß© 10Ô∏è‚É£ Example Network Monitoring Commands

### Check all Watch connections to API server:

```bash
sudo ss -tanp | grep 6443
```

### Check TLS sessions:

```bash
sudo openssl s_client -connect <apiserver-ip>:6443
```

### Inspect kubelet‚Äôs certificate:

```bash
openssl x509 -in /var/lib/kubelet/pki/kubelet-client.crt -text | grep Subject
```

Output:

```
Subject: O = system:nodes, CN = system:node:worker1
```

---

## üß† 11Ô∏è‚É£ Full Watch Communication Path

```mermaid
---
config:
  look: handDrawn
---
sequenceDiagram
    participant Kubelet
    participant API as API Server (6443)
    participant ETCD
    participant CA as Cluster CA

    Note over Kubelet,API: üîê Mutual TLS Handshake
    Kubelet->>API: GET /api/v1/pods?watch=true
    API->>ETCD: gRPC watch /registry/pods
    ETCD-->>API: ADDED nginx
    API-->>Kubelet: JSON {"type":"ADDED","object":{"name":"nginx"}}
    Kubelet->>API: ACK /status (Pod Running)
    API->>ETCD: Update /registry/pods/nginx/status
```

Everything ‚Äî from handshake to event delivery ‚Äî is **TLS-encrypted**, **authenticated**, and **stateful**.

---

## üîç 12Ô∏è‚É£ Real Verification (Try in Your Cluster)

Run:

```bash
kubectl get --raw /api/v1/pods?watch=true | head
```

You‚Äôll see the actual JSON stream, e.g.:

```json
{"type":"ADDED","object":{"metadata":{"name":"coredns"}}}
{"type":"MODIFIED","object":{"metadata":{"name":"kube-proxy"}}}
```

That‚Äôs the raw Watch API in action, through port 6443 over HTTPS.

---

## üß© 13Ô∏è‚É£ Summary

| Aspect             | Mechanism                                 |
| ------------------ | ----------------------------------------- |
| **Transport**      | HTTPS (TLS 1.2+ or HTTP/2)                |
| **Port**           | 6443                                      |
| **Authentication** | x509 mTLS or Bearer Token                 |
| **Authorization**  | RBAC / ABAC                               |
| **Persistence**    | HTTP chunked encoding / SPDY              |
| **Security**       | Full TLS chain (CA ‚Üí API Server ‚Üí Client) |
| **Visibility**     | `ss -tanp`, `tcpdump`, `/metrics`         |

---

## ‚ö° Final Takeaways

- All Watch streams are **stateful HTTPS connections**.
- Communication is fully encrypted and identity-verified.
- **No plaintext** or unauthenticated connections ever reach the API server.
- The same architecture applies whether you have 5 nodes or 5,000.
- Each Watch stream rides over port **6443** and fans out internally via the **Watch Cache**.
