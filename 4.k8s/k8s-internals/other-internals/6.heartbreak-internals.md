# ğŸ§© Deep Dive: Keepalives, Bookmarks & Reconnection Logic in Kubernetes Watch Connections

ğŸ‘‰ _â€œHow the kubelet and controller-manager maintain heartbeats and keep their watch connections alive using keepalive probes, Bookmarks, and retry logic (including timeouts and reconnection behavior)â€_?

---

## ğŸ§  1ï¸âƒ£ The Problem â€” Watch Streams Are Long-Lived

Each component (like `kubelet`, `kube-proxy`, or `controller-manager`) opens **one persistent HTTPS connection** to the API server:

```bash
GET /api/v1/pods?watch=true
```

That connection can stay open for **hours or days**, streaming events in real time.
Butâ€¦ networks drop, NATs time out, proxies reset, and TCP connections die silently ğŸ« 

So, Kubernetes needed a way to:

1. Detect stale connections
2. Resume efficiently without losing events
3. Avoid hammering etcd or the API server

---

## âš™ï¸ 2ï¸âƒ£ The Core Reliability Tools

| Mechanism                          | Purpose                     | Works Between       |
| ---------------------------------- | --------------------------- | ------------------- |
| **HTTP Keep-Alive**                | Keeps the TCP socket open   | Client â†” API Server |
| **Heartbeat / Bookmarks**          | Confirms â€œIâ€™m still aliveâ€  | API Server â†’ Client |
| **Reconnect with resourceVersion** | Resume from last event      | Client â†’ API Server |
| **etcd Watch Lease**               | Prevent stale etcd watchers | API Server â†” etcd   |
| **Retry-After & Backoff**          | Avoid overload on reconnect | Client              |

---

## ğŸ“¡ 3ï¸âƒ£ HTTP Keep-Alive â€” The Foundation

Each Watch uses **HTTP/1.1 persistent connections** or **HTTP/2 streams**.

- The TCP socket is reused.
- Periodic TCP keepalives prevent idle disconnection.
- API server sends data with **Transfer-Encoding: chunked**, so the connection never â€œcompletesâ€.

You can check it in flight:

```bash
curl -v -k https://<apiserver>:6443/api/v1/pods?watch=true
```

Look for:

```
Connection: keep-alive
Transfer-Encoding: chunked
```

âœ… If nothing changes for a while, the socket still stays alive thanks to TCP keepalive probes.

---

## ğŸ§© 4ï¸âƒ£ The Watch Bookmark Mechanism

Kubernetes added **Bookmark events** in v1.15 to solve the â€œsilent timeoutâ€ issue.

**Without Bookmarks:**
If no Pods change for 5 minutes, the client might think the connection is dead.

**With Bookmarks:**
The API server periodically sends:

```json
{
  "type": "BOOKMARK",
  "object": {
    "metadata": {
      "resourceVersion": "453982"
    }
  }
}
```

â¡ï¸ This acts as a _lightweight heartbeat_ â€” confirming both sides are still connected.

No object data = no load on API server or etcd.

### Enable in your Watch:

```bash
GET /api/v1/pods?watch=true&allowWatchBookmarks=true
```

---

## ğŸ§© 5ï¸âƒ£ Client-Side Reconnection Logic

Every Kubernetes component implements a â€œwatch loopâ€ roughly like this:

```go
for {
    rv := getLastResourceVersion()
    watch, err := apiserver.Watch(resource, rv)
    if err != nil {
        sleep(backoff)
        continue
    }

    for event := range watch.ResultChan() {
        if event.Type == ERROR && event.Code == 410 {
            // Resource version too old, do a full list again
            fullList()
        }
        process(event)
    }
}
```

So if the network drops or the Watch ends, it **re-establishes** automatically.

âœ… The client uses the latest `resourceVersion` from the last event or bookmark
âœ… This prevents full re-lists (saves etcd load)

---

## ğŸ§  6ï¸âƒ£ When Watch Connections Close

| Cause                   | API Server Response | Client Action                       |
| ----------------------- | ------------------- | ----------------------------------- |
| Timeout / Idle          | EOF / socket close  | Reconnect with last resourceVersion |
| API server restart      | Connection reset    | Reconnect automatically             |
| ResourceVersion too old | HTTP 410 Gone       | Full re-list                        |
| RBAC or cert change     | HTTP 403 Forbidden  | Retry after token refresh           |
| Network partition       | TCP timeout         | Retry after exponential backoff     |

Example 410 response:

```json
{ "type": "ERROR", "object": { "kind": "Status", "code": 410, "reason": "Gone" } }
```

Meaning: â€œYour bookmark is too old â€” start over.â€

---

## ğŸ” 7ï¸âƒ£ Exponential Backoff (Client Side)

To avoid overloading the API server when many watchers reconnect:

| Attempt | Wait Time  |
| ------- | ---------- |
| 1       | 200 ms     |
| 2       | 400 ms     |
| 3       | 800 ms     |
| 4       | 1.6 s      |
| â€¦       | up to 30 s |

Implemented in client-goâ€™s `RetryWatcher` and `Reflector`.

So after a cluster restart, thousands of kubelets reconnect gracefully â€” not all at once ğŸš¦

---

## ğŸ§© 8ï¸âƒ£ Reflectors & Informers â€” The Smart Cache Layer

All high-level controllers use the **Informer framework**, which wraps the Watch logic.

Components:

- **Reflector** â†’ maintains local cache from Watch
- **Informer** â†’ provides event handlers (`onAdd`, `onUpdate`, `onDelete`)
- **Indexer** â†’ stores objects in memory for quick lookups

```mermaid
graph TD
  A[API Server]
  B[Reflector (Watch Loop)]
  C[Informer (Handlers)]
  D[Indexer (Local Store)]
  A-->|Watch Events|B-->C-->D
```

âœ… Bookmarks update the resourceVersion in the Reflector
âœ… If Watch breaks, it relists starting from that version

---

## âš™ï¸ 9ï¸âƒ£ Heartbeats at the Node Level

Separate from Watch events, kubelet also maintains **node heartbeats**:

- Every 10 s: sends a `PATCH /api/v1/nodes/<name>/status`
- Updates `status.lastHeartbeatTime`
- If missed for > 40 s, NodeController marks node as `NotReady`

This is **not a Watch**, but an API heartbeat that works _alongside_ the Watch mechanism.

---

## ğŸ“Š 10ï¸âƒ£ Monitoring Watch Health

You can check Watch metrics from the API server:

```bash
curl -k https://<apiserver>:6443/metrics | grep watch
```

Key metrics:

| Metric                               | Meaning                     |
| ------------------------------------ | --------------------------- |
| `apiserver_watch_events_total`       | Number of events sent       |
| `apiserver_watch_duration_seconds`   | Duration of Watch sessions  |
| `apiserver_watch_bookmarks_total`    | Count of bookmark events    |
| `apiserver_watch_terminations_total` | Number of Watch disconnects |

---

## ğŸ§© 11ï¸âƒ£ Visual: Watch Reconnection Lifecycle

```mermaid
sequenceDiagram
    participant Kubelet
    participant API as API Server
    participant ETCD

    Note over Kubelet,API: ğŸ” TLS Keep-Alive active
    Kubelet->>API: GET /api/v1/pods?watch=true&allowWatchBookmarks=true
    API-->>Kubelet: {"type":"ADDED","object":{...}}
    API-->>Kubelet: {"type":"BOOKMARK","object":{"resourceVersion":"105"}}
    Note right of Kubelet: Connection lost (timeout)
    Kubelet->>API: Reconnect with resourceVersion=105
    API-->>Kubelet: Resume stream from 105 âœ…
```

âœ… Zero event loss
âœ… Zero full reload
âœ… Zero manual intervention

---

## ğŸ§  12ï¸âƒ£ ResourceVersion Lifecycle Recap

| Phase            | Example      | Description                  |
| ---------------- | ------------ | ---------------------------- |
| **Initial list** | `rv=100`     | Snapshot of current state    |
| **Streaming**    | `rv=101â€“150` | Continuous updates           |
| **Bookmark**     | `rv=150`     | Keepalive marker             |
| **Reconnect**    | `rv=150`     | Resumes from last seen event |
| **410 Gone**     | `rv=expired` | Full list again              |

---

## âš¡ 13ï¸âƒ£ Watch vs Heartbeat Summary

| Aspect      | Watch (API Stream)          | Heartbeat (Node Status) |
| ----------- | --------------------------- | ----------------------- |
| Protocol    | HTTPS (chunked/stream)      | HTTPS (PATCH request)   |
| Direction   | Server â†’ Client             | Client â†’ Server         |
| Trigger     | Object change               | Time-based              |
| Example     | Pod ADDED                   | Node heartbeat          |
| Reliability | Bookmarks + resourceVersion | Periodic resync         |

---

## ğŸ” 14ï¸âƒ£ Debugging Tips

### List active watchers

```bash
kubectl get --raw /metrics | grep watch
```

### Check API server logs for disconnects

```bash
journalctl -u kube-apiserver | grep watch
```

### See Reflector state

If youâ€™re writing a controller:

```go
reflector.LastSyncResourceVersion()
```

---

## ğŸ“š 15ï¸âƒ£ Key Takeaways

- Every Watch connection stays alive through:

  - **TCP keepalive**
  - **Periodic Bookmarks**
  - **Automatic reconnects using resourceVersion**

- Kubernetes avoids event loss by replaying from cache.
- Exponential backoff protects the API server during reconnect storms.
- Informers wrap all of this into an elegant, self-healing layer.
- Bookmarks are **lightweight heartbeats** â€” no data, just sync.

---

âœ… **Result:**
Even if your cluster restarts, API servers reboot, or networks flap â€” all controllers, schedulers, kubelets, and proxies stay consistent **without polling or downtime**.
