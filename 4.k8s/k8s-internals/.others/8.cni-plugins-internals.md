# ğŸ§© Deep Dive: How kubelet + CNI Plugins Create Pod Networking

_Behind the Scenes of Pod-to-Pod Connectivity_

ğŸ‘‰ _â€œHow kubelet, CNI plugins (like bridge, flannel, calico), and Linux network namespaces integrate together to attach Pods to the cluster network â€” including how API Server events trigger CNI calls under the hoodâ€?_

---

## ğŸ§  1ï¸âƒ£ The Big Picture â€” Who Does What

When a Pod is scheduled to a Node:

| Component                                      | Role                                               |
| ---------------------------------------------- | -------------------------------------------------- |
| **kubelet**                                    | Manages Pod lifecycle and triggers CNI plugins     |
| **CNI Plugin**                                 | Creates Podâ€™s network namespace, veth pair, and IP |
| **CNI Network Plugin (e.g., flannel, calico)** | Connects Nodeâ€™s bridge to cluster overlay          |
| **API Server**                                 | Keeps track of Podâ€™s IP and status                 |
| **etcd**                                       | Stores the authoritative cluster state             |

Everything is orchestrated via **kubelet â†’ CNI plugin** calls.

---

## âš™ï¸ 2ï¸âƒ£ The Lifecycle of Pod Networking

Hereâ€™s the full sequence of what happens when a Pod is created ğŸ‘‡

```mermaid
sequenceDiagram
    participant API as API Server
    participant Kubelet
    participant CNI as CNI Plugin (bridge/flannel/calico)
    participant Kernel as Linux Kernel
    participant ETCD

    API->>ETCD: Store Pod spec (Pending)
    API-->>Kubelet: Notify scheduled Pod
    Kubelet->>CNI: ADD command (CNI spec)
    CNI->>Kernel: Create netns, veth pair, bridge connect
    CNI-->>Kubelet: Return Pod IP
    Kubelet->>API: Update Pod.status.podIP
    API->>ETCD: Commit updated Pod object
    API-->>Other Pods: Watch event (Pod Ready)
```

âœ… Every Pod network is born this way â€” in real-time, per-node.

---

## ğŸ§© 3ï¸âƒ£ Step-by-Step Breakdown (Example Pod Creation)

### ğŸª„ Step 1: Scheduler assigns the Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
    - name: web
      image: nginx
```

Once created, itâ€™s **Pending** until the **scheduler** picks a Node:

```json
"spec": { "nodeName": "worker-node-1" }
```

---

### ğŸª„ Step 2: kubelet receives Pod spec

On `worker-node-1`, kubeletâ€™s **watch loop** gets:

```json
{ "type": "ADDED", "object": { "kind": "Pod", "metadata": { "name": "nginx" } } }
```

Kubelet now must create:

- A container runtime sandbox (Docker, containerd, CRI-O)
- A network namespace (netns)
- Network interfaces (via CNI plugin)

---

### ğŸª„ Step 3: kubelet triggers CNI `ADD`

Kubelet executes a **CNI binary** (e.g., `/opt/cni/bin/flannel` or `bridge`)
with a JSON payload via stdin:

```json
{
  "cniVersion": "0.4.0",
  "name": "k8s-pod-network",
  "type": "bridge",
  "containerId": "abcd1234",
  "netns": "/proc/1234/ns/net",
  "ifName": "eth0",
  "args": "K8S_POD_NAMESPACE=default;K8S_POD_NAME=nginx",
  "prevResult": {}
}
```

This is the **CNI specification** â€” every plugin must follow it.

---

## âš™ï¸ 4ï¸âƒ£ CNI Plugin Creates the Pod Network

The plugin performs these actions inside the nodeâ€™s Linux kernel ğŸ‘‡

| Step | Operation                                      | Linux Object                  |
| ---- | ---------------------------------------------- | ----------------------------- |
| 1    | Create **network namespace** for the container | `/proc/[pid]/ns/net`          |
| 2    | Create **veth pair**                           | `vethA` (host) â†” `eth0` (pod) |
| 3    | Attach host-side `vethA` to Linux bridge       | `cni0`                        |
| 4    | Assign IP to Pod                               | `10.244.1.15`                 |
| 5    | Configure default route via bridge             |                               |
| 6    | Return IP info to kubelet                      |                               |

---

### ğŸ§± Visual â€” Pod Network Creation

```mermaid
graph TD
  subgraph "Worker Node"
    subgraph "Pod netns"
      E[eth0 (10.244.1.15)]
    end
    H[vethA] -- bridge --> B[cni0 (Bridge)]
    B -- route --> N[eth0 (Node interface)]
  end
  CNI[CNI Plugin] --> E
  Kubelet --> CNI
```

âœ… Now the Pod has its own isolated network namespace, connected to the Node bridge.

---

## ğŸŒ‰ 5ï¸âƒ£ Node-to-Pod Connectivity

When you `ping` the Pod IP from the Node:

```bash
ping 10.244.1.15
```

The packet travels like this:

```mermaid
sequenceDiagram
    participant Node
    participant Bridge as cni0
    participant PodNet as Pod eth0

    Node->>Bridge: ARP resolve 10.244.1.15
    Bridge->>PodNet: Forward via vethA
    PodNet-->>Node: ICMP reply
```

All local Pod-to-Pod traffic on the same node goes through the `cni0` bridge.

---

## ğŸŒ 6ï¸âƒ£ Cross-Node Pod Communication (Overlay Network)

Now what if the Pod is on a **different node**?

Thatâ€™s where **CNI network plugins** like **Flannel**, **Calico**, or **Cilium** come in.

They extend the connectivity **across nodes** using one of these mechanisms:

| CNI Type    | Overlay Tech  | Data Plane             |
| ----------- | ------------- | ---------------------- |
| **Flannel** | VXLAN         | UDP encapsulation      |
| **Calico**  | BGP routing   | Native Layer 3         |
| **Weave**   | Fast UDP mesh | Peer overlay           |
| **Cilium**  | eBPF          | Kernel bypass datapath |

---

### Example: Flannel VXLAN Encapsulation

```mermaid
graph TD
  A[Pod A 10.244.1.5 on Node1] -->|VXLAN UDP Tunnel| B[Pod B 10.244.2.7 on Node2]
  subgraph Node1
    A-->VNI1[VXLAN Interface flannel.1]
  end
  subgraph Node2
    VNI2[VXLAN Interface flannel.1]-->B
  end
```

ğŸ§  Flannel maintains a mapping of:

```
10.244.1.0/24 â†’ Node1
10.244.2.0/24 â†’ Node2
```

So traffic between Pods is encapsulated and routed automatically.

---

## âš™ï¸ 7ï¸âƒ£ kubelet Updates API Server

Once the Podâ€™s IP is assigned:

```json
"status": {
  "podIP": "10.244.1.15",
  "phase": "Running"
}
```

kubelet PATCHes it to the API server:

```bash
PATCH /api/v1/namespaces/default/pods/nginx/status
```

Then CoreDNS and kube-proxy receive the Watch events we studied earlier â€”
updating DNS and routing in real time.

---

## ğŸ§  8ï¸âƒ£ Inside the CNI Plugin Directory

Every node has:

```
/etc/cni/net.d/
â”œâ”€â”€ 10-flannel.conflist
/opt/cni/bin/
â”œâ”€â”€ bridge
â”œâ”€â”€ flannel
â”œâ”€â”€ loopback
```

Example config:

```json
{
  "cniVersion": "0.4.0",
  "name": "flannel",
  "type": "flannel",
  "delegate": {
    "type": "bridge",
    "bridge": "cni0",
    "isGateway": true
  }
}
```

---

## ğŸ§© 9ï¸âƒ£ CNI Commands

| Command   | Purpose                                |
| --------- | -------------------------------------- |
| `ADD`     | Create network interface and assign IP |
| `DEL`     | Remove interface and release IP        |
| `CHECK`   | Verify connectivity                    |
| `VERSION` | Return pluginâ€™s version                |

All CNI plugins must implement these in Go or C.

---

## ğŸ§  10ï¸âƒ£ Behind-the-Scenes: Containerd & kubelet Integration

When using **containerd**:

- kubelet calls containerd via **CRI (Container Runtime Interface)**
- containerd launches a sandbox container (pause container)
- Then calls CNI plugin to attach the pause container network namespace

The â€œpauseâ€ container keeps the Pod network namespace alive even when app containers restart.

---

### Example Debug

```bash
crictl inspectp <POD_ID> | jq .info.runtimeSpec.linux.namespaces
```

Shows:

```json
[{ "type": "network", "path": "/proc/1234/ns/net" }]
```

---

## ğŸ” 11ï¸âƒ£ Hands-On Verification

### View Pod network namespaces:

```bash
sudo lsns -t net
```

### See veth pairs:

```bash
ip link | grep veth
```

### Check CNI bridge:

```bash
ip addr show cni0
```

### Show routing:

```bash
ip route | grep 10.244
```

### Capture overlay traffic:

```bash
tcpdump -i flannel.1 udp port 8472
```

---

## ğŸ§© 12ï¸âƒ£ Visual: Full Multi-Node Flow

```mermaid
---
config:
  look: handDrawn
---
graph TD
  subgraph "Node 1"
    A1[Pod A eth0 10.244.1.5]--veth-->B1[cni0]
    B1-->C1[flannel.1]
    C1-->E1[eth0]
  end
  subgraph "Node 2"
    A2[Pod B eth0 10.244.2.7]--veth-->B2[cni0]
    B2-->C2[flannel.1]
    C2-->E2[eth0]
  end
  E1--UDP VXLAN Tunnel-->E2
  A1--Encapsulated Packet-->A2
```

âœ… Pod-to-Pod communication flows over the overlay network (VXLAN encapsulation).
âœ… IPs remain unique across cluster (`10.244.x.x`).

---

## ğŸ§  13ï¸âƒ£ Summary Table

| Component            | Responsibility                  | Protocol/Mechanism |
| -------------------- | ------------------------------- | ------------------ |
| **kubelet**          | Calls CNI plugin                | Local exec         |
| **CNI plugin**       | Creates interfaces & assigns IP | netlink syscalls   |
| **Flannel / Calico** | Inter-node routing              | VXLAN / BGP        |
| **API Server**       | Stores Pod IP and emits events  | HTTPS/Watch        |
| **CoreDNS**          | Updates DNS records             | Watch              |
| **kube-proxy**       | Updates routing rules           | iptables/IPVS      |

---

## âš¡ 14ï¸âƒ£ Key Takeaways

- kubelet **triggers** CNI plugin on Pod creation via `ADD` command
- CNI plugin **creates the Pod network namespace** and connects it to node bridge
- Flannel/Calico **extend connectivity across nodes** using overlay or routing
- API Server broadcasts the Podâ€™s IP via Watch events to all components
- The â€œpauseâ€ container maintains Pod network lifetime
- All traffic is **mTLS secured** and **event-driven**, not static or polled

---

Would you like to continue with the **next advanced lab** â€”
ğŸ‘‰ _â€œHow Kubernetes Service routing works internally â€” ClusterIP, NodePort, LoadBalancer, and how kube-proxy translates service traffic to Pod endpoints (with iptables and IPVS examples)â€_ â€” to complete the full network path from client to Pod?
