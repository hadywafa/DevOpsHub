# âš™ï¸ **API Server Watch Architecture**

_(â€œHow One Pod Creation Event Reaches Thousands of Clients Efficientlyâ€)!_

---

## ğŸ”´ **The Problem** â€” Scaling the Watch API

Imagine 5,000 kubelets, each doing:

```bash
GET /api/v1/pods?fieldSelector=spec.nodeName=<node>&watch=true
```

When a single Pod is created, the API server must **notify every watcher** that cares about it â€” without querying etcd for each.

If it fetched from etcd 5,000 times per event, etcd would die instantly ğŸ’€.

---

## ğŸŸ¢ **The Solution** â€” Watch Cache + Fan-Out System

Kubernetes solves this using two magic layers inside the API Server:

<div align="center" style="background-color: #119684ff;color:#000; border-radius: 10px; border: 2px solid">

| Layer             | Role                                                   |
| ----------------- | ------------------------------------------------------ |
| **Watch Cache**   | Local in-memory copy of recent objects for fast access |
| **Watch Fan-Out** | One etcd stream â†’ many client streams                  |

</div>

Letâ€™s visualize it ğŸ‘‡

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
---
config:
  look: handDrawn
---
graph TD
  subgraph ETCD
    E["etcd (Pod objects)"]
  end
  subgraph API["API Server"]
    C["Watch Cache (Pods, Nodes, etc.)"]
    F["Fan-Out Engine"]
  end
  subgraph Clients
    K1["kubelet (node1)"]
    K2["kubelet (node2)"]
    K3["kube-proxy"]
    K4["controller-manager"]
  end

  E-->|gRPC watch|C
  C-->|JSON event stream|F
  F-->|fan-out|K1
  F-->|fan-out|K2
  F-->|fan-out|K3
  F-->|fan-out|K4
```

</div>

---

> âœ… Result: etcd is hit **once**, and thousands of watchers receive the update in milliseconds.

---

## ğŸªœ **The â€œList + Watchâ€ Pattern**

Every Kubernetes Watch session begins with a **list**, then continues as a **watch**.

### ğŸ§© Step 1 â€” List

The client gets the _current snapshot_:

```bash
GET /api/v1/pods
```

Response:

```json
{
  "items": [...],
  "metadata": {
    "resourceVersion": "25692"
  }
}
```

### ğŸ§© Step 2 â€” Watch

Then it asks:

```bash
GET /api/v1/pods?watch=true&resourceVersion=25692
```

This means:

> â€œStart sending me changes _after_ resourceVersion 25692.â€

If network drops, it reconnects using the **latest version** it saw.

---

## âš™ï¸ **The Watch Cache**

Each API server stores a **sliding window of recent changes** per resource type.

<div align="center" style="background-color: #119684ff;color:#000; border-radius: 10px; border: 2px solid">

| Resource | Cache Size | Stored Data                    |
| -------- | ---------- | ------------------------------ |
| Pods     | 1000       | Object snapshots + event types |
| Nodes    | 500        | Status and metadata            |
| Services | 500        | Spec + clusterIP               |

</div>

When you list Pods, the API server serves it **directly from the cache**, not etcd â€” drastically reducing etcd load.

ğŸ“ Internal code path:

```ini
pkg/storage/cacher.go
pkg/apiserver/watchcache.go
```

---

## ğŸª­ **The Fan-Out System**

Once an event (like Pod creation) arrives from etcd â†’
API serverâ€™s WatchCache â†’ Fan-Out engine checks all active watchers.

Each watcher has:

- Resource filter (namespace, fieldSelector, labelSelector)
- Last seen `resourceVersion`

The Fan-Out engine:

- Sends event only if it matches
- Updates watcherâ€™s `resourceVersion`
- Uses **non-blocking queues** per watcher to avoid backpressure

ğŸ‘‰ So even if one slow watcher lags, others still get updates in real-time.

---

## ğŸ“ **Real Flow Example** â€” Pod Created

Letâ€™s trace it step-by-step visually:

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
sequenceDiagram
    participant User as kubectl
    participant API as API Server
    participant Cache as Watch Cache
    participant ETCD
    participant Kubelet

    User->>API: POST /api/v1/pods (create nginx)
    API->>ETCD: Write new Pod object
    ETCD-->>API: Notify via gRPC Watch (ADDED)
    API->>Cache: Update local Watch Cache
    Cache-->>Kubelet: Send JSON event (ADDED nginx)
    Cache-->>Controller: Send JSON event (ADDED nginx)
    Cache-->>KubeProxy: Send JSON event (ADDED nginx)
```

</div>

âœ… One etcd write â†’ hundreds of clients updated instantly.

---

## ğŸ”„ **Handling Missed Events** â€” ResourceVersion Logic

When a client reconnects (due to timeout or network issue):

- It provides its **last known `resourceVersion`**
- API server sends only **new events**
- If the requested version is too old (expired), client does a **re-list**

Example:

```bash
GET /api/v1/pods?watch=true&resourceVersion=102394
```

If cache doesnâ€™t have older events â†’
Response:

```json
{ "type": "ERROR", "object": { "code": 410, "reason": "Gone" } }
```

That means:

> â€œToo old! Do a full list again.â€

---

## ğŸ”§ **Performance Features**

<div align="center" style="background-color: #119684ff;color:#000; border-radius: 10px; border: 2px solid">

| Optimization               | Description                                        |
| -------------------------- | -------------------------------------------------- |
| **Bookmark events**        | Lightweight heartbeat messages (no object payload) |
| **Chunked list responses** | Large lists split into pages                       |
| **Server-Side filtering**  | Label/field selectors reduce event size            |
| **Compression**            | gzip/deflate supported for Watch streams           |
| **WatchBookmarks=true**    | Reduces memory and keeps clients synced            |

</div>

---

## ğŸ“ **Example**: API Server Config Flags

You can tune cache and watch behavior in `kube-apiserver`:

```bash
--watch-cache=true
--watch-cache-sizes=pods#1000,nodes#500
--max-mutating-requests-inflight=200
--max-requests-inflight=400
```

---

## âš¡ **Efficiency Recap**

| Step                | Data Source            | Description                          |
| ------------------- | ---------------------- | ------------------------------------ |
| 1ï¸âƒ£ Client List      | Watch Cache            | Gets full snapshot (fast, in-memory) |
| 2ï¸âƒ£ Client Watch     | Fan-Out Engine         | Streams future changes               |
| 3ï¸âƒ£ API Server       | etcd Watch             | Receives low-level key changes       |
| 4ï¸âƒ£ Cache Sync       | Event replay           | Keeps memory state in sync           |
| 5ï¸âƒ£ Client Reconnect | ResourceVersion resume | Efficient delta sync                 |

---

## ğŸ“ **Example Metrics** (via `/metrics`)

You can inspect the cache behavior:

```bash
curl -k https://<apiserver>:6443/metrics | grep watch
```

Example output:

```ini
apiserver_watch_events_total{resource="pods"} 13240
apiserver_watch_cache_list_duration_seconds_count{resource="pods"} 54
apiserver_watch_fanout_total{resource="pods"} 2300
```

---

## ğŸ–¼ï¸ **Full End-to-End Watch Data Flow**

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
---
config:
  look: handDrawn
---
graph TD
  subgraph ETCD["etcd (Persistent store)"]
    KV["Pod Key-Value store"]
  end
  subgraph API["API Server"]
    WC["Watch Cache"]
    FO["Fan-Out Engine"]
  end
  subgraph Clients
    C1["kubelet"]
    C2["controller-manager"]
    C3["kube-proxy"]
  end

  KV-->|gRPC stream|WC
  WC-->|cached snapshot|FO
  FO-->|HTTPS chunked JSON|C1
  FO-->|HTTPS chunked JSON|C2
  FO-->|HTTPS chunked JSON|C3
```

</div>

---

## ğŸ **Key Takeaways**

- The **Watch Cache** prevents API server from hammering etcd.
- The **Fan-Out Engine** efficiently sends updates to many clients.
- Every Watch starts as a **List + Watch** pattern.
- **ResourceVersion** ensures continuity between sessions.
- API server â†’ etcd communication uses **gRPC Watch streams**.
- Kubernetes can scale to **thousands of watchers** with minimal latency.
