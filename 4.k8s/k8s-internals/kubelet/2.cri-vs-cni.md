# âš”ï¸ **CNI vs CRI Simplified**

## ğŸš§ **Kubernetes Pod Creation**

When Kubernetes creates a **Pod**, it actually builds a **set of Linux namespaces** that together form the Podâ€™s isolated â€œmini-environment.â€

Each namespace isolates a different part of the system (**network**, **processes**, **filesystem**, etc.).  
Different Kubernetes components are responsible for creating and configuring these namespaces.

---

## âš™ï¸ **Namespace Ownership**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Namespace Type          | Created / Managed By                  | Description                                                                                                                                                |
| ----------------------- | ------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Network Namespace**   | **CNI (Container Network Interface)** | Creates the Podâ€™s virtual network environment â€” veth pairs, IP address, routes, and DNS. All containers in the same Pod share this same network namespace. |
| **PID Namespace**       | **CRI (Container Runtime Interface)** | Creates the containerâ€™s process tree. Each container has its own PID namespace â€” isolates its running processes.                                           |
| **Mount Namespace**     | **CRI (via OCI runtime)**             | Manages the containerâ€™s filesystem â€” mounts, volumes, and rootfs.                                                                                          |
| **IPC / UTS Namespace** | **CRI (via OCI runtime)**             | Optional namespaces for hostname and inter-process communication isolation.                                                                                |

</div>

---

## ğŸªœ **End-to-End Flow**

1. **Kubelet** requests container creation from **CRI** (e.g., containerd or CRI-O).
2. **CRI** first creates the Pod â€œsandboxâ€ â†’ this defines the **network namespace**.
3. **CRI** then calls the **CNI plugin** to set up networking (IP, veth pair, routes).
4. After networking is ready, **CRI** launches the actual container process via **OCI runtime** (`runc`), which creates:

   - PID namespace
   - Mount namespace
   - UTS/IPC namespaces
   - Joins the existing network namespace (created by CNI)

5. The container starts running inside this fully isolated set of namespaces.

---

## ğŸ§  In One Line

> **CNI builds the Podâ€™s network home**,  
> **CRI builds the Podâ€™s processes and filesystem inside that home**,  
> and **Kubelet orchestrates both** to create a running containerized environment.

---

## ğŸ“– **Whatâ€™s the Real Job of CNI?**

The **Container Network Interface (CNI)** is **not just a plugin** â€” itâ€™s the _protocol contract_ that glues **Kubernetes**, **container runtimes (like containerd or Docker)**, and **the Linux network stack** together.

ğŸ§© Think of it this way:

> Kubelet says: â€œHey runtime, I need a new Pod with its own network.â€  
> Runtime says: â€œSure, but I donâ€™t know how to give it an IP.â€  
> CNI steps in: â€œRelax, Iâ€™ll create the network namespace, attach a veth, assign an IP, add routes, apply network policies â€” all cleanly defined in JSON.â€

---

## ğŸ”´ **The Problem Before CNI**

Before CNI, every runtime did **its own network magic**.

**Example: Using Docker (pre-CNI):**

When Kubernetes originally used **Docker directly**:

1. Docker created containers with its own network driver (`bridge`, `overlay`, etc.).
2. Each Pod was really a **group of Docker containers** sharing one network namespace.
3. Docker decided how to create the **veth pairs**, **bridges**, **iptables**, and **routing**.

The result?

âš ï¸ **Chaos**:

- kubelet couldnâ€™t predict how IPs were managed.
- Each runtime (Docker, rkt, cri-o) had different network handling.
- You couldnâ€™t apply cluster-wide network policies.
- Troubleshooting was a nightmare.

So Kubernetes said:  
ğŸ‘‰ â€œLetâ€™s standardize this. Iâ€™ll ask the runtime to just _call an external plugin_ via a well-defined interface.â€

Thatâ€™s CNI.

---

## ğŸ¨ **CNI Architecture**

 <div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
graph TD
  A[Kubelet] -->|"gRPC (CRI)"| B["Container Runtime (containerd)"]
  B -->|Exec CNI Plugin binary| C["CNI Plugin (e.g., Calico, Flannel)"]
  C -->|Configure| D[Linux Network Stack]
  D --> E[Pod Network Namespace]
```

</div>

### Components:

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Component                                   | Role                                                                 |
| ------------------------------------------- | -------------------------------------------------------------------- |
| **Kubelet**                                 | Asks runtime to create Pod network                                   |
| **CRI (Container Runtime Interface)**       | Abstraction layer between kubelet and runtime                        |
| **Container Runtime (containerd, dockerd)** | Executes `CNI ADD/DEL` commands                                      |
| **CNI Plugin**                              | Implements actual networking (creates interfaces, assigns IPs, etc.) |
| **Linux Kernel**                            | The real engine â€” handles namespaces, routes, veth pairs             |

</div>

---

## âš™ï¸ **Step-by-Step**: **What Happens When a Pod Is Created** `(With CNI)`

Letâ€™s walk through a **real flow** from YAML â†’ Running Pod.

### ğŸ”¹ 1. Pod Spec submitted

```bash
kubectl apply -f nginx-pod.yaml
```

The Pod spec reaches the **API Server**, which schedules it to a node.

---

### ğŸ”¹ 2. Kubelet â†’ Container Runtime

Kubelet on that node calls the **CRI**:

```bash
RunPodSandboxRequest
```

This tells the runtime (e.g., `containerd`) to set up the Pod _sandbox_ â€” including networking.

---

### ğŸ”¹ 3. Runtime calls CNI Plugin

`containerd` looks at its config:

```ini
/etc/cni/net.d/10-calico.conflist
```

Example content:

```json
{
  "cniVersion": "0.4.0",
  "name": "k8s-pod-network",
  "plugins": [
    {
      "type": "calico",
      "ipam": {
        "type": "calico-ipam"
      }
    }
  ]
}
```

Runtime executes:

```bash
/opt/cni/bin/calico add <containerID> <netns> <ifName>
```

---

### ğŸ”¹ 4. Plugin Creates the Network

The plugin (`calico`, `flannel`, `weave`, etc.):

1. Creates a **veth pair**

   - one end in Podâ€™s network namespace (`eth0`)
   - other in the nodeâ€™s namespace (e.g., `cali12345`)

2. Assigns an **IP address** (via IPAM plugin)

3. Adds **routes** inside and outside the namespace

4. Updates **iptables** or **eBPF** rules

5. Registers the IP in its **datastore (etcd or CRD)** for cross-node routing

---

### ğŸ”¹ 5. CNI Returns JSON to Runtime

Plugin responds like this:

```json
{
  "cniVersion": "0.4.0",
  "interfaces": [{ "name": "eth0", "sandbox": "/var/run/netns/1234" }],
  "ips": [{ "address": "10.244.1.5/24", "gateway": "10.244.1.1" }]
}
```

Runtime passes this info to kubelet â†’ kubelet updates Pod status:

```yaml
status:
  podIP: 10.244.1.5
```

âœ… Pod now has network access!

---

## ğŸ” **What About Without CNI** (Old Docker Way)?

Letâ€™s see what happens if CNI didnâ€™t exist.

### ğŸ”§ Old Way (Docker Bridge)

Docker creates a default `docker0` bridge:

```bash
docker network inspect bridge
```

Every container gets:

- An IP from the bridge subnet (e.g., `172.17.0.0/16`)
- Routes to talk only within that host
- NAT rules for outbound Internet

âš ï¸ Problems:

- No cross-node connectivity
- Each node reused same IP range â†’ collisions
- No control over Pod-to-Pod or Pod-to-Service routing

---

### âš¡ New Way (CNI)

Now, CNI plugins ensure:

- Each Pod has a **unique routable IP cluster-wide**
- Routes between nodes are automatically managed (via overlay, BGP, VXLAN, etc.)
- Policies (deny/allow traffic) are declarative

---

## ğŸ”¬ Example Comparison

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Feature                | Docker Networking (No CNI) | Kubernetes + CNI              |
| ---------------------- | -------------------------- | ----------------------------- |
| **Pod IP Scope**       | Node-local only            | Cluster-wide unique           |
| **Cross-node traffic** | âŒ NAT-based               | âœ… Overlay or routed          |
| **Network policy**     | âŒ Impossible              | âœ… Supported (Calico, Cilium) |
| **IP management**      | Random, per node           | Centralized via IPAM          |
| **Extensibility**      | Hard-coded drivers         | Modular CNI plugin chain      |

</div>

---

## ğŸ§° Real Example (Containerd + Calico Flow)

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
sequenceDiagram
  participant K as Kubelet
  participant R as containerd
  participant CNI as Calico Plugin
  participant L as Linux Kernel
  participant ET as etcd (Calico datastore)

  K->>R: RunPodSandbox()
  R->>CNI: Exec "ADD" with Pod namespace and ID
  CNI->>L: Create veth pair (eth0 <-> cali123)
  CNI->>L: Assign IP and routes
  CNI->>ET: Store Pod IP and node info
  CNI-->>R: Return IP + interface JSON
  R-->>K: Report Pod IP ready
  K->>API: Update Pod.status.podIP
```

</div>

---

## ğŸ§© Common CNI Plugins and How They Work

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Plugin                      | Mode            | Description                                 |
| --------------------------- | --------------- | ------------------------------------------- |
| **Flannel**                 | Overlay (VXLAN) | Simple Layer 3 overlay network              |
| **Calico**                  | BGP / eBPF      | High performance; supports network policies |
| **Weave**                   | Overlay         | Auto-meshes nodes, easy to deploy           |
| **Cilium**                  | eBPF-based      | Fast, modern, security-aware                |
| **Azure CNI / AWS VPC CNI** | Native          | Integrates Pods into cloud VPC directly     |

</div>

---

## ğŸ§  Key Takeaways

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Concept                                    | Description                                  |
| ------------------------------------------ | -------------------------------------------- |
| **CNI is a contract**, not a single plugin | Defines JSON-based ADD/DEL lifecycle         |
| **Runtime executes plugin binaries**       | containerd, cri-o, etc. just follow CNI spec |
| **Plugins implement real network logic**   | Create veths, bridges, routing, IPAM, etc.   |
| **Every Pod = isolated namespace + veth**  | Each has its own IP                          |
| **Before CNI, Docker owned networking**    | Now itâ€™s standardized and modular            |

</div>

---

## ğŸ’ Visual Recap

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
graph TD
  A[Kubelet] -->|CRI call| B[containerd]
  B -->|Exec ADD| C[CNI Plugin]
  C --> D[Linux Kernel]
  D --> E[Pod Network Namespace]
  C -->|Store IP| F[etcd / Plugin datastore]
  E -->|Send traffic| D
```

</div>

---

âœ… **In plain English:**

> CNI is the invisible bridge between Kubernetesâ€™ brain (Kubelet + API) and Linuxâ€™s networking muscles.
> Without it, Pods could never talk across nodes, obey policies, or have predictable IPs.
> It turned Kubernetes networking from chaos into a modular, pluggable, and policy-driven system.
