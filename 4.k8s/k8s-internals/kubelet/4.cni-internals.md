# 🧩 Kubernetes CNI Internals — The Real Network Magic Behind Pods

## 🤔 **Why Do We Even Need CNI?**

Let’s start simple.
When Kubernetes wants to **create a Pod**, the **kubelet** (on each node) must ensure:

- ✅ The Pod has a network interface (`eth0`).
- ✅ The Pod gets a valid IP address.
- ✅ The Pod can reach other Pods (same or different nodes).
- ✅ The Pod’s traffic can reach the outside world (and vice versa).

Now the question:

> “Can kubelet do all this on its own?”

**No.** Kubelet **does not configure networking directly** — it just **delegates** to a **Container Network Interface (CNI) plugin** to handle this.

## 📖 **The Role of CNI in Kubernetes Networking**

CNI = **Container Network Interface**, a **Linux networking specification** maintained by the Cloud Native Computing Foundation (CNCF).

CNI defines **how container runtimes (like containerd or CRI-O)** talk to **network plugins (like Flannel, Calico, Cilium)** to create, connect, and delete container network interfaces.

---

### 🧩 CNI Responsibilities

| Task                         | Description                                                 |
| ---------------------------- | ----------------------------------------------------------- |
| 🏗️ Create network interfaces | Connect Pod’s namespace to node’s network (via `veth` pair) |
| 🌐 Assign IP addresses       | Each Pod gets a unique IP inside the cluster CIDR           |
| 🧭 Configure routes          | Ensure packets reach other Pods across nodes                |
| 🛠️ Apply network policies    | Control traffic rules between Pods                          |
| 🔄 Clean up                  | Delete interfaces and release IPs when Pods die             |

---

## ⚙️ **CNI Architecture**: How It Fits in the Stack

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
graph TD
  subgraph Worker Node
    Kubelet -->|calls via CRI| Containerd
    Containerd -->|calls| CNI_Plugin
    CNI_Plugin -->|configures| Kernel[Linux Kernel Networking]
  end

  Kernel --> Bridge[Linux Bridge / Overlay / Routing]
  Bridge --> Pod[Pod Network Namespace]
```

</div>

---

🔹 **Kubelet** asks **container runtime** (e.g., `containerd`) to create a Pod sandbox.  
🔹 **containerd** triggers **CNI plugin scripts** (`/opt/cni/bin/`).

🔹 The **CNI plugin** does the actual work:

1. Creates `veth` pair
2. Attaches one end to Pod’s namespace
3. Connects other end to node’s bridge or overlay
4. Assigns an IP
5. Updates routes and ARP tables

---

## 🎭 **CNI Plugin Types**

CNI is a **standard**, not a product.  
Different implementations provide different network behaviors.

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Plugin      | Type         | Description                                                |
| ----------- | ------------ | ---------------------------------------------------------- |
| **bridge**  | Basic        | Connects Pods on same node via a Linux bridge (`cni0`)     |
| **Flannel** | Overlay      | Creates VXLAN/UDP tunnels for cross-node Pod communication |
| **Calico**  | Routed / BGP | Uses BGP routing, no encapsulation                         |
| **Cilium**  | eBPF         | Uses eBPF programs for high-performance dataplane          |
| **Weave**   | Overlay      | Uses fast UDP tunnels between nodes                        |

</div>

---

## 📦 **CNI Lifecycle** — When a Pod is Created

When you run `kubectl run nginx --image=nginx`, here’s the _exact event sequence_ 👇

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
sequenceDiagram
  participant K as Kubelet
  participant R as Containerd (CRI)
  participant C as CNI Plugin
  participant L as Linux Kernel

  K->>R: Create Pod Sandbox (via CRI)
  R->>C: ADD command (invoke CNI plugin)
  C->>L: Create veth pair + assign IP
  C->>L: Attach to bridge / overlay
  C->>R: Return IP + routes
  R->>K: Pod network ready
  K->>Pod: Start containers
```

</div>

---

## 📂 Inside `/etc/cni/net.d/` — The Network Config

Each node has config files that define how the CNI should behave.

Example: `10-flannel.conf`

```json
{
  "cniVersion": "0.3.1",
  "name": "podnet",
  "type": "flannel",
  "delegate": {
    "type": "bridge",
    "bridge": "cni0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
      "type": "host-local",
      "subnet": "10.244.1.0/24"
    }
  }
}
```

So when containerd calls CNI:

```ini
/opt/cni/bin/flannel ADD <network-config>
```

The CNI plugin reads this file and does its job.

---

## 🧑🏻‍💻 **CNI Plugin Commands** (The Four Core Operations)

| Command     | Triggered When | Purpose                                        |
| ----------- | -------------- | ---------------------------------------------- |
| **ADD**     | Pod created    | Create veth pair, assign IP, connect to bridge |
| **DEL**     | Pod deleted    | Remove interfaces and release IP               |
| **CHECK**   | Health check   | Ensure networking still valid                  |
| **VERSION** | CLI query      | Report CNI version supported                   |

---

## ✍🏻 **Example** — CNI in Action (bridge + veth)

Imagine:

- Pod IP: `10.244.1.5`
- Node bridge: `cni0 (10.244.1.1)`

```bash
# Create Pod namespace which is like a container (separate process)
ip netns add pod1

# Create veth pair (veth0, veth1)
ip link add veth0 type veth peer name veth1

# Attach veth0 to Pod namespace
ip link set veth0 netns pod1

# veth0 -> add an IP address to it (10.244.1.5/24) and set it up
ip addr add 10.244.1.5/24 dev veth0
ip link set veth0 up

# Attach veth1 to bridge (cni0) and set it up
brctl addif cni0 veth1
ip link set veth1 up
```

CNI plugins basically automate this for every Pod!

---

## ⁉️ **Why Kubelet and Kube-proxy Alone Are Not Enough**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Component                          | Purpose                                      | Does It Configure Linux Networking?      |
| ---------------------------------- | -------------------------------------------- | ---------------------------------------- |
| **Kubelet**                        | Pod lifecycle management                     | ❌ No (delegates to runtime)             |
| **Container Runtime (containerd)** | Run containers                               | ⚠️ Partial (calls CNI)                   |
| **kube-proxy**                     | Service → Pod load balancing (iptables/IPVS) | ❌ Only programs kernel rules            |
| **CNI Plugin**                     | Setup Pod networks and routes                | ✅ Yes (creates interfaces, IPs, routes) |

</div>

So:

- Kubelet = “Hey runtime, make me a Pod.”
- containerd = “Okay, I’ll call CNI to wire it.”
- CNI = “I’ll plug the cables and give you an IP.”
- kube-proxy = “I’ll handle Service routing to that IP.”

---

## 🌼 CNI Data Flow Summary

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
flowchart LR
  subgraph Node
  A[Kubelet] --> B["Container Runtime (containerd)"]
  B --> C[CNI Plugin]
  C --> D[Linux Kernel]
  D --> E[Pod Network Namespace]
  end

  D -.->|Kernel Routes| F[Other Nodes / CNI Overlay]
```

</div>

---

Every network packet a Pod sends passes through:

```ini
Pod eth0 → veth → bridge → overlay → target node → veth → target Pod
```

---

## 🚀 11. Summary Table — Kubernetes Networking Layers

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Layer | Component    | What It Does                           |
| ----- | ------------ | -------------------------------------- |
| L7    | kube-proxy   | ClusterIP → Pod IP load balancing      |
| L4    | kubelet      | Delegates Pod networking setup         |
| L3    | CNI plugin   | Configures Pod IPs, veth pairs, routes |
| L2    | Linux kernel | Handles packet forwarding              |
| L1    | Physical NIC | Sends packets to the wire              |

</div>

---

## 🧩 12. Example: Full CNI Flow During Pod Creation

1. **Scheduler** assigns Pod to Node.
2. **Kubelet** tells **containerd** to start Pod sandbox.
3. **containerd** executes `/opt/cni/bin/<plugin> ADD`.
4. **CNI plugin**:

   - Creates Pod network namespace.
   - Creates veth pair.
   - Assigns IP.
   - Connects to node bridge or overlay.

5. **containerd** runs container with configured namespace.
6. **kube-proxy** ensures Service → Pod IP mapping exists.

✅ The Pod now has full network connectivity!

---

## 🧠 13. Key Takeaways

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Concept                           | Meaning                                        |
| --------------------------------- | ---------------------------------------------- |
| **CNI = Network Plumber**         | It wires up Pods and gives them IPs            |
| **Kubelet = Manager**             | It tells container runtime when to call CNI    |
| **Container Runtime = Middleman** | It executes CNI binaries with configs          |
| **kube-proxy = Traffic Director** | It ensures traffic reaches correct Pods        |
| **Kernel = Engine**               | It forwards packets through bridges and routes |

</div>
