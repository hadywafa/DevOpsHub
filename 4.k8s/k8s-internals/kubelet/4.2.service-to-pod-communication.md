# ‚ö° Service-to-Pod Communication

_(ClusterIP / NodePort / LoadBalancer ‚Äî Deep Internals with kube-proxy, NAT, and Linux Flow)!_

This one completes the picture after Pod-to-Pod and CNI.
Ready for your favorite format ‚Äî structured, visual, detailed, and human.

---

## üéØ Goal

Understand **how a Service object** (ClusterIP, NodePort, or LoadBalancer) actually **routes traffic to backend Pods**, what **kube-proxy** does under the hood, and how the **Linux kernel and iptables** or **eBPF** enforce this redirection.

---

## üß© The Core Concept

> A Kubernetes **Service** is not a real ‚Äúthing‚Äù ‚Äî it‚Äôs a **virtual abstraction** implemented entirely via **iptables/eBPF rules** on each node, created and managed by **kube-proxy**.

So when you access `10.96.0.25:80` (ClusterIP), there‚Äôs no container listening on that IP ‚Äî it‚Äôs **virtual**, handled by kernel-level NAT.

---

## üß± The 3 Major Service Types

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Type             | Access Scope                    | Example                              |
| ---------------- | ------------------------------- | ------------------------------------ |
| **ClusterIP**    | Inside cluster only             | `10.96.0.25`                         |
| **NodePort**     | External (via node‚Äôs IP + port) | `192.168.1.5:30080`                  |
| **LoadBalancer** | Externally via cloud LB         | `lb.example.com:80 ‚Üí NodePort ‚Üí Pod` |

</div>

---

## üß† kube-proxy‚Äôs Role in All This

kube-proxy runs on **every node** and **watches the API Server** for Service and Endpoint changes.

It builds translation tables so that:

```ini
Service IP ‚Üí Endpoint Pods
```

### Example

Let‚Äôs say you have this Service:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 8080
```

And you have these Pods:

| Pod   | Node  | Pod IP      | Port |
| ----- | ----- | ----------- | ---- |
| web-1 | node1 | 10.244.1.10 | 8080 |
| web-2 | node2 | 10.244.2.15 | 8080 |

---

## üß© Step 1 ‚Äî kube-proxy Watches API Server

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
sequenceDiagram
  participant API
  participant KP as kube-proxy
  participant ET as etcd

  KP->>API: WATCH /api/v1/services?watch=true
  KP->>API: WATCH /api/v1/endpoints?watch=true
  API-->>KP: Stream of Service + Endpoint events
  KP->>KP: Update iptables/eBPF rules locally
```

</div>

‚úÖ Each kube-proxy maintains a local copy of all service/endpoint mappings.

---

## üß© Step 2 ‚Äî iptables Rule Creation (ClusterIP)

Let‚Äôs inspect real rules on a node:

```bash
iptables-save | grep KUBE-
```

Example snippet:

```bash
-A KUBE-SERVICES -d 10.96.0.25/32 -p tcp --dport 80 -j KUBE-SVC-ABCD123
-A KUBE-SVC-ABCD123 -m statistic --mode random --probability 0.5 -j KUBE-SEP-1
-A KUBE-SVC-ABCD123 -j KUBE-SEP-2
-A KUBE-SEP-1 -s 10.244.1.10/32 -j DNAT --to-destination 10.244.1.10:8080
-A KUBE-SEP-2 -s 10.244.2.15/32 -j DNAT --to-destination 10.244.2.15:8080
```

So when a packet arrives for 10.96.0.25:80 ‚Üí it gets **DNATed** to one of the backend Pods.

---

## ‚öôÔ∏è Step 3 ‚Äî Packet Flow (ClusterIP ‚Üí Pod)

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
sequenceDiagram
  participant A as Pod (client)
  participant N as Node kernel
  participant K as kube-proxy (iptables)
  participant B as Pod (backend)
  participant CNI as CNI Plugin

  A->>N: TCP SYN to 10.96.0.25:80
  N->>K: Match iptables rule (KUBE-SERVICES)
  K->>N: DNAT ‚Üí 10.244.2.15:8080
  N->>CNI: Route packet to Pod-B (via overlay or local veth)
  CNI->>B: Deliver packet to container
```

</div>

‚úÖ **ClusterIP ‚Üí Pod** redirection done purely by Linux iptables.

---

## üåê Step 4 ‚Äî NodePort Mode

Now, if you change the Service type:

```yaml
type: NodePort
ports:
  - port: 80
    targetPort: 8080
    nodePort: 30080
```

Then kube-proxy adds **more iptables rules**:

```bash
-A KUBE-NODEPORTS -p tcp --dport 30080 -j KUBE-SVC-ABCD123
```

So if someone outside hits:

```ini
http://<NodeIP>:30080
```

‚Üí it‚Äôs redirected to a backend Pod.

Even if the Pod is on another node, kube-proxy does **cross-node NAT** to route it.

‚úÖ This allows external access without a cloud load balancer.

---

## ‚òÅÔ∏è Step 5 ‚Äî LoadBalancer Type (Cloud Integration)

For **type: LoadBalancer**, kube-proxy itself does nothing new.
Instead, the **cloud controller manager (CCM)** provisions a **cloud-native LB** (like AWS ELB, Azure LB, GCP LB) that forwards traffic to each node‚Äôs **NodePort**.

Flow:

```ini
Client ‚Üí Cloud LoadBalancer ‚Üí NodePort (on any node) ‚Üí kube-proxy DNAT ‚Üí Pod
```

‚úÖ All kube-proxies handle packets identically; no need to know which node hosts which Pod.

---

## üß† Step 6 ‚Äî eBPF Mode (Modern Alternative)

Some CNIs (like Cilium) replace iptables completely using **eBPF**.

üß© Difference:

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Feature       | iptables      | eBPF                        |
| ------------- | ------------- | --------------------------- |
| Rule format   | Static tables | Compiled in-kernel programs |
| Speed         | Moderate      | Very fast                   |
| Observability | Hard          | Excellent                   |
| Update model  | Full reload   | Live patch                  |

</div>

---

> ‚úÖ eBPF mode bypasses NAT chains and handles load-balancing directly inside the kernel.

---

## üß© Real Flow Example

Assume:

| Object    | IP / Port        |
| --------- | ---------------- |
| ClusterIP | 10.96.0.25:80    |
| NodePort  | 30080            |
| Pod-1     | 10.244.1.10:8080 |
| Pod-2     | 10.244.2.15:8080 |

Then:

| Access Path              | Flow                               |
| ------------------------ | ---------------------------------- |
| `curl 10.96.0.25:80`     | ClusterIP ‚Üí iptables DNAT ‚Üí Pod IP |
| `curl Node1IP:30080`     | NodePort ‚Üí iptables DNAT ‚Üí Pod IP  |
| `curl LoadBalancerIP:80` | Cloud LB ‚Üí NodePort ‚Üí Pod IP       |

---

## üîç Inspect with Commands

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Check           | Command                                                        |     |
| --------------- | -------------------------------------------------------------- | --- |
| View rules      | `iptables-save | grep KUBE-`                                   |
| Show proxy mode | `kubectl -n kube-system get ds kube-proxy -o yaml | grep mode` |
| See endpoints   | `kubectl get endpoints web -o yaml`                            |     |
| Trace flow      | `conntrack -L | grep 10.96.0.25`                               |

</div>

---

## üíé Visual Summary

<div align="center" style="background-color: #255560ff; border-radius: 10px; border: 2px solid">

```mermaid
graph TD
  A[External Client] -->|LoadBalancer| B[NodePort 30080]
  B -->|kube-proxy iptables/eBPF| C[ClusterIP 10.96.0.25]
  C -->|DNAT| D["Pod (10.244.x.x)"]
  D -->|Response| B
```

</div>

---

## üß† TL;DR Recap

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Concept          | Description                                         |
| ---------------- | --------------------------------------------------- |
| **Service**      | Virtual IP + ruleset (no real pod)                  |
| **kube-proxy**   | Translates Service IP ‚Üí Pod IP using iptables/eBPF  |
| **ClusterIP**    | Internal-only access                                |
| **NodePort**     | Expose via node‚Äôs IP + port                         |
| **LoadBalancer** | Cloud-managed LB ‚Üí NodePort                         |
| **eBPF Mode**    | High-performance in-kernel replacement for iptables |

</div>

---

‚úÖ **In plain English:**

> Services in Kubernetes aren‚Äôt servers ‚Äî they‚Äôre clever _illusions_.
> kube-proxy, iptables, and the kernel cooperate to make one Pod reachable via a single virtual IP.
> Whether you connect from inside or outside, the system quietly rewrites and routes packets to the right backend.

---

Would you like me to continue next with **Ingress Internals** ‚Äî i.e., how HTTP/HTTPS traffic enters the cluster and flows through ingress controllers (Nginx / Traefik / Envoy) down to Services and Pods, with real diagrams?
