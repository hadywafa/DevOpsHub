# âš–ï¸ **Kubernetes Pods 103** â€” **Scheduling**, **Node Placement** & **Pod Disruption**

## ğŸ¯ **Goal**

Understand **how Kubernetes decides where to run pods**, and how to control that behavior using:

- Node selectors, affinity, and anti-affinity
- Taints and tolerations
- PodDisruptionBudgets (PDBs)
- Troubleshooting scheduling issues

---

## ğŸ“– **Scheduling Overview**

The **kube-scheduler** assigns pods to nodes based on:  
1ï¸âƒ£ Node resource availability  
2ï¸âƒ£ Scheduling rules (affinity, taints, etc.)  
3ï¸âƒ£ Pod constraints (selectors, nodeName, tolerations)

### Scheduler Flow ğŸ§ 

```mermaid
flowchart LR
A[Pod Created] --> B[Scheduler Watches Unschedulable Pods]
B --> C[Filters Nodes by Constraints]
C --> D[Scores Nodes]
D --> E[Selects Best Node]
E --> F[Pod Bound to Node]
```

---

## ğŸ‘¯â€â™‚ï¸ **Static Placement Options** (based on nodes labels)

### A. `nodeName` (Hard Binding)

Simplest method: assign pod directly to a specific node.

```yaml
spec:
  nodeName: worker-1
```

âš ï¸ No scheduler decision â€” it goes **directly** to that node.
If that node is down â†’ pod stays **Pending** `forever`.

---

### B. `nodeSelector` (Label Match)

Labels on nodes:

```bash
kubectl label node worker-1 disktype=ssd
```

Pod YAML:

```yaml
spec:
  nodeSelector:
    disktype: ssd
```

âœ… Pod will only schedule to nodes labeled `disktype=ssd`.  
ğŸ§  One-to-one match â€” no expressions or conditions.

List node labels:

```bash
kubectl get nodes --show-labels
```

---

## âš™ï¸ **Node Affinity** (Advanced Version of nodeSelector)

Node affinity uses **expressions**, **operators**, and **multiple rules**.
Defined under `.spec.affinity.nodeAffinity`.

### Example: Preferred SSD Nodes

```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: In
                values:
                  - ssd
```

ğŸ§  `requiredDuringSchedulingIgnoredDuringExecution`
â†’ Hard requirement during scheduling (pod wonâ€™t run otherwise).

---

### Optional vs Required Affinity

| Type                                                | Behavior                                           |
| --------------------------------------------------- | -------------------------------------------------- |
| **requiredDuringSchedulingIgnoredDuringExecution**  | Hard rule â€” must match or pod stays Pending.       |
| **preferredDuringSchedulingIgnoredDuringExecution** | Soft rule â€” scheduler prefers it but not required. |

Example:

```yaml
preferredDuringSchedulingIgnoredDuringExecution:
  - weight: 1
    preference:
      matchExpressions:
        - key: zone
          operator: In
          values:
            - us-east-1a
```

âœ… â€œPrefer zone us-east-1a, but donâ€™t block if unavailable.â€

---

### Common Operators

| Operator       | Meaning                         |
| -------------- | ------------------------------- |
| `In`           | keyâ€™s value must be in list     |
| `NotIn`        | keyâ€™s value must not be in list |
| `Exists`       | key exists (value ignored)      |
| `DoesNotExist` | key missing                     |
| `Gt`           | greater than (numeric)          |
| `Lt`           | less than (numeric)             |

---

## ğŸ§© **Pod Affinity** and **Anti-Affinity** (based on other pods labels)

**Pod Affinity means â€œstick togetherâ€; Anti-Affinity means â€œstay apart.â€ These rules help Kubernetes decide where to place your pods based on other pods already running.**

> âœ… Used to **co-locate or separate** pods based on labels of other pods.

---

### ğŸ¤ Pod Affinity (Stick Together)

- **Goal**: Place pods _close to each other_ â€” on the same node or zone.
- **Why**: Useful when pods need fast communication, like microservices that talk a lot.
- **Example**: You want your frontend pod to run on the same node as your backend pod.

  ```yaml
  affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: backend
          topologyKey: "kubernetes.io/hostname"
  ```

  > ğŸ’¡ This says: â€œSchedule this pod on the same node as any pod with label `app=backend`.â€

- **Example**: You want your frontend pod to run on the same node as your backend pod.

  ```yaml
  affinity:
  podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
              - key: app
              operator: In
              values:
                  - backend
          topologyKey: kubernetes.io/hostname
  ```

  > ğŸ’¡ Schedules this pod on the **same node** as pods labeled `app=backend`.

---

### ğŸš« Pod Anti-Affinity (Stay Apart)

- **Goal**: Spread pods out â€” avoid placing them together.
- **Why**: Improves fault tolerance. If one node fails, others still run your app.
- **Example**: You want replicas of your web app to run on different nodes.

  ```yaml
  affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: web
          topologyKey: "kubernetes.io/hostname"
  ```

  > ğŸ’¡ This says: â€œDonâ€™t schedule this pod on a node that already has a pod with label `app=web`.â€

- **Example**: You want replicas of your web app to run on different nodes.

  ```yaml
  affinity:
  podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
              - key: app
              operator: In
              values:
                  - frontend
          topologyKey: kubernetes.io/hostname
  ```

  > ğŸ’¡ Ensures frontend pods are **on different nodes** â€” great for HA.

---

### ğŸ¯ Common topologyKeys

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Key                             | Scope      |
| ------------------------------- | ---------- |
| `kubernetes.io/hostname`        | per-node   |
| `topology.kubernetes.io/zone`   | per-zone   |
| `topology.kubernetes.io/region` | per-region |

</div>

---

## ğŸ **Taints** and **Tolerations**

Taints mark **nodes** as â€œrestrictedâ€; tolerations allow **pods** to run there.

---

### ğŸš« **Taints**: â€œKeep Out Unless Allowedâ€

- **Applied to nodes** to _repel_ pods.
- A tainted node will **reject all pods** unless they have a matching toleration.
- Example: You want only special workloads (like GPU jobs) on a node.

  ```bash
  kubectl taint nodes node1 gpu=true:NoSchedule
  ```

  > ğŸ’¡ This means: â€œDonâ€™t schedule pods here unless they tolerate `gpu=true`.â€

---

### âœ… **Tolerations**: â€œIâ€™m Allowed Inâ€

- **Applied to pods** to _accept_ taints.
- A pod with a matching toleration can be scheduled on a tainted node.
- Example:

  ```yaml
  spec:
  tolerations:
      - key: "gpu"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  ```

  > ğŸ’¡ This pod says: â€œIâ€™m okay with the `gpu=true` taint â€” let me in.â€

---

### ğŸ¤·ğŸ»â€â™‚ï¸ **Taint Effects**:

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Effect               | Description                                 |
| -------------------- | ------------------------------------------- |
| **NoSchedule**       | Pod wonâ€™t be scheduled unless it tolerates. |
| **PreferNoSchedule** | Try to avoid scheduling, but not strict.    |
| **NoExecute**        | Evicts running pods that donâ€™t tolerate it. |

</div>

---

### ğŸ§ª **Check Taints & Tolerations**

List node taints:

```bash
kubectl describe node worker-1 | grep -i taints
```

List pods tolerating:

```bash
kubectl get pods -o custom-columns=NAME:.metadata.name,TOLERATIONS:.spec.tolerations
```

---

## âœğŸ» **Node Selectors + Affinity + Taints Combined Example**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: complex-scheduler
spec:
  nodeSelector:
    region: eu
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - matchExpressions:
            - key: disktype
              operator: In
              values: ["ssd"]
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "batch"
      effect: "NoSchedule"
  containers:
    - name: app
      image: nginx
```

ğŸ§  â€œOnly run in EU region, on SSD nodes, with batch taint tolerance.â€

---

## ğŸ”´ **Troubleshooting Scheduling Issues**

### ğŸ”¹ Check pod reason

```bash
kubectl describe pod <pod> | grep -A5 Events
```

Typical outputs:

- `0/3 nodes are available: 3 node(s) didn't match node selector.`
- `0/3 nodes had taints that the pod didn't tolerate.`
- `0/3 nodes didn't match pod affinity/anti-affinity rules.`

---

### ğŸ”¹ Verify Scheduler Logs

On control plane:

```bash
kubectl -n kube-system logs kube-scheduler-<node>
```

---

### ğŸ”¹ Verify Node Conditions

```bash
kubectl describe node <nodename> | grep -i Conditions
```

Common issues:

- `OutOfDisk`
- `MemoryPressure`
- `DiskPressure`
- `NetworkUnavailable`

---

## âš™ï¸ **Pod Disruption Budget** (PDB)

PDB controls **how many pods can be down** during maintenance or node drains.

Example: ensure at least one pod always runs.

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webapp-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: webapp
```

Alternative:

```yaml
maxUnavailable: 1
```

Apply:

```bash
kubectl apply -f pdb.yaml
```

Check status:

```bash
kubectl get pdb
```

---

### Why Use PDB?

- Protect apps from being **fully drained** during node upgrades.
- Ensure **HA deployments** keep enough replicas online.
- Works with **voluntary disruptions** (like `kubectl drain`), not crashes.

---

## ğŸ§° **Node Maintenance** (Drain & Cordon)

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Command                   | Purpose                       |
| ------------------------- | ----------------------------- |
| `kubectl drain <node>`    | Safely evict pods from a node |
| `kubectl cordon <node>`   | Mark node unschedulable       |
| `kubectl uncordon <node>` | Allow new pods again          |

</div>

> ğŸš¨ PDBs restrict how many pods drain at once.

---

### ğŸ“ Example Workflow

```bash
kubectl get nodes
kubectl cordon worker-2
kubectl drain worker-2 --ignore-daemonsets --delete-emptydir-data
```

After maintenance:

```bash
kubectl uncordon worker-2
```

---

## âœ… **Best Practices**

| Area                   | Best Practice                                                 |
| ---------------------- | ------------------------------------------------------------- |
| **nodeSelector**       | Use for simple 1:1 mapping.                                   |
| **Node Affinity**      | Use for multi-label, flexible matching.                       |
| **Pod Affinity**       | Group workloads needing local communication.                  |
| **Pod Anti-Affinity**  | Improve HA and spread workloads.                              |
| **Taints/Tolerations** | Isolate nodes for special workloads.                          |
| **PDBs**               | Protect against full pod eviction.                            |
| **Drain/Cordon**       | Always use these for maintenance, never delete pods manually. |

---

## ğŸ§ª Admin Hands-On Checklist

âœ… Label a node:

```bash
kubectl label node worker-1 disktype=ssd
```

âœ… Create a pod that uses nodeSelector:

```bash
kubectl run testpod --image=nginx --dry-run=client -o yaml > testpod.yaml
# edit YAML and add nodeSelector
kubectl apply -f testpod.yaml
```

âœ… Taint and tolerate:

```bash
kubectl taint nodes worker-1 dedicated=frontend:NoSchedule
kubectl get nodes -o wide
```

âœ… Apply PDB:

```bash
kubectl apply -f pdb.yaml
```

âœ… Drain node and verify:

```bash
kubectl drain worker-1 --ignore-daemonsets
kubectl get pods -A -o wide
```
