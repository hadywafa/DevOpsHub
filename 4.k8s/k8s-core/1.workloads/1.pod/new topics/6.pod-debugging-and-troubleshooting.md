Excellent ğŸ”¥ â€” youâ€™re about to complete the **Pod Administration** module with the final doc:

# ğŸ§¯ Kubernetes Pods 106 â€” Debugging, Troubleshooting & Recovery Techniques

> ğŸ¯ **Goal:** Learn how to investigate, debug, and recover broken Pods like a real cluster admin â€” using logs, events, ephemeral containers, and practical commands.

---

<div align="center" style="background-color:#fff; border-radius: 2px; border: 2px solid">
  <img src="https://kubernetes.io/images/docs/pod-lifecycle-events.svg" alt="Pod Debugging" style="width: 70%; border-radius: 10px" />
</div>

---

## ğŸ§  1. The Pod Lifecycle (Quick Recap)

Pods go through these **phases**:

```
Pending â†’ Running â†’ Succeeded / Failed / Unknown
```

Each container in a Pod has its own **state**:

* **Waiting**: Container is preparing (pulling image, waiting for resources)
* **Running**: Actively running
* **Terminated**: Completed or failed
* **CrashLoopBackOff**: Container keeps restarting

---

## ğŸ§© 2. Step-by-Step Troubleshooting Workflow

When a Pod misbehaves, always follow this sequence:

```mermaid
flowchart TD
A[Check Pod Status] --> B[Inspect Events]
B --> C[Check Logs]
C --> D[Inspect Container Details]
D --> E[Exec into Container / Debug]
E --> F[Inspect Node / Scheduler Issues]
```

Letâ€™s break these steps down ğŸ‘‡

---

## ğŸ§± 3. Step 1 â€” Check Pod Status

```bash
kubectl get pods -A -o wide
```

Focus on:

* **STATUS** â†’ Pending, CrashLoopBackOff, Error
* **READY** â†’ e.g., `1/2` means 1 container is running, 1 failed
* **RESTARTS** â†’ High restart count = unstable container

---

### Example Output

```
NAME           READY   STATUS             RESTARTS   AGE
nginx-demo     0/1     CrashLoopBackOff   4          2m
```

Next â†’ describe it ğŸ‘‡

---

## ğŸ§± 4. Step 2 â€” Describe the Pod (See Events)

```bash
kubectl describe pod nginx-demo
```

Scroll to bottom for **Events**:

```
Events:
  Type     Reason     Message
  ----     ------     -------
  Normal   Scheduled  Successfully assigned default/nginx-demo to node-1
  Normal   Pulling    Pulling image "nginx"
  Warning  Failed     Error: CrashLoopBackOff
```

### Common Event Reasons:

| Reason                 | Description                              |
| ---------------------- | ---------------------------------------- |
| `ImagePullBackOff`     | Image not found or authentication failed |
| `CrashLoopBackOff`     | App crashed repeatedly                   |
| `ErrImagePull`         | Bad image name or registry               |
| `OOMKilled`            | Memory limit exceeded                    |
| `CreateContainerError` | Bad volume or env setup                  |

---

## ğŸ§± 5. Step 3 â€” Inspect Logs

```bash
kubectl logs nginx-demo
```

If multi-container Pod:

```bash
kubectl logs nginx-demo -c <container-name>
```

View previous crash logs:

```bash
kubectl logs nginx-demo -p
```

---

### Example:

```
Error: Port 8080 already in use
```

â†’ App conflict or duplicate process.

---

## ğŸ§° 6. Step 4 â€” Get Inside the Pod (Exec Shell)

```bash
kubectl exec -it nginx-demo -- /bin/bash
# or for alpine/busybox
kubectl exec -it nginx-demo -- sh
```

âœ… Check running processes, environment, and configs:

```bash
ps aux
env
ls /etc/config
```

---

### Quick Fix Example:

```
kubectl exec -it nginx-demo -- sed -i 's/8080/80/g' /etc/nginx/conf.d/default.conf
```

---

## ğŸ§± 7. Step 5 â€” Pod Not Starting (Image or Config Issues)

### ğŸ” Check Image Pull

```bash
kubectl describe pod <pod> | grep -A3 "Failed"
```

Errors like:

```
Failed to pull image "myapp:v2": image not found
```

âœ… Fix by updating image:

```bash
kubectl set image pod/myapp myapp=repo/myapp:latest
```

---

### ğŸ” Check Environment Variables

```bash
kubectl exec -it <pod> -- printenv
```

Or YAML:

```bash
kubectl get pod <pod> -o yaml | grep -A5 env:
```

---

### ğŸ” Check Volume Mounts

```bash
kubectl describe pod <pod> | grep -A3 Mounts
```

If mount path missing â†’ verify PVC:

```bash
kubectl get pvc
kubectl describe pvc <name>
```

---

## ğŸ§  8. Step 6 â€” Troubleshooting Stuck or Pending Pods

```bash
kubectl describe pod <pod> | grep -A5 Events
```

Typical errors:

* `0/3 nodes available: 3 Insufficient memory`
* `node(s) didn't match node selector`
* `node(s) had taints that the pod didn't tolerate`

âœ… Fix by adjusting:

* resource requests
* node selectors
* tolerations

---

## ğŸ§° 9. Debugging CrashLoopBackOff

CrashLoopBackOff = Container starts, crashes, restarts repeatedly.

### Analyze:

```bash
kubectl logs <pod> -p
```

If app bug â†’ fix container or startup script.
If config issue â†’ verify readiness/liveness probes.

---

### Check restart count

```bash
kubectl get pods
```

If increasing rapidly â†’ CrashLoopBackOff confirmed.

---

### Restart behavior:

Pods controlled by Deployments auto-recreate â€” deleting wonâ€™t help.
Better approach:

```bash
kubectl rollout restart deployment <name>
```

---

## ğŸ§© 10. Step 7 â€” Use `kubectl debug` (Ephemeral Containers)

Ephemeral containers allow debugging **without modifying the Pod**.
Perfect for troubleshooting production Pods.

### Example:

```bash
kubectl debug -it <pod> --image=busybox --target=<container-name>
```

âœ… This launches a **temporary shell container** inside the Pod network namespace.

Inside:

```bash
ps aux
netstat -tulpn
```

Exit â†’ ephemeral container auto-deletes.

---

## ğŸ§± 11. Step 8 â€” Network Troubleshooting

If Pod canâ€™t reach another service:

```bash
kubectl exec -it <pod> -- curl <svc-name>:<port>
```

If fails:
1ï¸âƒ£ Check Service and Endpoints:

```bash
kubectl get svc <svc-name>
kubectl get endpoints <svc-name>
```

2ï¸âƒ£ If Endpoints = `<none>` â†’ backing Pods not Ready.
Check selectors:

```bash
kubectl get pods -l <label>
```

3ï¸âƒ£ Test DNS:

```bash
kubectl exec -it <pod> -- nslookup <svc-name>
```

---

## ğŸ§© 12. Step 9 â€” Pod Stuck in Terminating State

Common causes:

* Finalizers not cleared
* Mounted PVC busy
* Network connection hanging

Force delete:

```bash
kubectl delete pod <pod> --grace-period=0 --force
```

---

## ğŸ§  13. Step 10 â€” Pod Evicted by Node Pressure

Check status:

```bash
kubectl get pods --field-selector=status.phase=Failed
kubectl describe pod <name>
```

Look for:

```
The node was low on resource: memory.
```

âœ… Fix: increase node memory or use higher QoS class.
ğŸ§© Evicted pods are **not restarted** â€” controller (e.g. Deployment) must recreate.

---

## ğŸ§± 14. Step 11 â€” Init Containers Debugging

Init containers run **before** app containers to prepare environment.
If one fails â†’ main container never starts.

View init container logs:

```bash
kubectl logs <pod> -c <init-container-name>
```

Check status:

```bash
kubectl get pod <pod> -o jsonpath='{.status.initContainerStatuses[*].state}'
```

---

## ğŸ§© 15. Step 12 â€” Liveness & Readiness Probe Failures

Liveness probes control **restart** behavior.
Readiness probes control **traffic acceptance**.

Check failures:

```bash
kubectl describe pod <pod> | grep -A5 "Liveness probe failed"
```

Example YAML:

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10
```

âœ… If probe misconfigured â†’ app keeps restarting unnecessarily.

---

## ğŸ§© 16. Step 13 â€” Resource Problems (OOMKilled, Throttling)

```bash
kubectl describe pod <pod> | grep -i "OOMKilled"
```

Memory limit exceeded â†’ container killed.

For CPU:

```bash
kubectl top pod
```

Throttling visible in metrics.

Fix:

* Adjust resource limits
* Use QoS class `Guaranteed` for critical pods

---

## ğŸ§± 17. Step 14 â€” Node-Level Problems

If all pods pending or failing:

```bash
kubectl get nodes
kubectl describe node <node>
```

Look for:

* DiskPressure
* MemoryPressure
* NetworkUnavailable

Fix by:

```bash
kubectl cordon <node>
kubectl drain <node> --ignore-daemonsets
```

Then investigate system logs.

---

## ğŸ§° 18. Advanced Admin Commands Summary

| Task                  | Command                                             |                 |
| --------------------- | --------------------------------------------------- | --------------- |
| Get logs              | `kubectl logs <pod>`                                |                 |
| Get previous logs     | `kubectl logs -p <pod>`                             |                 |
| Describe pod          | `kubectl describe pod <pod>`                        |                 |
| Exec into container   | `kubectl exec -it <pod> -- bash`                    |                 |
| Attach to running pod | `kubectl attach <pod>`                              |                 |
| Debug ephemeral       | `kubectl debug -it <pod> --image=busybox`           |                 |
| Delete forcefully     | `kubectl delete pod <pod> --force --grace-period=0` |                 |
| Check probes          | `kubectl get pod <pod> -o yaml                      | grep -A5 probe` |

---

## ğŸ§  19. Common Real-World Scenarios

| Symptom               | Root Cause                  | Fix                           |
| --------------------- | --------------------------- | ----------------------------- |
| `CrashLoopBackOff`    | App crash / probe failure   | Check logs, fix probe         |
| `ImagePullBackOff`    | Bad image name / auth issue | Fix image reference or secret |
| `OOMKilled`           | Memory limit exceeded       | Increase limits or optimize   |
| `PodPending`          | Scheduler constraints       | Fix selectors or taints       |
| `No route to host`    | CNI issue                   | Restart network plugin        |
| `Terminating forever` | Finalizer lock              | Force delete pod              |
| `Evicted`             | Node pressure               | Scale nodes or raise limits   |

---

## ğŸ§­ 20. Hands-On Admin Lab

1ï¸âƒ£ Simulate crash:

```bash
kubectl run crashpod --image=busybox --restart=Never -- sh -c "exit 1"
```

Check:

```bash
kubectl get pods
kubectl describe pod crashpod
```

2ï¸âƒ£ Debug DNS:

```bash
kubectl run dns-test --image=busybox:1.28 -it -- nslookup kubernetes.default
```

3ï¸âƒ£ Add ephemeral debug:

```bash
kubectl debug -it crashpod --image=busybox --target=crashpod
```

4ï¸âƒ£ Fix deployment crash:

```bash
kubectl rollout restart deployment <name>
kubectl rollout status deployment <name>
```

---

## ğŸ§© 21. Recovery Best Practices

| Area                          | Action                                        |
| ----------------------------- | --------------------------------------------- |
| **Always check Events first** | They tell the full story                      |
| **Avoid blind deletes**       | Use `rollout restart` instead                 |
| **Use probes carefully**      | Donâ€™t make them too strict                    |
| **Monitor restarts**          | `kubectl get pods` frequently                 |
| **Keep a debug image handy**  | e.g. `nicolaka/netshoot`, `busybox`, `alpine` |

---

## âœ… Summary

| Concept           | You Can Now...                    |
| ----------------- | --------------------------------- |
| Pod Lifecycle     | Interpret every status            |
| Logs & Events     | Identify root causes quickly      |
| Ephemeral Debug   | Enter any Pod for inspection      |
| Resource Failures | Detect OOM, CPU throttling        |
| Node Issues       | Drain, isolate, and recover nodes |

---

## ğŸ§­ Next Major Section

ğŸ‰ Youâ€™ve completed **Pod Administration (1â€“6)**
Next, we move to the **Workload Level** â€” starting with:

### ğŸš€ **Deployments 101 â€” Creating, Scaling & Managing Deployments**

Itâ€™ll cover:

* ReplicaSets management
* Rollouts, revisions, rollbacks
* Canary updates & blue/green patterns

Would you like me to start with **Deployments 101** next?
