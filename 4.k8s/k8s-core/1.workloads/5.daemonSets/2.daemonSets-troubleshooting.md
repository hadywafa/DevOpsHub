# 🧰 **DaemonSets 102 — Troubleshooting, Updates & Node Control**

> 🎯 **Goal:** Learn to diagnose, fix, and control DaemonSets efficiently — including node-level scheduling issues, rolling updates, taints/tolerations, log collector problems, and node constraints.

---

## ⚙️ **Common Real-World Issues**

DaemonSets are stable in design but can fail for _operational_ reasons.
Here are the top issues admins face:

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Problem                            | Typical Symptom                           |
| ---------------------------------- | ----------------------------------------- |
| Pods not created on all nodes      | Missing Pods or uneven distribution       |
| Pods stuck in `Pending`            | Node selector mismatch or taints          |
| CrashLoopBackOff                   | Bad config, permissions, or missing paths |
| Updates not rolling                | Old Pods not replaced, image not updating |
| Pod not deleted after node removal | Orphaned Pod object left behind           |

</div>

---

## 🧩 1. **Pods Missing on Certain Nodes**

### 🔍 Symptom

```bash
kubectl get pods -o wide -l app=node-logger
```

Output:

```ini
NAME                NODE
node-logger-abc12   worker-1
node-logger-def45   worker-2
# Missing on worker-3
```

### 📋 Root Causes

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Cause                     | Explanation                              |
| ------------------------- | ---------------------------------------- |
| NodeSelector mismatch     | Node lacks required labels               |
| Taint prevents scheduling | Node tainted without matching toleration |
| Node NotReady             | Node in maintenance or cordoned          |
| Resource pressure         | Node can’t fit more Pods                 |

</div>

---

### 🧠 Diagnosis Steps

```bash
kubectl describe daemonset node-logger
```

Look for:

```ini
Events:
  Warning  FailedScheduling  No nodes match node selector
```

Then check node labels:

```bash
kubectl get nodes --show-labels
```

And taints:

```bash
kubectl describe node worker-3 | grep Taint
```

---

### 🩹 Fix

- **Add required label to node:**

  ```bash
  kubectl label node worker-3 node-role.kubernetes.io/worker=""
  ```

- **Add toleration to DaemonSet:**

  ```yaml
  spec:
    template:
      spec:
        tolerations:
          - key: "dedicated"
            operator: "Equal"
            value: "worker"
            effect: "NoSchedule"
  ```

- **Uncordon node (if drained):**

  ```bash
  kubectl uncordon worker-3
  ```

✅ Reapply the DaemonSet and verify Pods appear on all nodes.

---

## ⚙️ 2. **Pods Stuck in `Pending`**

### 🔍 Symptom

```bash
kubectl get pods
```

```ini
node-logger-xyz Pending
```

### 🧠 Diagnosis

Check events:

```bash
kubectl describe pod node-logger-xyz
```

Look for:

```ini
0/4 nodes are available: insufficient CPU
```

Or:

```ini
No nodes match node selector
```

### 🩹 Fix

- Check cluster capacity:

  ```bash
  kubectl top nodes
  ```

- Increase node pool size or lower Pod resource requests.
- Confirm node affinity rules are not too restrictive.

---

## ⚙️ 3. **CrashLoopBackOff**

### 🔍 Symptom

Pod restarts repeatedly:

```bash
kubectl get pods
```

```ini
node-logger-abcd   0/1   CrashLoopBackOff
```

### 🧠 Diagnosis

Check logs:

```bash
kubectl logs node-logger-abcd
```

If it says:

```ini
permission denied /var/log/syslog
```

→ your `hostPath` or container permissions are wrong.

### 🩹 Fix

- Validate your volume mount path:

  ```yaml
  volumes:
    - name: logs
      hostPath:
        path: /var/log
  ```

- Add proper `securityContext` if needed:

  ```yaml
  securityContext:
    runAsUser: 0
  ```

- Fix file permissions on host node if it’s a shared directory.

---

## 🔁 4. **DaemonSet Not Updating**

### 🔍 Symptom

You updated the image version, but Pods don’t change:

```bash
kubectl apply -f daemonset.yaml
kubectl get pods -l app=node-logger -o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[0].image
```

Output:

```ini
node-logger-1   filebeat:8.10.0
node-logger-2   filebeat:8.10.0
```

Even though YAML says `filebeat:8.11.0`.

### 📋 Root Causes

- Update strategy not set to `RollingUpdate`
- Selector mismatch between old and new template
- Cached image or pull errors

---

### 🧠 Diagnosis

```bash
kubectl describe daemonset node-logger | grep Strategy
kubectl get events --sort-by=.metadata.creationTimestamp
```

Look for:

```ini
Failed to pull image
```

---

### 🩹 Fix

- Ensure update strategy:

  ```yaml
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  ```

- Restart rollout:

  ```bash
  kubectl rollout restart daemonset node-logger
  ```

- Check rollout progress:

  ```bash
  kubectl rollout status daemonset node-logger
  ```

✅ Pods are replaced one by one.

---

## 🧱 5. **Orphaned Pods After Node Removal**

### 🔍 Symptom

A node was deleted, but DaemonSet Pods remain in the API:

```ini
kubectl get pods -A
```

You see Pods on non-existing nodes.

### 🧠 Cause

Controller couldn’t clean up before node removal (e.g., forced deletion).

---

### 🩹 Fix

Manually remove them:

```bash
kubectl delete pod <pod-name> --force --grace-period=0
```

Or clean up all non-existent-node Pods:

```bash
for pod in $(kubectl get pods -A --field-selector=status.phase=Unknown -o name); do
  kubectl delete $pod --force --grace-period=0
done
```

---

## 🔒 6. **DaemonSet Scheduling Control** (Affinity & Anti-Affinity)

DaemonSets often need **fine-grained node targeting**.

### 🎯 Node Affinity Example

Run only on storage nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-type
                    operator: In
                    values:
                      - storage
```

### 🚫 Pod Anti-Affinity Example

Avoid running multiple agent Pods on same node type:

```yaml
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app
              operator: In
              values:
                - node-agent
        topologyKey: "kubernetes.io/hostname"
```

---

## 🧪 **Checking DaemonSet Rollout & Events**

```bash
kubectl rollout status daemonset node-logger
kubectl rollout history daemonset node-logger
kubectl get events --sort-by=.metadata.creationTimestamp
```

---

## 🔖 **Debugging Checklist**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Step        | Command                                   | Purpose                      |
| ----------- | ----------------------------------------- | ---------------------------- |
| Describe DS | `kubectl describe ds <name>`              | View strategy, nodes, events |
| List Pods   | `kubectl get pods -o wide -l app=<label>` | Check distribution           |
| Node labels | `kubectl get nodes --show-labels`         | Validate selector coverage   |
| Node taints | `kubectl describe node <node>`            | Detect scheduling blocks     |
| Logs        | `kubectl logs <pod>`                      | Detect runtime errors        |
| Events      | `kubectl get events`                      | Find rollout or pull errors  |

</div>

---

## 🧭 **Pro Tips from Real Clusters**

- 🔹 **Always label nodes** clearly:
  e.g., `role=worker`, `zone=us-east1-b`, `gpu=true`.
- 🔹 **Don’t overuse hostPath** — prefer `emptyDir` or `configMap` when possible.
- 🔹 Use `kubectl get ds -A -o wide` to instantly check Pod/node mapping.
- 🔹 Monitor rollout via:

  ```bash
  watch kubectl get pods -l app=node-logger
  ```

- 🔹 When using monitoring/logging agents, keep them in **dedicated namespaces** (`logging`, `monitoring`, etc.) for easier RBAC control.

---

## ✅ **Summary**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Problem          | Cause                        | Fix                                 |
| ---------------- | ---------------------------- | ----------------------------------- |
| Pods missing     | Node selector/taint          | Add labels/tolerations              |
| Pods pending     | Insufficient resources       | Resize node or adjust requests      |
| CrashLoopBackOff | Bad config/permissions       | Check hostPath + logs               |
| Not updating     | Wrong strategy or image pull | Set RollingUpdate & restart rollout |
| Orphaned Pods    | Node removed                 | Force delete stale Pods             |

</div>
