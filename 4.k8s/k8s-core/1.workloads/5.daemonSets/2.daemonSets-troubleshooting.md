# ğŸ§° **DaemonSets 102 â€” Troubleshooting, Updates & Node Control**

> ğŸ¯ **Goal:** Learn to diagnose, fix, and control DaemonSets efficiently â€” including node-level scheduling issues, rolling updates, taints/tolerations, log collector problems, and node constraints.

---

## âš™ï¸ **Common Real-World Issues**

DaemonSets are stable in design but can fail for _operational_ reasons.
Here are the top issues admins face:

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Problem                            | Typical Symptom                           |
| ---------------------------------- | ----------------------------------------- |
| Pods not created on all nodes      | Missing Pods or uneven distribution       |
| Pods stuck in `Pending`            | Node selector mismatch or taints          |
| CrashLoopBackOff                   | Bad config, permissions, or missing paths |
| Updates not rolling                | Old Pods not replaced, image not updating |
| Pod not deleted after node removal | Orphaned Pod object left behind           |

</div>

---

## ğŸ§© 1. **Pods Missing on Certain Nodes**

### ğŸ” Symptom

```bash
kubectl get pods -o wide -l app=node-logger
```

Output:

```ini
NAME                NODE
node-logger-abc12   worker-1
node-logger-def45   worker-2
# Missing on worker-3
```

### ğŸ“‹ Root Causes

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Cause                     | Explanation                              |
| ------------------------- | ---------------------------------------- |
| NodeSelector mismatch     | Node lacks required labels               |
| Taint prevents scheduling | Node tainted without matching toleration |
| Node NotReady             | Node in maintenance or cordoned          |
| Resource pressure         | Node canâ€™t fit more Pods                 |

</div>

---

### ğŸ§  Diagnosis Steps

```bash
kubectl describe daemonset node-logger
```

Look for:

```ini
Events:
  Warning  FailedScheduling  No nodes match node selector
```

Then check node labels:

```bash
kubectl get nodes --show-labels
```

And taints:

```bash
kubectl describe node worker-3 | grep Taint
```

---

### ğŸ©¹ Fix

- **Add required label to node:**

  ```bash
  kubectl label node worker-3 node-role.kubernetes.io/worker=""
  ```

- **Add toleration to DaemonSet:**

  ```yaml
  spec:
    template:
      spec:
        tolerations:
          - key: "dedicated"
            operator: "Equal"
            value: "worker"
            effect: "NoSchedule"
  ```

- **Uncordon node (if drained):**

  ```bash
  kubectl uncordon worker-3
  ```

âœ… Reapply the DaemonSet and verify Pods appear on all nodes.

---

## âš™ï¸ 2. **Pods Stuck in `Pending`**

### ğŸ” Symptom

```bash
kubectl get pods
```

```ini
node-logger-xyz Pending
```

### ğŸ§  Diagnosis

Check events:

```bash
kubectl describe pod node-logger-xyz
```

Look for:

```ini
0/4 nodes are available: insufficient CPU
```

Or:

```ini
No nodes match node selector
```

### ğŸ©¹ Fix

- Check cluster capacity:

  ```bash
  kubectl top nodes
  ```

- Increase node pool size or lower Pod resource requests.
- Confirm node affinity rules are not too restrictive.

---

## âš™ï¸ 3. **CrashLoopBackOff**

### ğŸ” Symptom

Pod restarts repeatedly:

```bash
kubectl get pods
```

```ini
node-logger-abcd   0/1   CrashLoopBackOff
```

### ğŸ§  Diagnosis

Check logs:

```bash
kubectl logs node-logger-abcd
```

If it says:

```ini
permission denied /var/log/syslog
```

â†’ your `hostPath` or container permissions are wrong.

### ğŸ©¹ Fix

- Validate your volume mount path:

  ```yaml
  volumes:
    - name: logs
      hostPath:
        path: /var/log
  ```

- Add proper `securityContext` if needed:

  ```yaml
  securityContext:
    runAsUser: 0
  ```

- Fix file permissions on host node if itâ€™s a shared directory.

---

## ğŸ” 4. **DaemonSet Not Updating**

### ğŸ” Symptom

You updated the image version, but Pods donâ€™t change:

```bash
kubectl apply -f daemonset.yaml
kubectl get pods -l app=node-logger -o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[0].image
```

Output:

```ini
node-logger-1   filebeat:8.10.0
node-logger-2   filebeat:8.10.0
```

Even though YAML says `filebeat:8.11.0`.

### ğŸ“‹ Root Causes

- Update strategy not set to `RollingUpdate`
- Selector mismatch between old and new template
- Cached image or pull errors

---

### ğŸ§  Diagnosis

```bash
kubectl describe daemonset node-logger | grep Strategy
kubectl get events --sort-by=.metadata.creationTimestamp
```

Look for:

```ini
Failed to pull image
```

---

### ğŸ©¹ Fix

- Ensure update strategy:

  ```yaml
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  ```

- Restart rollout:

  ```bash
  kubectl rollout restart daemonset node-logger
  ```

- Check rollout progress:

  ```bash
  kubectl rollout status daemonset node-logger
  ```

âœ… Pods are replaced one by one.

---

## ğŸ§± 5. **Orphaned Pods After Node Removal**

### ğŸ” Symptom

A node was deleted, but DaemonSet Pods remain in the API:

```ini
kubectl get pods -A
```

You see Pods on non-existing nodes.

### ğŸ§  Cause

Controller couldnâ€™t clean up before node removal (e.g., forced deletion).

---

### ğŸ©¹ Fix

Manually remove them:

```bash
kubectl delete pod <pod-name> --force --grace-period=0
```

Or clean up all non-existent-node Pods:

```bash
for pod in $(kubectl get pods -A --field-selector=status.phase=Unknown -o name); do
  kubectl delete $pod --force --grace-period=0
done
```

---

## ğŸ”’ 6. **DaemonSet Scheduling Control** (Affinity & Anti-Affinity)

DaemonSets often need **fine-grained node targeting**.

### ğŸ¯ Node Affinity Example

Run only on storage nodes:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-type
                    operator: In
                    values:
                      - storage
```

### ğŸš« Pod Anti-Affinity Example

Avoid running multiple agent Pods on same node type:

```yaml
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app
              operator: In
              values:
                - node-agent
        topologyKey: "kubernetes.io/hostname"
```

---

## ğŸ§ª **Checking DaemonSet Rollout & Events**

```bash
kubectl rollout status daemonset node-logger
kubectl rollout history daemonset node-logger
kubectl get events --sort-by=.metadata.creationTimestamp
```

---

## ğŸ”– **Debugging Checklist**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Step        | Command                                   | Purpose                      |
| ----------- | ----------------------------------------- | ---------------------------- |
| Describe DS | `kubectl describe ds <name>`              | View strategy, nodes, events |
| List Pods   | `kubectl get pods -o wide -l app=<label>` | Check distribution           |
| Node labels | `kubectl get nodes --show-labels`         | Validate selector coverage   |
| Node taints | `kubectl describe node <node>`            | Detect scheduling blocks     |
| Logs        | `kubectl logs <pod>`                      | Detect runtime errors        |
| Events      | `kubectl get events`                      | Find rollout or pull errors  |

</div>

---

## ğŸ§­ **Pro Tips from Real Clusters**

- ğŸ”¹ **Always label nodes** clearly:
  e.g., `role=worker`, `zone=us-east1-b`, `gpu=true`.
- ğŸ”¹ **Donâ€™t overuse hostPath** â€” prefer `emptyDir` or `configMap` when possible.
- ğŸ”¹ Use `kubectl get ds -A -o wide` to instantly check Pod/node mapping.
- ğŸ”¹ Monitor rollout via:

  ```bash
  watch kubectl get pods -l app=node-logger
  ```

- ğŸ”¹ When using monitoring/logging agents, keep them in **dedicated namespaces** (`logging`, `monitoring`, etc.) for easier RBAC control.

---

## âœ… **Summary**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Problem          | Cause                        | Fix                                 |
| ---------------- | ---------------------------- | ----------------------------------- |
| Pods missing     | Node selector/taint          | Add labels/tolerations              |
| Pods pending     | Insufficient resources       | Resize node or adjust requests      |
| CrashLoopBackOff | Bad config/permissions       | Check hostPath + logs               |
| Not updating     | Wrong strategy or image pull | Set RollingUpdate & restart rollout |
| Orphaned Pods    | Node removed                 | Force delete stale Pods             |

</div>
