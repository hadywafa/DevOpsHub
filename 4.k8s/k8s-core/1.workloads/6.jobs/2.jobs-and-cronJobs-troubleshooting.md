# ⚙️ **Jobs & CronJobs 102 — Advanced Patterns, Parallelism & Troubleshooting**

> 🎯 **Goal:** Master advanced admin tasks for Kubernetes **Jobs** and **CronJobs**, including parallel job execution, failure handling, retries, monitoring, cleanup automation, and real troubleshooting techniques.

---

## 🧠 Deep Dive: Job Execution Models

Jobs in Kubernetes can run **one Pod**, **many Pods sequentially**, or **many Pods in parallel**.

### 🧩 Modes of Job Execution

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Mode                   | Description                  | Example                                   |
| ---------------------- | ---------------------------- | ----------------------------------------- |
| **Single Job**         | One Pod, one completion      | Basic batch task                          |
| **Fixed Parallel Job** | Multiple Pods in parallel    | Data chunk processing                     |
| **Indexed Job**        | Pods know their index number | Distributed workloads (e.g., ML training) |

</div>

---

## 🔢 Parallel Job Patterns

### 🧱 Example 1: Non-Indexed Parallel Job

All Pods are identical — used for jobs like bulk data transformation or image processing.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: image-transform
spec:
  completions: 6
  parallelism: 3
  template:
    spec:
      containers:
        - name: transformer
          image: alpine
          command: ["sh", "-c", "echo Transforming data on $(hostname); sleep 5"]
      restartPolicy: Never
```

🧠 **How it behaves:**

- Runs **3 Pods at a time**
- Continues until **6 Pods have completed successfully**

---

### 🧱 Example 2: Indexed Parallel Job (K8s 1.21+)

Each Pod gets an **index number** via environment variable `JOB_COMPLETION_INDEX`.

Used for **distributed data partitioning**.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: data-indexer
spec:
  completions: 4
  parallelism: 2
  completionMode: Indexed
  template:
    spec:
      containers:
        - name: worker
          image: busybox
          command:
            - sh
            - -c
            - |
              echo "Processing chunk index $(JOB_COMPLETION_INDEX)"
              sleep 3
      restartPolicy: Never
```

Output:

```ini
Processing chunk index 0
Processing chunk index 1
Processing chunk index 2
Processing chunk index 3
```

✅ Each Pod works on a unique dataset chunk.

---

## 🧩 Handling Job Failures & Retries

Failures are **normal** in Jobs. Kubernetes uses the **backoff mechanism** to retry.

### ⚙️ Key Parameters

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Field                     | Description                           | Default   |
| ------------------------- | ------------------------------------- | --------- |
| `backoffLimit`            | Max retry attempts before failing Job | 6         |
| `activeDeadlineSeconds`   | Max total runtime for Job             | Unlimited |
| `ttlSecondsAfterFinished` | Auto-delete Job after finish          | None      |

</div>

Example:

```yaml
spec:
  backoffLimit: 4
  activeDeadlineSeconds: 120
  ttlSecondsAfterFinished: 30
```

---

### 💡 Admin Tip:

Use `activeDeadlineSeconds` to ensure **hung Pods** don’t block resources indefinitely.

---

## 🧰 Troubleshooting Failing Jobs

### 🔍 Step 1 — Check Job status

```bash
kubectl get jobs
kubectl describe job <job-name>
```

Look for:

```ini
BackoffLimitExceeded
FailedCreate
DeadlineExceeded
```

---

### 🔍 Step 2 — Inspect Events

```bash
kubectl get events --sort-by=.metadata.creationTimestamp
```

Common errors:

- `FailedScheduling` → No nodes fit
- `ImagePullBackOff` → Auth or tag issue
- `OOMKilled` → Container out of memory

---

### 🔍 Step 3 — Review Pod logs

```bash
kubectl logs job/<job-name>
```

If multiple Pods exist:

```bash
kubectl logs <pod-name>
```

---

### 🔍 Step 4 — Restart job cleanly

```bash
kubectl delete job <job-name> && kubectl apply -f job.yaml
```

Or patch to force restart:

```bash
kubectl rollout restart job <job-name>
```

---

## ⚙️ Job Monitoring in Production

Use `kubectl` and metrics to track job health.

### Check status continuously:

```bash
watch kubectl get jobs
```

### Check job metrics (via metrics-server or Prometheus):

- Job completion rate
- Pod success/failure counts
- Duration per run

### Example — JSON output:

```bash
kubectl get job data-indexer -o jsonpath='{.status}'
```

Output:

```json
{
  "active": 0,
  "failed": 1,
  "succeeded": 3
}
```

---

## 🧩 TTL Controller — Automatic Cleanup

The **TTLAfterFinished controller** (enabled by default in modern clusters) automatically deletes old Job objects.

Example:

```yaml
spec:
  ttlSecondsAfterFinished: 300
```

🧠 _The Job and its Pods are deleted 5 minutes after finishing._

View controller status:

```bash
kubectl get deployment -n kube-system | grep ttl
```

---

## 🕒 CronJob Scheduling — Deep Dive

### 🎯 CronJob Internals

CronJob Controller → creates Job objects → Job Controller → manages Pods.

### 📦 Concurrency Handling

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Setting   | Effect                            |
| --------- | --------------------------------- |
| `Allow`   | Multiple runs can overlap         |
| `Forbid`  | Skip new run if old still running |
| `Replace` | Kill old job, start new one       |

</div>

Example:

```yaml
spec:
  concurrencyPolicy: Forbid
```

🧠 Use `Forbid` for **stateful** or **non-idempotent** tasks (like DB backups).

---

### 🧩 Missed Runs (Catch-Up Jobs)

If the controller misses a schedule (due to downtime or delay), use:

```yaml
spec:
  startingDeadlineSeconds: 200
```

Kubernetes will **catch up** missed runs if within 200 seconds.

---

## 🧱 Real Example — Cleanup CronJob with Safety Controls

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: temp-cleaner
spec:
  schedule: "*/10 * * * *" # every 10 minutes
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 300
  jobTemplate:
    spec:
      backoffLimit: 2
      ttlSecondsAfterFinished: 60
      template:
        spec:
          containers:
            - name: cleaner
              image: alpine
              command: ["sh", "-c", "echo Cleaning /tmp at $(date); rm -rf /tmp/*"]
          restartPolicy: OnFailure
```

✅ Safe cleanup every 10 minutes  
✅ No overlapping runs  
✅ Auto-deletes old Jobs

---

## 🧰 CronJob Troubleshooting Scenarios

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Symptom                | Likely Cause              | Fix                                                           |
| ---------------------- | ------------------------- | ------------------------------------------------------------- |
| CronJob didn’t run     | Wrong cron syntax         | Verify with crontab.guru                                      |
| Jobs overlap           | concurrencyPolicy = Allow | Use Forbid                                                    |
| Job failed immediately | Container error           | Check Pod logs                                                |
| Missed runs            | Controller downtime       | Set startingDeadlineSeconds                                   |
| Too many old Jobs      | Missing history limits    | Set `failedJobsHistoryLimit` and `successfulJobsHistoryLimit` |

</div>

---

### 🧠 Diagnostic Commands

```bash
kubectl describe cronjob <name>
kubectl get jobs --selector=cronjob-name=<name>
kubectl get events | grep <cronjob-name>
```

---

## 🧩 Cleanup Strategies for Admins

### Option 1 — TTL Controller

Already covered, cleans after finish.

### Option 2 — History Limits (CronJob)

```yaml
spec:
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 1
```

### Option 3 — Manual Cleanup

```bash
kubectl delete job --all
```

or targeted:

```bash
kubectl delete job -l app=batch
```

---

## 🔒 RBAC & Security for Jobs/CronJobs

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Component               | Best Practice                                              |
| ----------------------- | ---------------------------------------------------------- |
| **Namespace isolation** | Place Jobs in `batch`, `data`, or `maintenance` namespaces |
| **ServiceAccount**      | Assign least privilege SA                                  |
| **Secrets/ConfigMaps**  | Inject via env or volumeMounts                             |
| **NetworkPolicy**       | Restrict egress from batch workloads                       |
| **Resource Quotas**     | Prevent runaway parallel jobs                              |

</div>

Example:

```yaml
spec:
  template:
    spec:
      serviceAccountName: batch-runner
```

---

## 📈 Observability and Alerting

Integrate with **Prometheus** and **Grafana** dashboards:

- `kube_job_status_failed`
- `kube_job_status_succeeded`
- `kube_job_duration_seconds`

Alert if:

- Job success rate < 90%
- Duration > expected threshold
- Too many pending jobs

---

## 🧭 Pro Tips from the Field

💡 **Use Labels Everywhere:**

```yaml
metadata:
  labels:
    app: batch-job
    purpose: data-load
```

💡 **Avoid Overlapping Schedules:**
Use staggered cron expressions for large workloads.

💡 **Prefer Short-Lived Pods:**
Keep Job Pods under 10–15 minutes where possible for reliability.

💡 **Retry Gracefully:**
Set `restartPolicy: OnFailure`, not `Never`.

💡 **Use Namespace Quotas:**
Protect cluster stability from uncontrolled batch jobs.

---

## ✅ Summary Table

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Feature           | Job                        | CronJob                    |
| ----------------- | -------------------------- | -------------------------- |
| **Type**          | One-time batch             | Scheduled recurring        |
| **Parallelism**   | Supported                  | Supported (per run)        |
| **Retries**       | `backoffLimit`             | Per Job run                |
| **Timeouts**      | `activeDeadlineSeconds`    | Each run independently     |
| **Cleanup**       | TTL or manual              | TTL + history limits       |
| **Concurrency**   | Pod-level                  | Job-level                  |
| **Security**      | Use SA + RBAC              | Same as Job                |
| **Common Issues** | Pending, Backoff, Failures | Missed or overlapping runs |

</div>
