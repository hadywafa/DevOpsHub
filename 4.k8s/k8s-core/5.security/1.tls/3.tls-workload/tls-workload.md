# ğŸ” mTLS in Kubernetes with Istio â€” Full Flow in Your Style

> Goal:
> **Encrypt all pod-to-pod traffic + give each service a strong identity + control who can call whom**, _without_ changing your app code.

Weâ€™ll go through:

1. ğŸ§± Concept: What is mTLS in Istio?
2. ğŸ§  Architecture: How Istio does mTLS under the hood
3. âš™ï¸ Enabling mTLS (mesh-wide, namespace, and per workload)
4. ğŸ¯ Locking traffic using AuthorizationPolicy
5. ğŸ” Verifying that mTLS is actually working
6. âš ï¸ Common pitfalls (the â€œwhy is everything brokenâ€ section)

---

## ğŸ“– What is mTLS in Istio?

Normal Kubernetes (no mesh):

- `frontend` â†’ `backend` sends plain HTTP/TCP inside the cluster.
- Anyone who can reach the network can:

  - Sniff traffic
  - Impersonate services (no real identity)

With **Istio mTLS**:

- Each pod has a **sidecar (Envoy)**.
- All traffic between services **goes through Envoy**.
- Envoy â†” Envoy communication uses **mutual TLS**:

  - Traffic is **encrypted**
  - Each side **authenticates** the other using certificates.
  - Each workload has a strong identity:
    `spiffe://cluster.local/ns/<namespace>/sa/<service-account>`

So it becomes:

```text
App A â†’ Envoy A â†’ ğŸ” mTLS â†’ Envoy B â†’ App B
```

Your code still does:

```http
http://backend:8080
```

But wire-level traffic is **mTLS**, not plain.

---

## âš™ï¸ How Istio implements mTLS under the hood

### Components involved:

- **Istiod** = control plane + Certificate Authority (by default).
- **Envoy sidecar** = data plane proxy injected beside each pod.

### Flow when a pod starts:

1. Pod comes up in a namespace with `istio-injection=enabled`.
2. Sidecar container (Envoy) is injected.
3. Envoy connects to **Istiod** using bootstrap certs.
4. Istiod issues a **short-lived X.509 cert** (SPIFFE identity) to Envoy:

   - `spiffe://cluster.local/ns/my-namespace/sa/my-service-sa`

5. Envoy stores this cert + private key and uses it for:

   - Client auth (when calling others)
   - Server auth (when others call it)

### Flow when Service A calls Service B:

1. App A calls `http://service-b:8080`
2. iptables rules redirect traffic â†’ Envoy A.
3. Envoy A looks at Istio config and decides:

   - â€œI must talk to service B over mTLSâ€

4. Envoy A opens a **TLS connection** to Envoy B:

   - Sends its client certificate (Aâ€™s identity)
   - Verifies Bâ€™s server certificate (Bâ€™s identity)

5. If both sides verify â†’ secure mTLS tunnel
6. Inside that tunnel, HTTP/gRPC traffic flows.

Your app never sees TLS â€” sidecars handle everything.

---

## ğŸªœ Enabling mTLS â€” Step by Step

### ğŸ”¹ Step 0: Enable sidecar injection

For any namespace you want in the mesh:

```bash
kubectl label namespace my-app istio-injection=enabled
```

Then **restart pods** in that namespace so they get Envoy.

---

### ğŸ”¹ Step 1: Mesh-wide mTLS with PeerAuthentication

Istio uses `PeerAuthentication` to configure **how workloads expect incoming traffic**:

- `STRICT` â†’ must be mTLS
- `PERMISSIVE` â†’ accept both mTLS and plain
- `DISABLE` â†’ plain only

#### âœ… Option A â€” Start safe with PERMISSIVE (good migration strategy)

```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: istio-system
spec:
  mtls:
    mode: PERMISSIVE
```

- Pods with sidecars:

  - Prefer mTLS for in-mesh traffic
  - Still accept plain traffic (from legacy/non-mesh clients).

#### âœ… Option B â€” Go full secure with STRICT (everything must be mTLS)

```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: istio-system
spec:
  mtls:
    mode: STRICT
```

- Now, for all workloads in the mesh:

  - Incoming traffic must be **mTLS**, not plain.
  - Non-mesh clients talking directly to pods will **fail**.

> ğŸ§  In practice:
> Many teams go: `DISABLE â†’ PERMISSIVE â†’ STRICT`
> as they gradually inject sidecars and migrate clients.

---

### ğŸ”¹ Step 2: Namespace-level mTLS (your â€œstagingâ€ / â€œprodâ€ style)

You donâ€™t have to do mesh-wide all at once.
You can control namespace by namespace.

Example: enable STRICT mTLS only in `payments` namespace:

```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: payments-strict
  namespace: payments
spec:
  mtls:
    mode: STRICT
```

Now:

- Every workload in `payments` expects **mTLS** on inbound.
- Other namespaces can still be permissive or disabled.

---

### ğŸ”¹ Step 3: Workload-level mTLS (very similar to your Cilium style: selector)

Example: only `app=backend` in namespace `shop` should enforce mTLS:

```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: backend-strict
  namespace: shop
spec:
  selector:
    matchLabels:
      app: backend
  mtls:
    mode: STRICT
```

- Only pods with `app=backend` require mTLS inbound.
- Other pods in `shop` use whatever other PeerAuthentication applies (namespace/global).

---

## ğŸ‘®ğŸ» Using identities with AuthorizationPolicy (zero trust)

mTLS gives you **who you are** (identity).
`AuthorizationPolicy` uses that identity to decide **who can call whom**.

### Identity format (SPIFFE):

```text
spiffe://cluster.local/ns/<namespace>/sa/<service-account>
```

So if a pod uses service account `frontend-sa` in namespace `shop`, its identity is:

```text
cluster.local/ns/shop/sa/frontend-sa
```

(Thatâ€™s what you put in `principals`.)

---

### ğŸ”¹ Example: only frontend â†’ backend (same namespace)

Namespace: `shop`
Service accounts: `frontend-sa`, `backend-sa`

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: backend-ingress
  namespace: shop
spec:
  selector:
    matchLabels:
      app: backend # this policy applies to backend pods
  rules:
    - from:
        - source:
            principals:
              - "cluster.local/ns/shop/sa/frontend-sa"
```

Meaning:

- Target = all pods with `app=backend` in `shop`.
- Allowed source = **only workloads using `frontend-sa` in `shop`**.
- Everyone else â†’ denied.

Now couple this with:

```yaml
PeerAuthentication (shop namespace) â†’ STRICT mTLS
```

And you have:

- Encrypted traffic
- Strong identity
- AuthZ based on service accounts

---

### ğŸ”¹ Example: backend â†’ db only, and nothing else

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: db-ingress
  namespace: shop
spec:
  selector:
    matchLabels:
      app: db
  rules:
    - from:
        - source:
            principals:
              - "cluster.local/ns/shop/sa/backend-sa"
```

Result:

- Only pods with identity `cluster.local/ns/shop/sa/backend-sa` can call `app=db`.
- frontend â†’ db is blocked, even though they share namespace.

Thatâ€™s clean zero-trust segmentation with mTLS identities.

---

## ğŸ” Verifying that mTLS is actually working

### ğŸ§ª Check TLS mode between services

Istio has a handy command:

```bash
istioctl authn tls-check <src-pod>.<ns> <dst-service>.<ns>.svc.cluster.local
```

Example:

```bash
istioctl authn tls-check frontend-5c8d96b68c-abcde.shop backend.shop.svc.cluster.local
```

Youâ€™ll get output like:

- `mTLS` âœ… (what you want)
- `plaintext` âŒ (no TLS)
- `TLS` (one-way TLS, usually external)

---

### ğŸ” Inspect certificates on a podâ€™s sidecar

```bash
istioctl pc secret <pod-name>.<namespace>
```

Youâ€™ll see the issued cert:

- CN / SANs including:

  - `spiffe://cluster.local/ns/...`

---

### ğŸ‘€ Use Kiali (if installed)

- Graph view shows **lock icons** between services when mTLS is on.
- You can visually see which services talk mTLS vs plaintext.

---

## âš ï¸ Common Pitfalls (aka â€œwhy did everything break?â€)

### âŒ 1. Enabling STRICT mTLS while some clients are **not in the mesh**

- If a pod doesnâ€™t have a sidecar, it cannot speak Istio mTLS.
- STRICT means: â€œI only accept mTLS from Envoy clients.â€
- So:

  - Non-injected workloads
  - External tools hitting ClusterIP directly
    will suddenly fail.

âœ… Fix: start with `PERMISSIVE` during migration.

---

### âŒ 2. Forgetting service accounts when using `principals`

Your `AuthorizationPolicy` uses:

```yaml
principals:
  - cluster.local/ns/shop/sa/frontend-sa
```

But your Deployment:

```yaml
spec:
  serviceAccountName: another-name
```

Now the identity doesnâ€™t match â†’ everything denied.

âœ… Fix: always align `serviceAccountName` with the identities in your AuthorizationPolicy.

---

### âŒ 3. Trying to mTLS external services directly

Istio mTLS is **for in-mesh workloads**.
For external APIs (e.g., Stripe, GitHub), you:

- Use **egress gateways** or **simple TLS origination** (client-side TLS)
- Thatâ€™s _not_ the same as Istio internal SPIFFE mTLS.

---

### âŒ 4. Forgetting sidecar injection

- If namespace is not labeled `istio-injection=enabled`, or you did not re-deploy pods, there will be **no sidecar**.
- No sidecar == no mTLS.

---

## ğŸ¯ Mini â€œend-to-endâ€ example in your style

Letâ€™s define a small â€œshopâ€ app:

- Namespace: `shop`
- Workloads: `frontend` (sa `frontend-sa`), `backend` (sa `backend-sa`), `db` (sa `db-sa`)
- Requirements:

  - All service-to-service traffic is mTLS.
  - Only `frontend` â†’ `backend`.
  - Only `backend` â†’ `db`.

### 1ï¸âƒ£ Enable injection

```bash
kubectl create namespace shop
kubectl label namespace shop istio-injection=enabled
```

Deploy your services with proper `serviceAccountName`s.

---

### 2ï¸âƒ£ Enable STRICT mTLS in `shop`

```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: shop-strict
  namespace: shop
spec:
  mtls:
    mode: STRICT
```

---

### 3ï¸âƒ£ Authorization: frontend â†’ backend only

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: backend-ingress
  namespace: shop
spec:
  selector:
    matchLabels:
      app: backend
  rules:
    - from:
        - source:
            principals:
              - "cluster.local/ns/shop/sa/frontend-sa"
```

---

### 4ï¸âƒ£ Authorization: backend â†’ db only

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: db-ingress
  namespace: shop
spec:
  selector:
    matchLabels:
      app: db
  rules:
    - from:
        - source:
            principals:
              - "cluster.local/ns/shop/sa/backend-sa"
```

Now you have:

- ğŸ” All traffic between services = mTLS
- ğŸ‘® Identity-aware policies = who can call whom
- ğŸš« Any other service: denied automatically
