# ü¶• **Static Provisioning Deep Dive**

## üìñ **Concept Overview**

Normally when you use a StorageClass (like `gp3`), Kubernetes dynamically creates a **new disk** each time a PVC is created.

But if you already have disks (like pre-provisioned EBS volumes or Azure Managed Disks), you can **manually register** them in Kubernetes as **PersistentVolumes (PVs)** and **bind** them to **PVCs** manually.

This is known as **static provisioning**.

---

## üñºÔ∏è **Architecture Recap**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

```mermaid
flowchart LR
    A[Existing Disk in Cloud] --> B["PersistentVolume (manual)"]
    B --> C["PersistentVolumeClaim (user)"]
    C --> D["Pod (mounts the claim)"]
```

</div>

---

In dynamic provisioning:

- PVs are created automatically by the StorageClass and CSI driver.

In static provisioning:

- You **create the PV yourself** and tell Kubernetes which existing disk to use.

---

## ‚òÅÔ∏è **Case 1: AWS EBS ‚Äî Using Existing EBS Volume**

### Step 1Ô∏è‚É£ ‚Äî Check your existing EBS volume

You can list existing volumes in AWS:

```bash
aws ec2 describe-volumes --region us-east-1 --query "Volumes[*].{ID:VolumeId,AZ:AvailabilityZone,State:State,Size:Size}"
```

Let‚Äôs say you already have:

```ini
vol-0abcd123456789xyz  | us-east-1a | in-use=false | 20GiB
```

That‚Äôs your target volume.

---

### Step 2Ô∏è‚É£ ‚Äî Create a PersistentVolume (PV)

Here‚Äôs how you manually register that EBS volume:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-existing-ebs
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: vol-0abcd123456789xyz # your actual AWS volume ID
    fsType: ext4
```

**Notes:**

- `volumeHandle` is literally your existing AWS EBS volume ID.
- `storageClassName: manual` is optional ‚Äî but we‚Äôll match it later in PVC.
- `persistentVolumeReclaimPolicy: Retain` ensures Kubernetes won‚Äôt delete the disk if you delete the claim.

---

### Step 3Ô∏è‚É£ ‚Äî Create a PersistentVolumeClaim (PVC)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-existing-ebs
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: manual
```

When this PVC is created:

- Kubernetes sees the PV ‚Äúpv-existing-ebs‚Äù matches the PVC specs (same size, access mode, SC name).
- It automatically binds the PVC to that PV.
- You can verify:

```bash
kubectl get pv,pvc
```

You should see:

```ini
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS
pv-existing-ebs     20Gi       RWO            Retain           Bound    default/pvc-existing-ebs   manual
```

---

### Step 4Ô∏è‚É£ ‚Äî Mount in a Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-ebs
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: ebsdata
  volumes:
    - name: ebsdata
      persistentVolumeClaim:
        claimName: pvc-existing-ebs
```

Now your Pod will mount the **existing EBS volume** directly.

If you write to `/usr/share/nginx/html`, the data is saved on your existing disk!

---

### Step 5Ô∏è‚É£ ‚Äî Confirm the Mount

Inside the Pod:

```bash
kubectl exec -it test-ebs -- df -h
```

You‚Äôll see your `/usr/share/nginx/html` mounted to `/var/lib/kubelet/pods/.../volumes/kubernetes.io~csi/...`

---

## üî• **Tip: Retain Policy for Safety**

If you delete your PVC or Pod, the EBS volume is **not deleted** because of:

```yaml
persistentVolumeReclaimPolicy: Retain
```

That‚Äôs crucial when using existing disks ‚Äî otherwise, you risk Kubernetes deleting your real data.

---

## ‚òÅÔ∏è **Case 2: Azure Disk ‚Äî Using Existing Managed Disk**

The concept is the same; the driver and syntax differ slightly.

### Step 1Ô∏è‚É£ ‚Äî Find your existing Azure disk

List your managed disks:

```bash
az disk list --resource-group myResourceGroup --query "[].{Name:name,Id:id,Size:diskSizeGb,Zone:zones}" -o table
```

Assume the disk ID is:

```ini
/subscriptions/xxxxx/resourceGroups/myResourceGroup/providers/Microsoft.Compute/disks/my-existing-disk
```

---

### Step 2Ô∏è‚É£ ‚Äî Create the PV

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-existing-azuredisk
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: disk.csi.azure.com
    volumeHandle: /subscriptions/xxxxx/resourceGroups/myResourceGroup/providers/Microsoft.Compute/disks/my-existing-disk
    fsType: ext4
```

üí° The `volumeHandle` is the **full Azure Resource ID** of the existing disk.

---

### Step 3Ô∏è‚É£ ‚Äî Create the PVC

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-existing-azuredisk
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: manual
```

Kubernetes binds them automatically (same as AWS case).

---

### Step 4Ô∏è‚É£ ‚Äî Mount in a Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-azuredisk
spec:
  containers:
    - name: web
      image: nginx
      volumeMounts:
        - mountPath: /mnt/azure
          name: azdata
  volumes:
    - name: azdata
      persistentVolumeClaim:
        claimName: pvc-existing-azuredisk
```

Done ‚Äî your Pod now uses your existing Azure disk. üü¢

---

## ‚öôÔ∏è **Static vs Dynamic Summary**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Aspect                | Static Provisioning                    | Dynamic Provisioning                   |
| --------------------- | -------------------------------------- | -------------------------------------- |
| Who creates PV        | Admin manually                         | Kubernetes auto-creates                |
| When PV created       | Before PVC                             | After PVC creation                     |
| StorageClass required | Optional                               | Required                               |
| Common use case       | Reuse existing disks, migrate old data | Cloud-native app deployments           |
| Risk of deletion      | Controlled by `Retain`                 | Controlled by SC policy                |
| Example               | Pre-existing AWS EBS / Azure Disk      | New volume from gp3 or managed-premium |

</div>

---

## üëÄ **Behind the Scenes (CSI Driver View)**

Even though you‚Äôre using an existing volume, the **CSI driver** still participates:

- It still handles the **mount/attach/detach** operations.
- But it skips **CreateVolume**, since the volume already exists.

The PV `csi.volumeHandle` tells the driver which existing backend volume to attach to the node.

Example flow:

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

```mermaid
sequenceDiagram
    participant K8s as K8s Control Plane
    participant CSI as CSI Driver (Azure or AWS)
    participant Cloud as Cloud API

    K8s->>CSI: AttachVolume (vol-0abcd123456789xyz)
    CSI->>Cloud: Attach existing volume to node
    Cloud-->>CSI: OK, attached
    CSI-->>K8s: VolumeReady
    K8s->>Node: Mount /var/lib/kubelet/pods/.../volumes
```

</div>

---

So the volumeHandle bridges Kubernetes PV to real backend volume.

---

## üîÇ **Reusing Volumes Across Clusters**

You can attach the same volume to a new cluster ‚Äî but be careful:

- Most volumes (`ReadWriteOnce`) can only be attached to **one node at a time**.
- You must **detach** it from any old EC2/VM before Kubernetes can use it again.
- For shared access, you need `ReadWriteMany` backends like:

  - AWS EFS
  - Azure Files
  - NFS
  - CephFS

---

## üí° **Troubleshooting Tips**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Symptom                       | Root Cause                      | Fix                           |
| ----------------------------- | ------------------------------- | ----------------------------- |
| PVC stuck in `Pending`        | PV size/SC mismatch             | Match specs exactly           |
| Pod stuck `ContainerCreating` | Volume still attached elsewhere | Detach from old EC2/VM        |
| Volume attach failed          | Wrong AZ or node                | Check zone alignment          |
| Volume deleted accidentally   | Wrong reclaim policy            | Always use `Retain`           |
| ‚ÄúRead-only filesystem‚Äù        | Volume was previously used      | Reformat or fsck before reuse |

</div>

---

## üí≠ **Migration Use Case (Common Real-World Scenario)**

Imagine:

- You ran an app directly on EC2 with EBS volumes.
- Now you‚Äôre moving that app into Kubernetes.

‚úÖ You can reuse your EBS data:

1. Stop EC2 instance.
2. Note the EBS volume ID.
3. Create PV using that volumeHandle.
4. Bind to PVC.
5. Mount in Pod.
6. Your data is now accessible inside Kubernetes.

That‚Äôs how large enterprises migrate legacy workloads without data loss.

---

## üß∞ **Key Takeaways**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

| Concept                  | Explanation                                                      |
| ------------------------ | ---------------------------------------------------------------- |
| **Static Provisioning**  | Manually create PVs for existing disks                           |
| **Dynamic Provisioning** | Kubernetes auto-creates PVs via StorageClass                     |
| **volumeHandle**         | Unique backend disk identifier (EBS ID, Azure resource ID, etc.) |
| **CSI Driver**           | Still handles mount/attach even in static mode                   |
| **ReclaimPolicy**        | Use `Retain` for existing volumes                                |
| **Zone Awareness**       | Ensure PV and Node are in same AZ                                |
| **RWX/RWO Modes**        | Match your backend capability                                    |

</div>

---

## üèÅ **Diagram Summary**

<div align="center" style="background-color: #141a19ff;color: #a8a5a5ff; border-radius: 10px; border: 2px solid">

```mermaid
flowchart TD
    A[(Existing AWS EBS / Azure Disk)] -->|Admin manually defines| B["PersistentVolume (PV)"]
    B -->|Bound automatically| C["PersistentVolumeClaim (PVC)"]
    C -->|Referenced by| D[Pod]
    D -->|Mounted via| E[CSI Driver]
```

</div>

---

‚úÖ **In short:**

- Yes ‚Äî you can absolutely use your existing AWS EBS or Azure disks.
- You just register them as **PersistentVolumes** manually using the correct `volumeHandle`.
- Kubernetes won‚Äôt create or delete them (if you set `Retain`).
- The CSI driver only attaches/detaches/mounts them.
