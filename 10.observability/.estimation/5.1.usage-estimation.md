**Considering telecom project details**

Okay, so the user wants a detailed document on monthly estimates for logs and traces for their telecom project. I’ll need to break it down into structured sections, like assumptions, trace sizing, log sizing, and a summary table. I’ll use the earlier numbers, like 12 microservices, 1000 RPS, and 100% sampling for the first two days. I’ll also calculate logs: 10GB/day per microservice, so 120GB/day for 12 services, which totals 3.6TB/month. I’ll also mention compression and logical vs physical storage. Let’s get started!

### Project observability sizing overview

Here’s a clean, doc-ready breakdown of **monthly usage estimates** for your telecom project—both **traces** and **logs**—based on the architecture and preferences you shared.

---

## 1. Architecture and traffic assumptions

**System shape**

- **Frontend:** 1 main app (web/mobile entrypoint)
- **Backend:** 12 microservices (auth, billing, catalog, CRM, etc.)
- **Pattern:** One end-to-end user request typically touches multiple of these services.

**Traffic**

- **Throughput:** \( R = 1000 \) requests per second (RPS) to the frontend
- All numbers below are based on **end-to-end requests**, not per-service RPS.

**Trace model**

- **Spans per microservice per request:** 2–5
- **Microservices per request:** up to 12
- **Total spans per trace:**  
  \[
  12 \cdot (2\text{ to }5) \approx 24\text{ to }60 \text{ spans}
  \]
- **Span size:** 1–3 KB
- **Average trace size (planning):**  
  \[
  \approx 80\text{ KB per end-to-end request}
  \]

We’ll use:

- \( Z\_{\text{trace}} = 80\text{ KB} \) per trace

**Log model**

Telecom microservices are log-heavy (billing, payments, CRM, fraud, etc.):

- **Average logs per microservice:** \(\approx 10\text{ GB/day}\)
- **Number of microservices:** 12

We’ll use:

- \( L\_{\text{svc}} = 10\text{ GB/day per service} \)

---

## 2. Trace storage estimation

### 2.1. Base formula

Let:

- \( R = 1000 \) RPS
- \( Z\_{\text{trace}} = 80\text{ KB} \)
- \( D = 86400 \) seconds/day
- \( T = \) sampling rate (1.0 = 100%)

**Daily trace volume:**

\[
V*{\text{day}} = R \cdot T \cdot D \cdot Z*{\text{trace}}
\]

**Monthly trace volume (30 days):**

\[
V*{\text{month}} = 30 \cdot V*{\text{day}}
\]

### 2.2. 100% sampling for 2 days

For **100% sampling** (\(T = 1.0\)):

\[
V\_{\text{day,100\%}} = 1000 \cdot 1.0 \cdot 86400 \cdot 80\text{ KB}
\]

\[
1000 \cdot 86400 = 86{,}400{,}000 \text{ traces/day}
\]

\[
86{,}400{,}000 \cdot 80\text{ KB} \approx 6{,}912{,}000{,}000\text{ KB} \approx 6.9\text{ TB/day}
\]

For **2 days**:

\[
V\_{2\text{d,100\%}} \approx 2 \cdot 6.9\text{ TB} \approx 13.8\text{ TB}
\]

> **Traces @ 100% sampling, 2 days ≈ 13.8 TB**

### 2.3. Best-practice tiered sampling for the remaining 28 days

A practical, production-friendly strategy:

- **Days 1–2:** 100% sampling (full fidelity, incident/debug window)
- **Days 3–7:** 10% sampling (detailed, but cheaper)
- **Days 8–30:** 1% sampling (long-term, trend/forensics)

#### 10% sampling (days 3–7, 5 days)

For **10%** (\(T = 0.10\)):

\[
V\_{\text{day,10\%}} = 6.9\text{ TB} \cdot 0.10 \approx 0.69\text{ TB/day}
\]

For **5 days**:

\[
V\_{5\text{d,10\%}} \approx 5 \cdot 0.69\text{ TB} \approx 3.45\text{ TB}
\]

#### 1% sampling (days 8–30, 23 days)

For **1%** (\(T = 0.01\)):

\[
V\_{\text{day,1\%}} = 6.9\text{ TB} \cdot 0.01 \approx 0.069\text{ TB/day} \approx 69\text{ GB/day}
\]

For **23 days**:

\[
V\_{23\text{d,1\%}} \approx 23 \cdot 0.069\text{ TB} \approx 1.6\text{ TB}
\]

### 2.4. Total monthly trace volume

\[
V\_{\text{month,trace}} \approx 13.8\text{ TB} + 3.45\text{ TB} + 1.6\text{ TB} \approx 18.85\text{ TB}
\]

> **Estimated trace volume per month ≈ 19 TB (logical size)**  
> Backend compression (e.g. Tempo/Jaeger on object storage) will reduce physical storage, but plan capacity on the logical number.

---

## 3. Log storage estimation

### 3.1. Per-microservice log volume

Assumption:

- **Average per microservice:**  
  \[
  L\_{\text{svc}} = 10\text{ GB/day}
  \]
- **Number of microservices:** 12

**Daily logs across all services:**

\[
L\_{\text{day,total}} = 12 \cdot 10\text{ GB/day} = 120\text{ GB/day}
\]

### 3.2. Monthly log volume

For **30 days**:

\[
L\_{\text{month,total}} = 120\text{ GB/day} \cdot 30 = 3600\text{ GB} \approx 3.6\text{ TB/month}
\]

> **Estimated log volume per month ≈ 3–4 TB**

You can treat **3.6 TB/month** as the central estimate, with realistic variation:

- **Low verbosity / no payloads:** 2–3 TB/month
- **High verbosity / JSON / payload logging:** 4–6 TB/month

---

## 4. Combined monthly observability footprint

| Type       | Strategy / Assumption                                  | Estimated Monthly Volume |
| ---------- | ------------------------------------------------------ | ------------------------ |
| **Traces** | 1000 RPS, 80 KB/trace, 100% (2d) + 10% (5d) + 1% (23d) | **≈ 19 TB**              |
| **Logs**   | 12 microservices, ~10 GB/day each                      | **≈ 3.6 TB**             |

> **Total observability (logical) ≈ 22–23 TB per month**

Physical storage will be lower due to compression (especially for logs and traces stored on S3/Blob/GCS with gzip/snappy), but this is a solid **capacity planning baseline**.

---

## 5. How to present this in your project docs

You can summarize the key message like this:

- **Traffic:** 1000 RPS, telecom-grade, 12 microservices
- **Traces:**
  - Per-request trace ≈ 80 KB (24–60 spans across 12 services)
  - 100% sampling for 2 days → ~13.8 TB
  - Tiered sampling (10% for 5 days, 1% for 23 days) → ~5 TB
  - **Total traces ≈ 19 TB/month**
- **Logs:**
  - ≈10 GB/day per microservice
  - 12 services → 120 GB/day
  - **Total logs ≈ 3.6 TB/month**
- **Total observability footprint:** ~22–23 TB/month (logical)

If you tell me which backend you’re leaning toward (e.g. Loki + Tempo on S3/Blob, Elasticsearch, OpenSearch), I can add **index/retention/shard** recommendations and a short “capacity & cost” section you can drop straight into a design doc.
