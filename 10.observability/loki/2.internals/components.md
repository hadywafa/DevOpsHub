# ğŸ§© The `8 Loki components`, grouped by **category**

Think of Loki as 3 pipelines:

- **Write path**: accept logs â†’ replicate â†’ buffer â†’ flush to object store
- **Read path**: accept queries â†’ split/parallelize â†’ fetch chunks â†’ return results
- **Backend/control plane**: index plumbing + compaction + scheduling + rules

## âœ… Write path (ingestion)

These are the â€œlogs are coming in right now!â€ components.

### ğŸ“Œ 1. **Distributor (D)** â€” â€œTraffic cop for logsâ€

- Receives incoming pushes from Promtail/agents (`/loki/api/v1/push`)
- Validates labels/limits, applies tenant settings
- Hashes each stream and chooses which ingesters should own it
- Ensures **replication factor** (send same data to multiple ingesters)

**Mental model:** _Load balancer + sharding router for logs._

---

### ğŸ“Œ 2. **Ingester (I)** â€” â€œHot buffer + flush machineâ€

- Holds log data in-memory as **chunks**
- Periodically **flushes chunks** to **object storage**
- Keeps recent data â€œhotâ€ so queries can read fresh logs quickly
- Often uses **WAL on disk** so a restart doesnâ€™t lose recent unflushed logs

**Mental model:** _RAM cache for recent logs + durable flush to the bucket._

---

## âœ… Read path (query execution)

These are the â€œuser asks a question: show me logsâ€ components.

### ğŸ“Œ 3. **Query Frontend (FE)** â€” â€œQuery optimizer + splitterâ€

- Receives queries (`/loki/api/v1/query_range`, etc.)
- Splits big time-range queries into smaller chunks
- Does queueing, retrying, caching (results cache)
- Makes queries cheaper and safer for the cluster

**Mental model:** _API gateway for queries + â€œmake this query run fastâ€ brain._

---

### ğŸ“Œ 4. **Querier (Q)** â€” â€œWorker that actually runs the queryâ€

- Executes the query pieces given by FE
- Reads:
  - **index** (to find which chunks might match)
  - **chunks** from object storage
  - recent data from ingesters (depending on setup)

- Merges partial results

**Mental model:** _The CPU muscle that searches logs._

---

### ğŸ“Œ 5. **Query Scheduler (QS)** â€” â€œAir traffic control for queriersâ€

- Optional but very helpful at scale
- Central queue that feeds queriers fairly
- Prevents one user/dashboard from starving everything else
- Helps you scale queriers without chaos

**Mental model:** _Fair queue + traffic shaping for query execution._

---

## âœ… Backend / control plane (index + lifecycle + automation)

These are the â€œkeeps Loki healthy and efficient over timeâ€ components.

### ğŸ“Œ 6. **Index Gateway (IG)** â€” â€œIndex cache and index access layerâ€

- Loki index lives in object storage (or shipped index)
- IG reduces expensive object-store calls by caching/serving index data
- Improves query latency + reduces cost (fewer GETs)

**Mental model:** _CDN for index metadata._

---

### ğŸ“Œ 7. **Compactor (C)** â€” â€œStorage janitor + organizerâ€

- Compacts index blocks to keep them efficient
- Applies retention/deletes (depends on your retention setup)
- Writes temporary files while compacting â‡’ often needs disk

**Mental model:** _Garbage collector + defragmenter for Loki storage._

---

### ğŸ“Œ 8. **Ruler (R)** â€” â€œAlerting engine for logsâ€

- Evaluates LogQL rules periodically (like Prometheus rules, but for logs)
- Sends alerts to Alertmanager or writes recording rules output
- Usually backend-ish (not on hot ingestion path)

**Mental model:** _Scheduled queries + alert triggers._

---

## ğŸ Quick cheat-sheet: â€œwho talks to object storage?â€

- **Write path**: Ingester flushes chunks â†’ object storage âœ…
- **Read path**: Querier reads chunks/index from object storage âœ…
- **Backend**: Compactor and Index Gateway interact heavily with object storage âœ…

---

## ğŸš€ Deployment modes (and why they exist)

Loki can run the same â€œ8 rolesâ€ in different shapes.

### âš™ï¸ Monolithic (Single Binary)

One process contains multiple roles inside it.

Your first diagram shows one **Loki binary** holding several components.

âœ… Pros:

- Easiest to run
- Great for dev/small setups

âŒ Cons:

- Scaling is blunt (scale everything together)
- No separation of read vs write load

---

### ğŸ¤¹ğŸ»â€â™€ï¸ Simple Scalable (SSD mode)

This is what youâ€™re using.

Instead of running 8 separate microservices, Loki bundles them into **3 targets**:

#### 1. **write target** (stateful)

Contains:

- **Distributor (D)**
- **Ingester (I)**

#### 2. **read target** (stateless logically)

Contains:

- **Query Frontend (FE)**
- **Querier (Q)**

#### 3. **backend target** (stateful)

Contains:

- **Compactor (C)**
- **Index Gateway (IG)**
- **Query Scheduler (QS)**
- **Ruler (R)** (often here)

âœ… Pros:

- You can scale **write** independently from **read**
- Much simpler than full microservices
- â€œProduction-ready sweet spotâ€ for most teams

âŒ Cons:

- Still bundled: you canâ€™t scale querier without also scaling FE (inside the read target)
- Less granular than full distributed microservices

Your 2nd diagram is literally: **3 write pods + 2 read pods + 1 backend pod**, all talking to object storage.

---

### ğŸ¦  Microservices / Distributed mode

Each component runs as its own Deployment/StatefulSet.

- Distributor separate
- Ingester separate
- FE separate
- Querier separate
- IG separate
- Compactor separate
- QS separate
- Ruler separate

âœ… Pros:

- Maximum scaling control and efficiency
- Best for very large clusters

âŒ Cons:

- More moving parts, more ops complexity

---

## ğŸ¯ Mapping table: components â†’ category â†’ SSD target â†’ microservices

| Component            | Category                    | SSD target  | Stateful?                                       |
| -------------------- | --------------------------- | ----------- | ----------------------------------------------- |
| Distributor (D)      | Write                       | **write**   | No (but part of write set)                      |
| Ingester (I)         | Write                       | **write**   | **Yes** (needs persistence/WAL)                 |
| Query Frontend (FE)  | Read                        | **read**    | No                                              |
| Querier (Q)          | Read                        | **read**    | No                                              |
| Query Scheduler (QS) | Backend (query control)     | **backend** | No (usually)                                    |
| Index Gateway (IG)   | Backend (index/cache)       | **backend** | Often yes-ish (benefits from disk cache)        |
| Compactor (C)        | Backend (storage lifecycle) | **backend** | Often yes (working dir)                         |
| Ruler (R)            | Backend (automation)        | **backend** | Usually stateless (but depends on rule storage) |

---

## ğŸ’¡ â€œWhich component goes whereâ€ in your SSD Helm install

If youâ€™re using Simple Scalable, you mainly tune:

- `write.replicas` (D + I together)
- `read.replicas` (FE + Q together)
- `backend.replicas` (C + IG + QS + R together)

Thatâ€™s why in SSD you size by **target**, not by the individual component count.
