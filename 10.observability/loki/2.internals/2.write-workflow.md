# ğŸŸ¦ Loki Write Workflow

> From Alloy âœ Loki âœ Azure Blob (with Indexing)

## ğŸ¯ Goal of the write path

Turn a firehose of log lines into durable, queryable data:

- **Durable** (doesnâ€™t disappear when pods restart)
- **Queryable** (fast â€œfind logs by labels/timeâ€)
- **Cheap** (object storage is the long-term system of record)

Loki does that by storing **two main things** in object storage:

1. ğŸ§Š **Chunks** (compressed log content)
2. ğŸ—‚ï¸ **Index** (TSDB blocks / shipped index metadata that maps labels+time â†’ chunks)

---

## ğŸ§© The write-path components (who does what)

## ğŸ› ï¸ 0. Grafana Alloy (client/agent)

- Collects logs from files/journald/k8s
- Adds labels (job, namespace, pod, app, clusterâ€¦)
- Batches logs and sends them to Loki using:
  - `POST /loki/api/v1/push`
  - usually **snappy-compressed protobuf**

**Alloyâ€™s job:** _ship logs efficiently and consistently labeled._

---

## ğŸšª 1. Gateway / Ingress (Nginx in Helm)

- Receives Alloy traffic
- Routes the request to the correct Loki target:
  - Write API calls â†’ **write**
  - Query API calls â†’ **read**

**Gatewayâ€™s job:** _traffic routing + a stable endpoint._

---

## ğŸ§­ 2. `Distributor (D)` â€” â€œwrite router + replicationâ€

First Loki component in write path.

### What it does

- âœ… Validates the request
  - tenant / auth (if enabled)
  - label rules, max label count/length
  - rate limits, ingestion limits

- âœ… Computes a **stream hash**
  - A â€œstreamâ€ is `{labels}`; each unique label-set is a stream

- âœ… Uses the **ring** to choose ingesters
- âœ… Replicates to multiple ingesters (replication factor)

**Distributorâ€™s job:** _decide â€œwhich ingesters should own this streamâ€ and ensure redundancy._

---

## ğŸ—ºï¸ 3. Ring / KV (memberlist, etcd, consulâ€¦)

Not a â€œworker componentâ€ but essential for routing.

### What it does

- Tracks:
  - which ingesters are alive
  - token ranges (shards)

- Distributor queries it to pick ingesters for a stream.

**Ringâ€™s job:** _consistent sharding + membership._

---

## ğŸ§± 4. `Ingester (I)` â€” â€œbuffer + WAL + flush + TSDB index creatorâ€

This is the heart of the write path.

### A. In-memory chunk building (hot buffer)

- Appends log entries to **in-memory chunks**
- Chunk is per stream; it grows until itâ€™s:
  - too big, or
  - too old, or
  - idle long enough

**Ingester memory = the â€œhot areaâ€ of Loki.**

---

### B. ğŸ’¾ WAL (Write-Ahead Log) on disk (recommended in prod)

If enabled, ingester writes incoming data to WAL:

- Stored on the ingester PVC
- Ensures that if the pod/node crashes before flush:
  - ingester can restart
  - replay WAL
  - rebuild chunks
  - avoid losing recent logs

**WAL is durability for the â€œbefore flushâ€ window.**

---

### C. ğŸ§Š Chunk flush to Azure Blob (persistent log content)

When flush triggers occur, the ingester:

- compresses the chunk
- uploads it as an object to Azure Blob

This is where the actual log payload becomes durable long-term.

---

### D. ğŸ—‚ï¸ TSDB index block creation (THIS is your missing piece âœ…)

**TSDB indexing is created by the Ingester.**

When ingester flushes (or during periodic block cutting), it also:

- writes index data into **local TSDB structures**
- produces **TSDB blocks** (index blocks)
- then ships those blocks to object storage

So indexing is _not_ a separate microservice. It is a storage engine behavior executed in the ingester.

**Ingester job includes:**
âœ… â€œCreate TSDB index blocksâ€ + âœ… â€œShip them to object storageâ€.

---

## â˜ï¸ 5. Azure Blob Storage â€” â€œsystem of recordâ€

Blob becomes the source of truth:

- ğŸ§Š chunk objects: the compressed logs
- ğŸ—‚ï¸ TSDB index blocks: metadata for lookup
- ğŸ§¹ retention/compaction markers: produced later (backend)

---

## ğŸ§¹ 6. `Compactor (C)` â€” not on hot write path, but crucial later

Compactor runs in **backend** target and:

- compacts TSDB blocks (merge/optimize)
- applies retention/deletes (depending on your retention mode)

It doesnâ€™t handle the initial ingestion, but it keeps storage efficient and enforces lifecycle.

---

## ğŸ”¥ End-to-end write sequence diagram (with indexing) ğŸ˜ˆğŸ§ŠğŸ—‚ï¸ğŸ’¾â˜ï¸

```mermaid
sequenceDiagram
  autonumber
  participant A as ğŸ› ï¸ Alloy
  participant GW as ğŸšª Gateway/Ingress
  participant D as ğŸ§­ Distributor (WRITE)
  participant R as ğŸ—ºï¸ Ring/KV (memberlist)
  participant I as ğŸ§± Ingester (WRITE, Stateful)
  participant WAL as ğŸ’¾ WAL PVC
  participant TSDB as ğŸ—‚ï¸ TSDB Local Block Dir (PVC/emptyDir)
  participant AZ as â˜ï¸ Azure Blob Storage
  participant C as ğŸ§¹ Compactor (BACKEND)

  Note over A: ğŸ§¾ Build batch: labels + timestamp + line<br/>ğŸ“¦ Compress (snappy protobuf)
  A->>GW: ğŸ“¤ POST /loki/api/v1/push
  GW->>D: ğŸ”€ Route to WRITE target

  Note over D: âœ… Validate + enforce limits<br/>ğŸ§© Hash stream labels
  D->>R: ğŸ—ºï¸ Lookup ingesters for this stream (ring)
  R-->>D: ğŸ¯ Selected ingesters (replication set)

  par Replication (RF)
    D->>I: ğŸ“¦ Push stream batch âœ Ingester#1
    D->>I: ğŸ“¦ Push stream batch âœ Ingester#2
    D->>I: ğŸ“¦ Push stream batch âœ Ingester#3
  end

  Note over I: ğŸ§  Append to in-memory chunks<br/>ğŸ—‚ï¸ Track metadata needed for index

  alt WAL enabled âœ…
    I->>WAL: ğŸ“ WAL append (durability)
    WAL-->>I: âœ… ack
  else WAL disabled âŒ
    Note over I: âš ï¸ Crash before flush can lose recent data
  end

  I-->>D: âœ… ACK (accepted)
  D-->>GW: âœ… 204 No Content
  GW-->>A: âœ… Success

  Note over I: â±ï¸ Flush trigger:<br/>â€¢ chunk idle / size / age<br/>Now persist BOTH: ğŸ§Šchunks + ğŸ—‚ï¸index

  I->>AZ: ğŸ§Š Upload chunk object(s) (compressed log data)
  Note over AZ: ğŸ§Š Stored: CHUNKS<br/>the real log payload

  I->>TSDB: ğŸ—‚ï¸ Cut/Write TSDB index block locally
  TSDB-->>I: âœ… Block ready

  I->>AZ: ğŸ—‚ï¸ Ship TSDB block to Blob (index shipper)
  Note over AZ: ğŸ—‚ï¸ Stored: TSDB INDEX BLOCKS<br/>labels+time âœ chunk references

  Note over C: ğŸ§¹ Later (async): compaction + retention
  C->>AZ: ğŸ§¹ Compact index blocks / apply retention
```

---

## ğŸ§¾ What is stored where (super clear)

### In Azure Blob (durable)

âœ… **Chunks** (compressed log data)
âœ… **TSDB Index Blocks** (metadata for fast lookup)
âœ… **Compactor metadata** (retention/compaction markers)

### On Write PVC (stateful ingester)

âœ… WAL files (if enabled)
âœ… sometimes local TSDB temp/index directories (depends on config + chart)

### In memory (ingester)

âœ… active chunks (recent logs not flushed yet)
âœ… stream metadata

---

## ğŸ›ï¸ The main knobs that control write behavior

### 1) Replication factor (durability vs cost)

- How many ingesters receive the same write
- More replicas = safer, more resource use

### 2) WAL enabled (recommended prod)

- `enabled: true` improves crash safety

### 3) Chunk flush behavior (frequency of blob writes)

Chunk idle/age/size thresholds decide:

- how often blobs get created
- how much WAL space can accumulate before flush

### 4) TSDB block shipping

TSDB blocks are created + shipped as part of ingester write lifecycle.
(Compactor later optimizes.)

---

## âœ… Final mental model (easy)

- **Distributor**: validates + sharding + replication
- **Ingester**: buffers logs, WAL for safety, **flushes chunks**, **creates TSDB blocks**, ships both to Blob
- **Azure Blob**: durable store for chunks + index
- **Compactor**: later cleanup + retention enforcement
