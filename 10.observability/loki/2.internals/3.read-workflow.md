# ğŸŸ© Loki Read Workflow

> From Grafana Query âœ Loki âœ Azure Blob (with Indexing + Caching)

You asked for the **same quality as the write topic**, but for the **read path**. Here it is: every component, when index is used, who fetches chunks, what gets cached, and where Azure Blob fits.

---

## ğŸ¯ Goal of the read path

Turn a LogQL query like:

> â€œShow me logs for `{app="api", env="prod"}` over last 6hâ€

â€¦into results fast and safely, without melting the cluster.

To do that Loki must:

1. Use the **index** to find which chunks might match (cheap-ish metadata)
2. Fetch and filter **chunks** from storage (expensive-ish data)
3. Return merged/ordered results

---

## ğŸ§© Read-path components (who does what)

## ğŸ§‘â€ğŸ’» 0. Client (Grafana / curl / API consumer)

- Sends LogQL queries to Loki:
  - instant: `/loki/api/v1/query`
  - range: `/loki/api/v1/query_range`

- Often repeats similar queries (dashboards) â†’ caching matters

---

## ğŸšª 1. Gateway / Ingress (Nginx in Helm)

- Routes query APIs to the **READ target** in SSD mode

---

## ğŸ§  2. Query Frontend (FE) â€” â€œquery brains + splitter + cacheâ€

This is the **first Loki read component**.

### What it does

- âœ… Parses and validates the query
- âœ… Splits large time ranges into smaller sub-queries (sharding)
- âœ… Retries failed sub-queries
- âœ… Enforces fairness / limits
- âœ… **Result cache** (very important for dashboards)

**Mental model:** _API layer for queries + â€œmake this query run fast and safeâ€._

---

## ğŸš¦ 3. Query Scheduler (QS) â€” â€œcentral queue for queriersâ€

Optional but strongly recommended for production stability.

### What it does

- Receives sub-queries from FE
- Queues them fairly (multi-tenant fairness)
- Hands them to queriers when theyâ€™re free

**Mental model:** _Traffic control for queriers._

---

## ğŸ’ª 4. Querier (Q) â€” â€œthe worker that actually runs the queryâ€

The querier does the real work:

### What it does (core)

1. Uses the **index** to get candidate chunk references for label/time
2. Fetches candidate **chunks**
3. Decompresses + filters lines by LogQL
4. Aggregates/merges results

**Mental model:** _CPU that reads index + chunks and produces answers._

---

## ğŸ—‚ï¸ 5. Index Gateway (IG) â€” â€œindex access acceleratorâ€

Index lives in object storage (TSDB blocks shipped there).

Querier needs index data to map:

> labels+time â†’ chunk refs

Index Gateway helps by:

- caching/serving index data to reduce expensive object-store reads
- improving query latency and lowering object storage GETs

**Mental model:** _CDN for index metadata._

---

## ğŸ§± 6. Ingester (I) â€” â€œrecent data that might not be flushed yetâ€

Even though read is â€œstatelessâ€, Loki still needs to include the newest logs that may still be in ingesters (not yet flushed to Blob).

So queriers often query:

- **Ingester** for â€œrecent windowâ€
- **Object storage** for older data

**Mental model:** _Hot cache for the most recent logs._

---

## â˜ï¸ 7. Azure Blob Storage â€” â€œsystem of recordâ€

Querier reads from Blob:

- ğŸ—‚ï¸ TSDB index blocks (or index metadata)
- ğŸ§Š chunk objects

---

## ğŸ”¥ End-to-end READ sequence diagram (with indexing + caching) ğŸŸ©ğŸ—‚ï¸ğŸ§Šâš¡

```mermaid id="3u4h5s"
sequenceDiagram
  autonumber
  participant U as ğŸ§‘â€ğŸ’» User/Grafana
  participant GW as ğŸšª Gateway/Ingress
  participant FE as ğŸ§  Query-Frontend (READ target)
  participant RC as ğŸ§Š Results Cache (memcached/embedded)
  participant QS as ğŸš¦ Query-Scheduler (BACKEND target)
  participant Q as ğŸ’ª Querier (READ target)
  participant IG as ğŸ—‚ï¸ Index-Gateway (BACKEND target)
  participant I as ğŸ§± Ingester (WRITE target)
  participant AZ as â˜ï¸ Azure Blob Storage

  Note over U: ğŸ” Sends LogQL query_range<br/>with labels + time window
  U->>GW: ğŸ“¤ GET /loki/api/v1/query_range
  GW->>FE: ğŸ”€ Route to READ target

  Note over FE: âœ… Validate + apply limits<br/>ğŸ§© Split query by time/shards<br/>ğŸ§Š Try result cache first
  FE->>RC: ğŸ§Š Lookup cached result (query+range hash)
  alt Cache hit âœ…
    RC-->>FE: âœ… Cached response
    FE-->>GW: ğŸ“¦ Return results fast
    GW-->>U: âœ… 200 OK (cached)
  else Cache miss âŒ
    RC-->>FE: âŒ miss

    Note over FE: ğŸš¦ Enqueue sub-queries (parallel work)
    FE->>QS: ğŸ“¬ Enqueue sub-queries (shards)
    QS-->>FE: âœ… Accepted

    loop For each sub-query shard
      QS->>Q: ğŸ§¾ Dispatch work item to a querier

      Note over Q: ğŸ—‚ï¸ Step 1: Find candidate chunks using INDEX
      Q->>IG: ğŸ—‚ï¸ Request index lookup (labels+time)
      IG->>AZ: â˜ï¸ Read TSDB index blocks (if not cached)
      AZ-->>IG: ğŸ—‚ï¸ Index data
      IG-->>Q: ğŸ¯ Candidate chunk refs (IDs, time ranges)

      Note over Q: ğŸ§± Step 2: Include newest data (not flushed yet)
      Q->>I: ğŸ§± Query ingesters for recent window
      I-->>Q: ğŸ§¾ Recent matching log entries / chunk data

      Note over Q: ğŸ§Š Step 3: Fetch chunks from object storage
      Q->>AZ: ğŸ§Š GET chunk objects for candidate refs
      AZ-->>Q: ğŸ§Š Compressed chunks

      Note over Q: ğŸ§  Step 4: Execute LogQL filtering + aggregation
      Q-->>QS: âœ… Sub-result (partial)
    end

    Note over FE: ğŸ§© Merge partial results + order + finalize
    QS-->>FE: ğŸ“¦ All shard results ready
    FE->>RC: ğŸ§Š Store result in cache (TTL)
    FE-->>GW: âœ… 200 OK results
    GW-->>U: âœ… Response
  end
```

---

## ğŸ—‚ï¸ Where indexing fits in the read workflow (the key steps)

### âœ… Step A â€” â€œIndex lookupâ€

- **Querier** needs to find **which chunks to read**
- It uses index data (TSDB blocks) to map:
  - `{labels}` + `[time range]` â†’ chunk references

- **Index Gateway** helps serve/cache that index data
- Index ultimately lives in **Azure Blob**

So:

> **Index is read before chunks**, otherwise querier would have to scan everything (impossible).

### âœ… Step B â€” â€œChunk fetchâ€

Once the querier has chunk refs:

- it GETs chunks from Blob
- decompresses and filters by LogQL

---

## ğŸ§Š Caching in the read path (what, why, where)

### 1. ğŸ§Š Results Cache (Query Frontend)

- Caches the final response for repeated dashboard queries
- Huge ROI for Grafana dashboards
- Control with:
  - enable/disable
  - TTL
  - embedded vs memcached

**This reduces repeated query execution.**

---

### 2. ğŸ—‚ï¸ Index cache (Index Gateway)

- Caches index blocks/segments to avoid repeated Blob reads
- Biggest win for â€œsame labels/time patternsâ€ queries

**This reduces object store GETs and speeds up chunk discovery.**

---

### 3. ğŸ§Š Chunk cache (optional / external)

- Caches chunk objects so repeated queries donâ€™t refetch blobs
- Usually external (memcached) if you want shared caching

**This reduces chunk GETs + decompression work.**

---

## ğŸ›ï¸ The knobs that control the read path (practical)

### Query splitting / performance

- Query-frontend splitting window and parallelism
- Max outstanding requests per tenant/user
- Query timeouts

### Scheduler fairness

- Enable query-scheduler
- Queue depth, fairness limits

### Caches

- results cache TTL (frontend)
- index caching behavior (index-gateway)
- chunk cache TTL and size

### â€œRead fresh dataâ€ window

- Controls how much queriers ask ingesters vs only object storage
- Bigger window = fresher results but more ingester load

---

## âœ… Quick mental model (super easy)

- **Frontend** = splits + caches + merges
- **Scheduler** = fair queue
- **Querier** = reads **index â†’ chunks** + filters
- **Index Gateway** = speeds up index reads
- **Ingester** = serves the â€œnot yet flushedâ€ recent logs
- **Azure Blob** = source of truth for index+chunks
