# Ingester needs RWO PV

## 1) Does a StatefulSet _have to_ have its own PV?

**Kubernetes doesn’t force it.** You _can_ run a StatefulSet with:

- no volumes at all (purely for stable pod names / ordered rollout), or
- an `emptyDir` (ephemeral), or
- even a shared RWX volume (rare and usually not ideal).

**But** the _main reason_ people choose StatefulSet is:

- **stable identity** (`pod-0`, `pod-1`, …)
- **stable per-pod storage** via `volumeClaimTemplates` (PVC per replica)

So: **not mandatory technically, but typical/expected**.

---

## 2) For Loki ingester StatefulSet: PV per instance or one PV?

### ✅ Best practice: **PV per ingester replica**

For Loki **ingester**, you normally want **one PVC per pod**.

Why:

- Each ingester writes a local **WAL (Write-Ahead Log)** and keeps **in-memory chunks** + on-disk state (depending on config).
- When an ingester restarts, the WAL helps recover recent data and avoid loss.
- Sharing one disk across multiple ingesters is usually wrong: file locking, corruption risks, noisy-neighbor IO, and operational pain.

In Helm charts you’ll see something like:

- `ingester` as **StatefulSet**
- `volumeClaimTemplates` → **each pod gets its own PVC**
  So if you have 3 replicas: **3 PVCs**.

---

## 3) “If each ingester has its own disk… how do they share / replicate data?”

This is the key Loki idea:

### Loki does **NOT** replicate by sharing disk.

It replicates **by writing the same stream/chunks to multiple ingesters** (network replication), controlled by:

- **ring membership** (consistent hashing ring)
- **replication_factor** (usually 2 or 3)

Flow:

1. A client (Promtail/Alloy) pushes logs to Loki **distributor**
2. Distributor hashes the stream labels → picks the target ingesters in the ring
3. Distributor sends the same data to **N ingesters** (`replication_factor = N`)
4. Each ingester stores that data in **its own memory/WAL**
5. Later the ingesters flush chunks to the long-term store (object storage like S3/Azure Blob) and index components handle query paths depending on mode.

So the “sharing” is done at **ingestion time** via **replication**, not via a shared PV.

---

## 4) When can ingester run without PV?

You _can_ run without PV (ephemeral), but you accept tradeoffs:

### Ephemeral ingester (no PV)

- On restart, WAL is gone → higher risk of losing the most recent unflushed logs.
- More pressure to flush quickly.
- In some environments it’s acceptable (dev / low criticality).

### With PV (recommended for production)

- Better durability during restarts, node drains, rolling updates.
- WAL can replay after restart.

---

## 5) So should you create “one PV” for all ingesters?

Only if you’re using a storage backend that supports **RWX** and you _really_ know what you’re doing — and even then it’s usually a bad fit for ingesters.

For ingester, the correct pattern is:

- **replicas = N**
- **PVC per replica**
- replication handled by Loki ring, not shared filesystem

---

## 6) Quick practical answer for your case

- **Yes:** if `ingester` is StatefulSet and you care about durability, configure **PVC per ingester pod**.
- **No:** don’t try “one PVC for all ingesters” to make them share data. That’s not Loki’s replication model.

---

If you paste your Loki Helm values for:

- `loki.ingester`
- `loki.commonConfig.replication_factor` (or wherever your chart puts it)
- storage config (Azure Blob)

…I can tell you exactly whether your setup is “safe” for restarts and how many PVCs you’ll get.

---

## Why RWO is the right choice

- Each ingester pod should have **its own private disk** (WAL + local state).
- That disk should be mounted by **one node at a time** → exactly what **RWO** enforces.
- StatefulSet + `volumeClaimTemplates` is designed around **PVC-per-pod**, typically **RWO**.

## Why not RWX (ReadWriteMany)?

RWX means multiple pods/nodes can mount the same volume concurrently. For ingesters that’s usually a bad idea:

- WAL files are **not meant for concurrent writers**
- higher risk of **corruption / locking issues**
- performance tends to be worse (NFS/SMB style backends)
- it does **not** help replication (Loki replicates over the network via ring, not shared disk)

So: **RWX does not give you “shared data” in the Loki sense**.

---

## What access mode to pick by component (quick mapping)

- **Ingester (StatefulSet)** ✅ **RWO**
- **Compactor (often single replica, stateful-ish)** usually ✅ **RWO** (needs local working dir; some charts can run without PVC but PVC is common)
- **Ruler (if using local rule files)** depends; often ConfigMap/object-store; PVC usually not required
- **Querier/Query-frontend/Distributor/Gateway** usually **no PVC** (stateless)

---

## StorageClass matters more than RWX vs RWO

Pick a StorageClass that matches your environment:

- Cloud block disks (Azure Disk, EBS, etc.) → perfect for **RWO**
- On-prem (Ceph RBD, Longhorn, OpenEBS, etc.) → also good for **RWO**
- NFS RWX → not ideal for ingester WAL unless you _must_ (and even then, I’d avoid)

---

## Recommended Loki ingester PVC settings (rule of thumb)

- AccessMode: **RWO**
- Size: start **10–50Gi** per ingester (depends on ingestion + retention + flush)
- Volume type: **fast block storage** (SSD if possible)

---
