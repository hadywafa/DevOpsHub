# ğŸ“ Tempo architecture

> **Grafana Tempo receives spans, groups them by TraceID, stores them cheaply in object storage, and lets you fetch/search traces fast (by TraceID or TraceQL).**

Pipeline (typical):
**App (OTel SDK/auto-instr)** â†’ **Alloy / OTel Collector** â†’ **Tempo** â†’ **Grafana UI**

---

<div align="center" style="background-color:#E6E6E6; border-radius: 10px; border: 2px solid">
    <img src="image/1770323636755.png" alt="SLI, SLO, SLA & Error Budgets" style="width: 90%;">
</div>

---

## âš”ï¸ Two deployment styles: Monolithic vs Distributed

## ğŸ“¦ Monolithic mode

One Tempo process contains most roles together (internally):

- ingesting spans
- writing blocks to storage
- reading/searching traces

Itâ€™s simpler, fewer k8s objects, great to start.

## ğŸ¤¹â€â™‚ï¸ Distributed mode

Tempo splits responsibilities into **separate components** (pods):
Distributor, Ingester, Query-Frontend, Querier, Compactor, etc.
This is for high throughput + horizontal scaling.

> Even in monolithic, it helps to understand the â€œlogical componentsâ€ because the behavior is the same.

---

## ğŸ’¬ Mental model: â€œMail sorting centerâ€

Imagine spans are **letters** arriving from many services.

Tempo must:

1. **Receive** letters (spans)
2. **Group** letters by **TraceID** (same â€œcase fileâ€)
3. **Pack** them into **blocks** and store in **object storage**
4. Later, when you ask â€œgive me case file Xâ€, it must **find** and **return** all letters for that TraceID fast

Now map that to Tempo components.

---

## ğŸšª 1. Distributor (the â€œreceiving desk + routerâ€)

### What it does

- Accepts spans in multiple formats (OTLP, Jaeger, Zipkinâ€¦)
- **Routes spans to the right Ingester(s)** by hashing **TraceID**
- Uses a **consistent hash ring** so routing stays stable as pods scale

### Why hashing by TraceID matters

All spans for the same trace should land on the same ingester set (or predictable subset), so Tempo can build blocks efficiently.

### Best practice

- **OTLP (protobuf) ingestion is fastest**, so Alloy typically uses OTLP exporter to Tempo.

---

## ğŸ—ï¸ 2.Ingestr (the â€œbuilder + packer + uploaderâ€

### What it does

1. Receives spans from Distributor
2. Keeps â€œrecent tracesâ€ in memory (and/or WAL depending config)
3. Builds **blocks** over a time/size window
4. Writes blocks to **object storage**
5. Creates **indexes + bloom filters** so queries are fast later

### What is a â€œblockâ€?

A block is like a **packed box** containing many traces for a time window.

In object storage youâ€™ll see something like:

```ini
<bucket>/<tenant>/<blockID>/
  meta.json
  index
  data
  bloom_0 ... bloom_n
```

### Why parquet/index/bloom?

- **Parquet** layout helps with efficient columnar reads
- **Index** helps locate candidate traces
- **Bloom filters** help quickly skip blocks that definitely donâ€™t contain a trace/span youâ€™re looking for

> In monolithic mode, the same process still â€œacts as an ingesterâ€ internally.

---

## ğŸ›ï¸ 3. Query Frontend (the â€œsmart receptionist for queriesâ€)

Grafana usually talks to **Query Frontend** (in distributed mode).

### What it does

- Receives requests like:
  - **Get trace by TraceID**
  - **Search using TraceQL**

- **Shards** the search space into many sub-queries
- Sends shards to multiple Queriers in parallel
- Collects results and returns a combined response

### Why it exists

Search can be expensive. Sharding + parallelism is how Tempo stays responsive.

Example endpoint conceptually:

- `GET /api/traces/<traceID>` (what Grafana uses)

---

## ğŸ” 4. Querier (the â€œdetective that reads storageâ€)

### What it does

When you ask for a trace (or TraceQL search):

- Looks in **Ingesters** first for **recent not-yet-flushed** data
- Looks in **object storage blocks** for historical data
- Uses bloom/index to avoid scanning everything
- Returns spans back to Query Frontend

Querier has its own HTTP endpoint, but:

> You typically query **Query Frontend**, not Queriers directly.

---

## ğŸ§¹ 5. Compactor (the â€œwarehouse optimizer + garbage collectorâ€)

### What it does

- Compacts blocks â†’ fewer bigger blocks (less fragmentation, faster queries)
- Deduplicates data (advanced topic)
- Enforces **retention** (deletes old blocks after N days)

### Key point for you (retention question)

Retention is _not_ â€œonly a setting in the ingesterâ€.
Retention is **actually enforced by compaction/retention jobs** that delete old data in object storage.

---

## ğŸ—“ï¸ 6. Backend Scheduler + Worker ğŸ§‘â€ğŸ­ (newer compaction architecture)

Tempo is moving from â€œCompactor does all jobsâ€ to:

- **Scheduler**: decides what jobs must run and tracks them
- **Workers**: execute jobs (compaction, retention, and more later)

Important operational rules (from your reference):

- Only one scheduler active
- When enabling workers, scale compactor to 0 to avoid conflicts
- Workers maintain blocklist coordination via the ring

(If youâ€™re on monolithic, you might not touch this yet.)

---

## ğŸ“ˆ 7. Metrics Generator (optional) (turn traces into Prometheus metrics)

This is the â€œbridgeâ€ between tracing and metrics.

### What it can generate

- **Span metrics** (a.k.a. `traces_spanmetrics_*`)
  - request rate (RPS)
  - error rate
  - latency histograms

Example families youâ€™ll see in Prometheus:

- `traces_spanmetrics_calls_total`
- `traces_spanmetrics_latency_sum`
- `traces_spanmetrics_latency_bucket` âœ… (your earlier question)

### Why itâ€™s useful

You can build RED dashboards (Rate, Errors, Duration) from traces even if apps donâ€™t expose good metrics.

---

## ğŸª£ Object Storage (the â€œsource of truthâ€)

Tempo stores trace blocks in object storage:

- S3
- GCS
- **Azure Blob** (your case)

Why object storage?

- Cheap
- Durable
- Scales well
  Tempo keeps minimal local state compared to systems like Elasticsearch-based tracing.

---

## ğŸ§  Three critical â€œTempo truthsâ€ (must remember)

## 1. There is no â€œend of traceâ€

A trace can receive spans later (retries, async processing, delayed spans).

## 2. Querying TraceID returns â€œwhat Tempo currently hasâ€

You get all spans stored/ingested _so far_ for that TraceID.

## 3. TraceQL search is parallel and not guaranteed chronological

Frontends return results as they arrive; they may stop early when reaching a limit.

---

<div align="center" style="background-color: #2b3436ff; border-radius: 12px; padding: 10px; border: 2px solid #3e4b4d;">

```mermaid
flowchart LR
    %% STYLE DEFINITIONS
    classDef rect fill:#1f2628,stroke:#4a5a5c,stroke-width:1px,color:#e8e8e8;
    classDef hex fill:#243033,stroke:#4a5a5c,stroke-width:1px,color:#e8e8e8;
    classDef proc fill:#1c2b2e,stroke:#4a5a5c,stroke-width:1px,color:#e8e8e8;
    classDef animate stroke-dasharray: 9,5,stroke-dashoffset: 900,animation: dash 25s linear infinite;

    %% NODES
    A@{ shape: rect, label: "ğŸ“± Apps / Services\nOTel SDK / Autoâ€‘Instrumentation" }
    B@{ shape: stadium, label: "ğŸ§© Alloy / OTel Collector" }

    D@{ shape: hex, label: "ğŸ“¤ Distributor" }
    I@{ shape: procs, label: "ğŸ“¦ Ingester" }

    S@{ shape: cyl, label: "ğŸ—„ï¸ Object Storage\nAzure Blob" }

    QF@{ shape: stadium, label: "ğŸ” Query Frontend" }
    QR@{ shape: procs, label: "ğŸ§  Queriers" }

    G@{ shape: stadium, label: "ğŸ“Š Grafana UI" }

    MG@{ shape: proc, label: "ğŸ“ˆ Metrics Generator (optional)" }
    P@{ shape: stadium, label: "ğŸ“¡ Prometheus / Mimir" }

    %% FLOWS
    A e1@--> B
    B e2@--> D
    D e3@--> I
    I e4@--> S

    G e5@--> QF
    QF e6@--> QR
    QR e7@--> S
    QR e8@--> I

    I e9@--> MG
    MG e10@--> P

    %% ANIMATION
    class e1,e2,e3,e4,e5,e6,e7,e8,e9,e10 animate
```

</div>
