# Existing pipeline

```yaml
name: Build and Push Docker Images

on:
  push:
    branches:
      - main
      - dev
  workflow_dispatch:

jobs:
  build-and-push:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        include:
          - project: osTaxConsoleCore
            tag: tax-api
          - project: osDashboardApi
            tag: dashboard

    env:
      IMAGE_NAME: nabilmokhtar0/peppol-ae

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up .NET SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0.x"

      - name: Restore dependencies
        run: dotnet restore core/${{ matrix.project }}/${{ matrix.project }}.csproj

      - name: Build
        run: dotnet build core/${{ matrix.project }}/${{ matrix.project }}.csproj --no-restore --configuration Release

      - name: Publish
        run: dotnet publish core/${{ matrix.project }}/${{ matrix.project }}.csproj --no-build --configuration Release --output core/${{ matrix.project }}/publish_output

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build Docker image
        run: |
          docker build -t $IMAGE_NAME:${{ matrix.tag }} -f core/${{ matrix.project }}/DockerFile .

      - name: Push Docker image
        run: |
          docker push $IMAGE_NAME:${{ matrix.tag }}
```

```yaml
# docker-compose.build.yaml
services:
  console.api:
    image: ${CONTAINER_REGISTRY-}/${GITHUB_REPOSITORY}/console.api:${SHORT_SHA}-${build_tag}
    build:
      context: .
      dockerfile: core/osTaxConsoleCore/DockerFile

  dashboard.api:
    image: ${CONTAINER_REGISTRY-}/${GITHUB_REPOSITORY}/dashboard.api:${SHORT_SHA}-${build_tag}
    build:
      context: .
      dockerfile: core/osDashboardApi/DockerFile
```

---

## 1Ô∏è‚É£ What the existing pipeline actually does

### üîî Triggers

```yaml
on:
  push:
    tags:
      - "v*.*.*" # only semver tags like v1.2.3
  pull_request:
    workflow-dispatch:
    branches:
      - dev
      - alfa
    types: [closed]
```

- **Tag push** ‚Üí runs on `v*.*.*` (release build).
- **PR closed** into `dev` or `alfa` ‚Üí runs after PR is closed (likely used to promote to `dev`/`qa`).
- There is also a weird indentation of `workflow-dispatch` under `pull_request`, but GitHub will treat `workflow_dispatch` as a top-level key if fixed.

So this workflow **does NOT** run on normal pushes to `dev/main/alfa`, only:

- Tag pushes (`vX.Y.Z`)
- PR closed events (to dev/alfa)

---

### üß± Job 1: `build-and-push`

Key steps:

1. **Checkout code**

   ```yaml
   - uses: actions/checkout@v4
   ```

2. **Get short SHA**

   ```bash
   SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
   echo "SHORT_SHA=..." >> $GITHUB_ENV
   ```

   This becomes available as `${{ env.SHORT_SHA }}`.

3. **Build & push Docker images using `docker-compose.build.yml`**

   ```yaml
   - name: Build and push Docker images
     env:
       CONTAINER_REGISTRY: ${{ env.CONTAINER_REGISTRY }} # ghcr.io
       SHORT_SHA: ${{ env.SHORT_SHA }}
       GITHUB_REPOSITORY: ${{ github.repository }}
     run: |
       echo "${{ secrets.GITHUB_TOKEN }}" | docker login ${{ env.CONTAINER_REGISTRY }} -u ${{ github.actor }} --password-stdin
       if [[ "${{ github.ref }}" == refs/heads/alfa ]]; then
         export build_tag=dev
       elif [[ "${{ github.ref }}" == refs/tags/v*.*.* ]]; then
         export build_tag=qa    
       elif [[ "${{ github.ref }}" == refs/heads/main ]]; then
         export build_tag=latest
       fi
       docker compose --parallel 5 -f docker-compose.build.yml build
       docker compose --parallel 5 -f docker-compose.build.yml push 
       echo "image=${{ env.SHORT_SHA }}" >> "$GITHUB_OUTPUT"
       echo "build_tag=$build_tag" >> $GITHUB_OUTPUT
   ```

   What this means:

   - Logs in to **GHCR** with `GITHUB_TOKEN`.
   - Derives a **`build_tag`** based on the ref:

     - `refs/heads/alfa` ‚Üí `dev`
     - `refs/tags/v*.*.*` ‚Üí `qa`
     - `refs/heads/main` ‚Üí `latest`

   - Runs docker compose build/push using `docker-compose.build.yml`.
   - Exposes outputs:

     - `image` ‚Üí `SHORT_SHA`
     - `build_tag` ‚Üí `dev | qa | latest`

4. **docker-compose.build.yml**

   You shared:

   ```yaml
   services:
     console.api:
       image: ${CONTAINER_REGISTRY-}/${GITHUB_REPOSITORY}/console.api:${SHORT_SHA}-${build_tag}
       build:
         context: .
         dockerfile: core/osTaxConsoleCore/DockerFile

     dashboard.api:
       image: ${CONTAINER_REGISTRY-}/${GITHUB_REPOSITORY}/dashboard.api:${SHORT_SHA}-${build_tag}
       build:
         context: .
         dockerfile: core/osDashboardApi/DockerFile
   ```

   So for each service, the image ends up as:

   ```text
   ghcr.io/<org>/<repo>/console.api:<SHORT_SHA>-<build_tag>
   ghcr.io/<org>/<repo>/dashboard.api:<SHORT_SHA>-<build_tag>
   ```

---

### üß± Job 2: `update-helm-chart`

This job:

1. Checks out a **separate Helm repo**: `orchida-tax/helm-deployments`

2. Installs `yq`

3. Updates the appropriate `values-*.yaml` file(s):

   ```bash
   BUILD_TAG=${{ needs.build-and-push.outputs.build_tag }}
   IMAGE=${{ needs.build-and-push.outputs.image }}   # SHORT_SHA

   for service in $SERVICES_NAMES; do
     if [[ "$BUILD_TAG" == "latest" ]]; then
       yq -i ".services.${service}.image.tag = \"$IMAGE-$BUILD_TAG\"" values-staging.yaml
       yq -i ".services.${service}.image.tag = \"$IMAGE-$BUILD_TAG\"" values-prod.yaml
     else
       yq -i ".services.${service}.image.tag = \"$IMAGE-$BUILD_TAG\"" values-$BUILD_TAG.yaml
     fi
   done
   ```

   So Helm values get tags like:

   ```text
   <SHORT_SHA>-dev
   <SHORT_SHA>-qa
   <SHORT_SHA>-latest
   ```

4. Commits and pushes those changes back to `main` in the Helm repo.

So overall, the pipeline:

- Builds all backend Docker images with tag `<SHORT_SHA>-<env>`
- Pushes them to GHCR
- Updates Helm chart values for each environment so that deployments pick up the new images.

---

Gotcha, no worries at all ‚Äî this is a _really_ common confusion in monorepos + Helm land üòä

Let‚Äôs answer your question directly:

> **‚ÄúI want to know what images this pipeline does, and how `console.api` maps to more than one k8s resource.‚Äù**

---

## 1Ô∏è‚É£ What images does the old pipeline actually build?

From your `docker-compose.build.yml`:

```yaml
services:
  console.api:
    image: ${CONTAINER_REGISTRY-}/${GITHUB_REPOSITORY}/console.api:${SHORT_SHA}-${build_tag}
    build:
      context: .
      dockerfile: core/osTaxConsoleCore/DockerFile

  dashboard.api:
    image: ${CONTAINER_REGISTRY-}/${GITHUB_REPOSITORY}/dashboard.api:${SHORT_SHA}-${build_tag}
    build:
      context: .
      dockerfile: core/osDashboardApi/DockerFile
```

And from the workflow:

```yaml
env:
  CONTAINER_REGISTRY: ghcr.io
# ...
GITHUB_REPOSITORY: ${{ github.repository }} # e.g. orchida-tax/tax-api
```

So the pipeline builds **two images** from this repo:

1. **console.api image**

   Final name & tag:

   ```text
   ghcr.io/<GITHUB_REPOSITORY>/console.api:<SHORT_SHA>-<build_tag>
   ```

   For your repo `orchida-tax/tax-api`, example:

   ```text
   ghcr.io/orchida-tax/tax-api/console.api:f499b29-dev
   ```

2. **dashboard.api image**

   Final name & tag:

   ```text
   ghcr.io/<GITHUB_REPOSITORY>/dashboard.api:<SHORT_SHA>-<build_tag>
   ```

   Example:

   ```text
   ghcr.io/orchida-tax/tax-api/dashboard.api:f499b29-dev
   ```

üëâ **Important:**
Only **those two images** are built by this pipeline: `console.api` and `dashboard.api`.

All the magic with `pub_api`, `extractor_api`, `dashboard_api` in Helm is just **different deployments pointing to those same images.**

---

## 2Ô∏è‚É£ How does this map to your Helm chart?

From your `values-prod.yaml` snippet:

### a) `dashboard_api` service

```yaml
services:
  dashboard_api:
    image:
      repository: orchida-tax/tax-api/dashboard.api
      tag: "f499b29-dev"
```

This corresponds to image:

```text
ghcr.io/orchida-tax/tax-api/dashboard.api:f499b29-dev
```

‚úÖ That matches the **`dashboard.api` image** built by the pipeline.

---

### b) `pub_api` service

```yaml
pub_api:
  image:
    repository: orchida-tax/tax-api/console.api
    tag: "f499b29-dev"
```

This corresponds to:

```text
ghcr.io/orchida-tax/tax-api/console.api:f499b29-dev
```

‚úÖ That uses the **`console.api` image**.

---

### c) `extractor_api` service

```yaml
extractor_api:
  image:
    repository: orchida-tax/tax-api/console.api
    tag: "f499b29-dev"
```

Again, this is:

```text
ghcr.io/orchida-tax/tax-api/console.api:f499b29-dev
```

‚úÖ Also uses the **same `console.api` image**.

---

### üëâ So what‚Äôs the mapping?

- Image **`console.api`** is built once, then used by:

  - `services.pub_api`
  - `services.extractor_api`

- Image **`dashboard.api`** is built once, then used by:

  - `services.dashboard_api`

Kubernetes doesn‚Äôt care that it‚Äôs the same image;
Helm just creates **different Deployments** with different env/ports/ingress, all pointing to the same repo+tag.

So your mental model:

```text
console.api      ‚îÄ‚ñ∂  pub_api (deployment A)
                 ‚îî‚ñ∂  extractor_api (deployment B)

dashboard.api    ‚îÄ‚ñ∂  dashboard_api (deployment C)
```

---

## 3Ô∏è‚É£ What does `SERVICES_NAMES` in the workflow really control?

From the workflow:

```yaml
env:
  SERVICES_NAMES: "pub_api extractor_api dashboard_api"
```

And later in `update-helm-chart`:

```bash
for service in $SERVICES_NAMES; do
  if [[ "$BUILD_TAG" == "latest" ]]; then
    yq -i ".services.${service}.image.tag = \"$IMAGE-$BUILD_TAG\"" values-staging.yaml
    yq -i ".services.${service}.image.tag = \"$IMAGE-$BUILD_TAG\"" values-prod.yaml
  else
    yq -i ".services.${service}.image.tag = \"$IMAGE-$BUILD_TAG\"" values-$BUILD_TAG.yaml
  fi
done
```

This does **NOT** control which Docker images are built.
That‚Äôs controlled by `docker-compose.build.yml`.

Instead, `SERVICES_NAMES` controls **which Helm services get their `.image.tag` updated** to `SHORT_SHA-build_tag`.

Meaning:

- It will update:

  - `.services.pub_api.image.tag`
  - `.services.extractor_api.image.tag`
  - `.services.dashboard_api.image.tag`

to something like:

```text
f499b29-dev
```

But:

- `pub_api` + `extractor_api` both point to `console.api` repo.
- `dashboard_api` points to `dashboard.api` repo.

So when the workflow runs, it:

1. Builds:

   - `ghcr.io/.../console.api:SHORTSHA-build_tag`
   - `ghcr.io/.../dashboard.api:SHORTSHA-build_tag`

2. Updates **three Helm entries** to that same tag:

   - `pub_api.image.tag = SHORTSHA-build_tag` ‚Üí console.api image
   - `extractor_api.image.tag = SHORTSHA-build_tag` ‚Üí console.api image
   - `dashboard_api.image.tag = SHORTSHA-build_tag` ‚Üí dashboard.api image

That‚Äôs how **one image (`console.api`) powers multiple k8s resources**.

---

## 4Ô∏è‚É£ Other services in your values file

In your `values-prod.yaml` you also have:

- `dashboard` ‚Üí `orchida-tax/dashboard-front`
- `peppol_ap` ‚Üí `orchida-tax/ap-peppol`
- `validation_engine` ‚Üí `orchida-tax/validation-engine`
- `test_api` ‚Üí `orchida-tax/workflows`

Those images are **not built by this pipeline** (at least not by the snippet you shared).
They likely come from **other repos** or other pipelines.

---

## 5Ô∏è‚É£ If we refactor to matrix later‚Ä¶

When we move to a matrix-based build, we should define the **build matrix by image**, not by Helm service.

Matrix should contain:

```yaml
- console.api  -> core/osTaxConsoleCore/DockerFile
- dashboard.api -> core/osDashboardApi/DockerFile
```

And **keep**:

```yaml
SERVICES_NAMES="pub_api extractor_api dashboard_api"
```

for Helm, because those are the k8s/Helm _consumers_ of those images.

So:

- **Matrix = images to build**
- **SERVICES_NAMES = Helm services to update**

---

## 2Ô∏è‚É£ Bottlenecks, pitfalls, and ‚Äúouch‚Äù points

### ‚ùå 1. Ref handling is fragile / partially wrong

The logic:

```bash
if [[ "${{ github.ref }}" == refs/heads/alfa ]]; then ...
elif [[ "${{ github.ref }}" == refs/tags/v*.*.* ]]; then ...
elif [[ "${{ github.ref }}" == refs/heads/main ]]; then ...
fi
```

But the workflow **does not** trigger on:

- push to `main`
- push to `alfa`

It triggers on:

- `push` tags
- `pull_request` with `types: [closed]`

For `pull_request` events, `github.ref` = `refs/pull/<number>/merge` (or similar), **not** `refs/heads/<branch>`.

So:

- `refs/heads/alfa` case ‚Üí almost never hit.
- `refs/heads/main` case ‚Üí also never hit (no push on main trigger).
- Only reliable case is: `refs/tags/v*.*.*` ‚Üí `qa`.

Result:

- For PR closed ‚Üí `build_tag` might be **empty**, causing weird behavior.
- The mapping between branches and build_tag/env is unclear and fragile.

‚úÖ Better: use `github.ref_type` (`branch` vs `tag`), `github.ref_name`, and for PRs use `github.base_ref`.

---

### ‚ùå 2. No real monorepo awareness (always build everything)

- Uses `docker compose build` with all services in one shot.
- Even if **only `dashboard.api` changed**, it builds **all services**.

This is:

- Slower: more build time, more docker layers.
- Wasteful: unnecessary pushes to registry.
- Harder to debug individual service failures.

‚úÖ Better: use a **matrix per service** and optionally a **dynamic matrix** that only includes changed services.

---

### ‚ùå 3. Single job with compose makes logs & failures messy

- All services are built by `docker compose` in one step.
- If a build fails:

  - It‚Äôs harder to see **which service** failed.
  - Logs are mixed.

- You can‚Äôt see per-service status in the GitHub UI.

‚úÖ Better: one matrix job per service ‚Üí each has its own check and log stream.

---

### ‚ùå 4. Tagging strategy is too limited for modern workflows

Current tag pattern:

```text
<SHA7>-<build_tag>    # build_tag = dev|qa|latest
```

Problems:

- No full SHA tag.
- No branch-specific tags like `<branch>-<short_sha>` or `<branch>-latest`.
- No direct SemVer alias `<version>` for releases beyond the `qa` env concept.
- Helm is bound to this tag shape, making evolution harder.

You already asked for something richer like:

- `sha-<full_sha>`
- `<branch>-<short_sha>`
- `<branch>-latest`
- `<semver>` on tag push

which is not supported in the current design.

---

### ‚ùå 5. docker-compose file is doing CI work it doesn‚Äôt need to

`docker-compose.build.yml` is used only to:

- Define image names
- Specify Dockerfiles

For CI, this is:

- An extra indirection layer.
- Harder to control per-service behaviors (build args, tags, caching).
- Not leveraging **docker/build-push-action** + buildx + cache.

‚úÖ Better: build each service **directly** with `docker build` or `docker/build-push-action`, and keep docker-compose for **local dev**.

---

### ‚ùå 6. No build cache optimization

- Plain `docker compose build` ‚Üí no GitHub cache integration.
- For .NET builds, this wastes time (restore, publish, etc.) on every CI run.

‚úÖ Best practice: use `docker/build-push-action` with `cache-from/cache-to: type=gha` to reuse layers between runs.

---

## 3Ô∏è‚É£ Best practices for Docker builds in a .NET monorepo

Here‚Äôs the ‚Äúgood DevOps citizen‚Äù approach for your scenario:

1. **Use a matrix per service**

   ```yaml
   strategy:
     matrix:
       include:
         - service: console.api
           dockerfile: core/osTaxConsoleCore/DockerFile
         - service: dashboard.api
           dockerfile: core/osDashboardApi/DockerFile
   ```

   - Independent jobs per service ‚Üí better isolation & logs.
   - Easy to add/remove services.

2. **Generate robust `build_tag` based on ref type**

   - Use:

     - `github.ref_type` (`branch` or `tag`)
     - `github.ref_name` (branch name or tag name)
     - `github.event_name` (for PRs, use `github.base_ref`)

   - Clearly define mapping:

     - `dev` ‚Üí dev env
     - `alfa` ‚Üí qa env
     - `main` ‚Üí prod (`latest`)

3. **Tag images with multiple tags**
   For each image:

   - `sha-<full_sha>`
   - `<branch>-<short_sha>`
   - `<branch>-latest`
   - On tag push:

     - `vX.Y.Z` and optionally `X.Y.Z`

   - Plus keep your Helm tag: `<SHORT_SHA>-<build_tag>` so you don‚Äôt break existing infra.

4. **Separate jobs**

   - `prepare-meta` job:

     - calculates `SHORT_SHA`, `FULL_SHA`, `BRANCH_SAFE`, `BUILD_TAG`, `VERSION_TAG`.

   - `build-and-push` job (matrix):

     - builds & pushes images for each service using metadata.

   - `update-helm-chart` job:

     - updates Helm repo using `SHORT_SHA` + `BUILD_TAG`.

5. **Use docker/build-push-action + buildx**

   - Built-in support for:

     - GHCR login
     - caching
     - multiple tags per build

---

## 4Ô∏è‚É£ Final recommended solution (for your exact case)

### üåê High-level design

**Jobs:**

1. `prepare-meta`

   - Compute:

     - `SHORT_SHA` (7 chars)
     - `FULL_SHA`
     - `BUILD_TAG` ‚Üí `dev | qa | latest` based on branch/tag
     - `BRANCH_SAFE` (sanitized branch name)
     - `VERSION_TAG` if this is a semver tag

2. `build-and-push` (matrix per service)

   - For each service:

     - Build Docker image from its Dockerfile.
     - Push with **multiple tags**:

       - `sha-<FULL_SHA>`
       - `<BRANCH_SAFE>-<SHORT_SHA>` (when branch)
       - `<BRANCH_SAFE>-latest` (when branch)
       - `<SHORT_SHA>-<BUILD_TAG>` (for Helm)
       - On tag push: `<VERSION_TAG>` and `<PLAIN_VERSION>` (without leading v)

3. `update-helm-chart`

   - Same logic as your old job:

     - Use `SHORT_SHA` and `BUILD_TAG` to update:

       - `values-dev.yaml`
       - `values-qa.yaml`
       - `values-staging.yaml` / `values-prod.yaml` for `latest`

   - No change to Helm structure required.

### üß† Why this is better for you

- **Still compatible** with your Helm setup
  ‚Üí `values-*.yaml` still receives tags in the form `<SHORT_SHA>-<BUILD_TAG>`.
- **More powerful tagging** for your registry consumers:

  - Easy rollbacks by SHA.
  - Branch-level testing tags like `dev-1234abc`.
  - Real release tags `v1.2.3`.

- **Monorepo friendly:**

  - Matrix per service.
  - You can later add dynamic matrix (only changed services).

- **Cleaner CI logs & easier debugging**

  - Each service = its own job in GitHub UI.

- **Faster builds**

  - Buildx + cache integration.
  - Ability to limit `max-parallel` if self-hosted.

If you want, I can now take your **exact branching/environment logic** (dev, alfa, main, tags) and write a **fully final YAML** for you that includes:

- Matrix per service
- Multi-tagging: `sha_full`, `branch-short_sha`, `branch-latest`, semver on tag
- And still outputs `<SHORT_SHA>-<build_tag>` for Helm.
